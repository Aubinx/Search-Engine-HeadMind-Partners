{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook\n",
    "\n",
    "This notebook is your search engine. \n",
    "\n",
    "For testing your work, we will run each cell. Thus, your code we'll have to fit the structure expected.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "- Install libraries (if you use Colab and needed),\n",
    "- Import the modules,\n",
    "- Declare global variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = \"datascience.stackexchange.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log10,sqrt\n",
    "import re\n",
    "import tkinter as tk\n",
    "import nltk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath):\n",
    "    return pd.read_xml(filepath, parser=\"etree\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts = extract_data(filepath=FILEPATH+\"/Posts.xml\")\n",
    "users = extract_data(filepath=FILEPATH+\"/Users.xml\")\n",
    "comments = extract_data(filepath=FILEPATH+\"/Comments.xml\")\n",
    "votes = extract_data(filepath=FILEPATH+\"/Votes.xml\")\n",
    "badges = extract_data(filepath=FILEPATH+\"/Badges.xml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def index_data():\n",
    "    # TODO\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenization with transformers bert uncased tokenizer\n",
    "# from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# import re\n",
    "# def tokenize_data(string):\n",
    "#     tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#     string = re.sub(r'[^\\w\\s]', '', string)\n",
    "#     return tokenizer.tokenize(str(string))\n",
    "# # make posts smaller\n",
    "# posts = posts.iloc[:1000, :]\n",
    "# posts['Title'] = posts['Title'].apply(tokenize_data)\n",
    "# posts['Body'] = posts['Body'].apply(tokenize_data)\n",
    "# print(len(posts))\n",
    "# load posts\n",
    "file = \"posts.pkl\"\n",
    "with open(file, 'rb') as f:\n",
    "    posts = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(posts['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed\n",
    "\n",
    "def get_inverted_index_title(posts): # returns inverted index\n",
    "    inverted_index = {}\n",
    "    for i in range(len(posts)):\n",
    "        for word in posts['Title'][i]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {}\n",
    "            if i not in inverted_index[word]:\n",
    "                inverted_index[word][i] = 0\n",
    "            inverted_index[word][i] += 1\n",
    "    return inverted_index\n",
    "\n",
    "def get_inverted_index_body(posts): # returns inverted index\n",
    "    inverted_index = {}\n",
    "    for i in range(len(posts)):\n",
    "        for word in posts['Body'][i]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {}\n",
    "            if i not in inverted_index[word]:\n",
    "                inverted_index[word][i] = 0\n",
    "            inverted_index[word][i] += 1\n",
    "    return inverted_index\n",
    "  \n",
    "inverted_index_body = get_inverted_index_body(posts)\n",
    "inverted_index_title = get_inverted_index_title(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save and Load your Index(es) in Pickle format\n",
    "# def save_index(savepath):\n",
    "#     with open(savepath, 'w') as file:\n",
    "#          pkl.dump(data, file)\n",
    "#          file.close()\n",
    "\n",
    "\n",
    "\n",
    "# def load_index(savepath):\n",
    "#     with open(savepath, 'r') as file:\n",
    "#         data = pkl.load(file)\n",
    "#         file.close()\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(inverted_index,k):\n",
    "    stop_words = []\n",
    "    for word in inverted_index:\n",
    "        if len(inverted_index[word]) > len(posts)/k:\n",
    "            stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "stop_words_body = get_stop_words(inverted_index_body,4)\n",
    "stop_words_title = get_stop_words(inverted_index_title,20)\n",
    "print(stop_words_body)\n",
    "print(stop_words_title)\n",
    "# remove stop words from inverted index\n",
    "def remove_stop_words(inverted_index, stop_words):\n",
    "    inverted_index_removed = {}\n",
    "    for word in inverted_index:\n",
    "        if word not in stop_words:\n",
    "            inverted_index_removed[word] = inverted_index[word]\n",
    "    return inverted_index_removed\n",
    "\n",
    "inverted_index_body_removed = remove_stop_words(inverted_index_body, stop_words_body)\n",
    "inverted_index_title_removed = remove_stop_words(inverted_index_title, stop_words_title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Il faut mettre le nombre de vues, le titre et les commentaires\n",
    "def get_tags_from_postid(posts, id):\n",
    "    taglist = posts[posts[\"Id\"] == id][\"Tags\"]\n",
    "    return taglist\n",
    "\n",
    "def get_text(doc_id,posts):\n",
    "    text = posts[posts[\"Id\"] == doc_id][\"Body\"]\n",
    "    return text\n",
    "    \n",
    "def get_title(doc_id,posts):\n",
    "    title = posts[posts[\"Id\"] == id][\"Title\"]\n",
    "    return title\n",
    "\n",
    "\n",
    "def doc_tags_association(posts):\n",
    "    association = []\n",
    "    for i in range(0, len(posts)):\n",
    "        association += [(i, get_tags_from_postid(posts, i))]\n",
    "    return association\n",
    "\n",
    "\n",
    "def get_votes(posts, id):\n",
    "    association = doc_tags_association(posts)\n",
    "    return posts[posts[\"Id\"] == association[posts]]\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_avec_repetition(text):\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    return t.split()\n",
    "\n",
    "def vectorize_element(element, vocabulary):\n",
    "    n = len(vocabulary)\n",
    "    vecteur = np.zeros((1, n))\n",
    "    element_tokens = tokenize_avec_repetition(element)\n",
    "    key_list = list(vocabulary.keys())\n",
    "    for elem in element_tokens:\n",
    "        i = key_list.index(elem)\n",
    "        vecteur[0][i] += 1\n",
    "    return vdecteur\n",
    "\n",
    "\n",
    "def match(query, doc_id, vocabulary, pcosbody, pcostitle, ptags, pvotes, pvues, pcommentaires):\n",
    "    rcosdoc = mesure_cos_element(query,element,vocabulary)\n",
    "    vectq=np.array(vectorize_element(query,vocabulary))\n",
    "\n",
    "    dtags = get_tags_from_postid(posts, doc_id)\n",
    "    rtags = mesure_cos_element(query, dtags, vocabulary)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    for i in range(0, len(vectq)):\n",
    "        for j in range(0, len(dtags)):\n",
    "            if vectq[i] == dtags[j]:\n",
    "                rtags += 1\n",
    "\n",
    "    rvotes = log10( 1+\n",
    "        get_votes(posts, doc_id)\n",
    "    )  # on prend le log car un score avec 10000 likes n'est pas forct 100 fois plus pertinent qu'un post avec  d0 likes\n",
    "\n",
    "    return pcos * rcos + ptags * rtags + pvotes * rvotesef match(query, doc_id, vocabulary, pcosbody, pcostitle, ptags, pvotes, pvues, pcommentaires):\n",
    "    rcosdoc = mesure_cos_element(query,element,vocabulary)\n",
    "\n",
    "    vectq=np.array(vectorize_element(query,vocabulary))\n",
    "\n",
    "    dtags = get_tags_from_postid(posts, doc_id)\n",
    "    rtags = mesure_cos_element(query, dtags, vocabulary)\n",
    "\n",
    "    rvotes = log10( 1+\n",
    "        get_votes(posts, doc_id)\n",
    "    )  # on prend le log car un score avec 10000 likes n'est pas forc√©ment 100 fois plus pertinent qu'un post avec 100 likes\n",
    "\n",
    "    return pcos * rcos + ptags * rtags + pvotes * rvotesocumentet du nombre de like\n",
    "\n",
    "    for i in range (0,len(posts)): \n",
    "        classement += [(i,atch( query_tk, i , pcos= 1, ptags=1, pvotes=1))]\n",
    "\n",
    "    \n",
    "    classement_trie = sorted(classement, key=lambda x: x[1])\n",
    "\n",
    "    classement = []\n",
    "    # take the top doc_id with the highest match score with the query\n",
    "    for i in range(0, top):\n",
    "        classement += [classement[i][0]]\n",
    "\n",
    "\n",
    "    return classement\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas s√ªr de garder cette partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
