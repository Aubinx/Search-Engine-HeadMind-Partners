{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook\n",
    "\n",
    "This notebook is your search engine. \n",
    "\n",
    "For testing your work, we will run each cell. Thus, your code we'll have to fit the structure expected.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "- Install libraries (if you use Colab and needed),\n",
    "- Import the modules,\n",
    "- Declare global variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = \"datascience.stackexchange.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log10,sqrt\n",
    "import re\n",
    "import tkinter as tk\n",
    "import nltk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath):\n",
    "    return pd.read_xml(filepath, parser=\"etree\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts = extract_data(filepath=FILEPATH+\"/Posts.xml\")\n",
    "users = extract_data(filepath=FILEPATH+\"/Users.xml\")\n",
    "comments = extract_data(filepath=FILEPATH+\"/Comments.xml\")\n",
    "votes = extract_data(filepath=FILEPATH+\"/Votes.xml\")\n",
    "badges = extract_data(filepath=FILEPATH+\"/Badges.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def index_data():\n",
    "    # TODO\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# import re\n",
    "def tokenize_data(string):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string)\n",
    "    return tokenizer.tokenize(str(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenization with transformers bert uncased tokenizer\n",
    "\n",
    "# # make posts smaller\n",
    "# posts = posts.iloc[:1000, :]\n",
    "# posts['Title'] = posts['Title'].apply(tokenize_data)\n",
    "# posts['Body'] = posts['Body'].apply(tokenize_data)\n",
    "# print(len(posts))\n",
    "# load posts\n",
    "file = \"posts.pkl\"\n",
    "with open(file, 'rb') as f:\n",
    "    posts = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [information, retrieval, vs, recommendation, s...\n",
      "1                                [the, idea, behind, drop]\n",
      "2        [maximize, accuracy, with, differential, evolu...\n",
      "3                                                       []\n",
      "4                    [help, using, b, to, rank, sentences]\n",
      "                               ...                        \n",
      "75622    [best, way, to, detect, newly, incoming, an, i...\n",
      "75623    [how, to, represent, and, graphical, display, ...\n",
      "75624                               [rotate, vector, anti]\n",
      "75625    [random, forest, partial, dependence, plot, ef...\n",
      "75626    [can, get, in, a, p, data, return, a, random, ...\n",
      "Name: Title, Length: 75627, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(posts['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed\n",
    "\n",
    "def get_inverted_index_title(posts): # returns inverted index\n",
    "    inverted_index = {}\n",
    "    for i in range(len(posts)):\n",
    "        for word in posts['Title'][i]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {}\n",
    "            if i not in inverted_index[word]:\n",
    "                inverted_index[word][i] = 0\n",
    "            inverted_index[word][i] += 1\n",
    "    return inverted_index\n",
    "\n",
    "def get_inverted_index_body(posts): # returns inverted index\n",
    "    inverted_index = {}\n",
    "    for i in range(len(posts)):\n",
    "        for word in posts['Body'][i]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {}\n",
    "            if i not in inverted_index[word]:\n",
    "                inverted_index[word][i] = 0\n",
    "            inverted_index[word][i] += 1\n",
    "    return inverted_index\n",
    "  \n",
    "inverted_index_body = get_inverted_index_body(posts)\n",
    "inverted_index_title = get_inverted_index_title(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save and Load your Index(es) in Pickle format\n",
    "# def save_index(savepath):\n",
    "#     with open(savepath, 'w') as file:\n",
    "#          pkl.dump(data, file)\n",
    "#          file.close()\n",
    "\n",
    "\n",
    "\n",
    "# def load_index(savepath):\n",
    "#     with open(savepath, 'r') as file:\n",
    "#         data = pkl.load(file)\n",
    "#         file.close()\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'if', 'this', 'is', 'so', 'pi', 'have', 'a', 'hr', 're', 'nor', 'and', 'that', 'can', 'be', 'as', 'of', 'are', 'the', 'by', 'but', 'to', 'with', 'i', 'an', 'on', 'which', 'data', 'or', 'not', 'there', 'model', 'pre', 'code', 'for', 'p', 'it', 'you', 'would', 'like', 'pt', 'using', 'do', 'use', 'one', 'from', 'your']\n",
      "['the', 'with', 'to', 'and', 'for', 'of', 'in', 'how', 'a', 'data', 'is']\n"
     ]
    }
   ],
   "source": [
    "def get_stop_words(inverted_index,k):\n",
    "    stop_words = []\n",
    "    for word in inverted_index:\n",
    "        if len(inverted_index[word]) > len(posts)/k:\n",
    "            stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "stop_words_body = get_stop_words(inverted_index_body,4)\n",
    "stop_words_title = get_stop_words(inverted_index_title,20)\n",
    "print(stop_words_body)\n",
    "print(stop_words_title)\n",
    "# remove stop words from inverted index\n",
    "def remove_stop_words(inverted_index, stop_words):\n",
    "    inverted_index_removed = {}\n",
    "    for word in inverted_index:\n",
    "        if word not in stop_words:\n",
    "            inverted_index_removed[word] = inverted_index[word]\n",
    "    return inverted_index_removed\n",
    "\n",
    "inverted_index_body_removed = remove_stop_words(inverted_index_body, stop_words_body)\n",
    "inverted_index_title_removed = remove_stop_words(inverted_index_title, stop_words_title)\n",
    "\n",
    "# # remove stop words from posts\n",
    "# def remove_stop_words_posts(posts, stop_words_body, stop_words_title):\n",
    "#     posts_removed = posts.copy()\n",
    "#     for i in range(len(posts_removed)):\n",
    "#         posts_removed['Title'][i] = [word for word in posts_removed['Title'][i] if word not in stop_words_title]\n",
    "#         posts_removed['Body'][i] = [word for word in posts_removed['Body'][i] if word not in stop_words_body]\n",
    "#     return posts_removed\n",
    "\n",
    "# posts = remove_stop_words_posts(posts, stop_words_body,stop_words_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(inverted_index_body_removed):\n\u001b[1;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m j, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(inverted_index_body_removed[word]):\n\u001b[0;32m----> 7\u001b[0m         tfidfMatrix_body[i,j] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m log10(inverted_index_body_removed[word][doc])) \u001b[39m*\u001b[39m log10(\u001b[39mlen\u001b[39m(posts)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(inverted_index_body_removed[word]))\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(tfidfMatrix_body)\n\u001b[1;32m     11\u001b[0m \u001b[39m# build an SVD model, n_components = 100 is chosen in random\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# svd_model = TruncatedSVD(n_components=100, \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m#                          algorithm='randomized',\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m#                          n_iter=10, random_state=42)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Latent Semantic indexing\n",
    "\n",
    "\n",
    "tfidfMatrix_body = np.zeros((len(inverted_index_body_removed), len(posts)))\n",
    "for i, word in enumerate(inverted_index_body_removed):\n",
    "    for j, doc in enumerate(inverted_index_body_removed[word]):\n",
    "        tfidfMatrix_body[i,j] = (1 + log10(inverted_index_body_removed[word][doc])) * log10(len(posts)/len(inverted_index_body_removed[word]))\n",
    "\n",
    "print(tfidfMatrix_body)\n",
    "\n",
    "# build an SVD model, n_components = 100 is chosen in random\n",
    "# svd_model = TruncatedSVD(n_components=100, \n",
    "#                          algorithm='randomized',\n",
    "#                          n_iter=10, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_search(query, df=posts,inverted_index, top=5):\n",
    "  # each document get a score\n",
    "  # OKAPI model (BM25)\n",
    "  # print('query : ', query)\n",
    "  # print('top : ', top)\n",
    "  query_words = tokenize_data(query)\n",
    "  k1 = 1.2\n",
    "  b = 0.75\n",
    "  k3 = 1000\n",
    "  # average length of a document\n",
    "  m = df['Body'].apply(lambda x: len(x)).mean()\n",
    "  N = len(df)\n",
    "  RSV_score = {}\n",
    "  # for each post in df :\n",
    "  for _, row in df.iterrows():\n",
    "    # sum over all words in the query and in the post\n",
    "    # length of the post\n",
    "    Ld = len(df)\n",
    "    # term frequency in the query\n",
    "    def tf(word):\n",
    "      return sum([1 for w in query_words if w == word])\n",
    "    def d_f(word):\n",
    "      if word not in inverted_index:\n",
    "        #print(word)\n",
    "        return 0\n",
    "      else:\n",
    "        return len(inverted_index[word])\n",
    "    RSV_score[row['Id']] = sum([(k1+1)*tf(word)/(k1*((1-b)+b*Ld/m)+tf(word))*(k3+1)*tf(word)/(k3+tf(word))*np.log((N-d_f(word)+0.5)/(d_f(word)+0.5)) for word in query_words if word in row['words']])\n",
    "  # return the top Ids of the posts from RSV\n",
    "  sorted_keys = sorted(RSV_score, key=RSV_score.get, reverse=True) # the Id column in the best order\n",
    "  # the values of sorted_keys are values of df[\"Id\"]\n",
    "  # return the top documents in the same order as sorted_keys\n",
    "  new_df = df.copy()\n",
    "  new_df['RSV_score'] = new_df['Id'].apply(lambda x: RSV_score[x])\n",
    "  new_df = new_df.sort_values(by='RSV_score', ascending=False)\n",
    "  return new_df[new_df[\"Id\"].isin(sorted_keys[0:top])]\n",
    "  #new_df[new_df[\"Id\"].isin(sorted_keys[0:top])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Il faut mettre le nombre de vues, le titre et les commentaires\n",
    "def get_tags_from_postid(posts, id):\n",
    "    taglist = posts[posts[\"Id\"] == id][\"Tags\"]\n",
    "    return taglist\n",
    "\n",
    "def get_text(doc_id,posts):\n",
    "    text = posts[posts[\"Id\"] == doc_id][\"Body\"]\n",
    "    return text\n",
    "\n",
    "def get_reputation(doc_id,posts):\n",
    "    # Réputation de l'auteur du post\n",
    "    reputation = users[users[\"Id\"] == int(posts[posts[\"Id\"] == doc_id][\"OwnerUserId\"])][\"Reputation\"].values[0]\n",
    "    return reputation\n",
    "    \n",
    "def get_title(doc_id,posts):\n",
    "    title = posts[posts[\"Id\"] == id][\"Title\"]\n",
    "    return title\n",
    "\n",
    "def get_RSV_score(query, df,inverted_index, top,doc_id):\n",
    "    dataf=proba_search(query,df,inverted_index,top)\n",
    "    RSV_score=dataf[dataf[\"Id\"]==doc_id]['RSV_score']\n",
    "    return RSV_score\n",
    "\n",
    "\n",
    "def doc_tags_association(posts):\n",
    "    association = []\n",
    "    for i in range(0, len(posts)):\n",
    "        association += [(i, get_tags_from_postid(posts, i))]\n",
    "    return association\n",
    "\n",
    "def get_votes(posts, id):\n",
    "    association = doc_tags_association(posts)\n",
    "    return posts[posts[\"Id\"] == association[posts]]\n",
    "\n",
    "def tokenize_avec_repetition(text):\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    return t.split()\n",
    "\n",
    "def vectorize_element(element, vocabulary):\n",
    "    n = len(vocabulary)\n",
    "    vecteur = np.zeros((1, n))\n",
    "    element_tokens = tokenize_avec_repetition(element)\n",
    "    key_list = list(vocabulary.keys())\n",
    "    for elem in element_tokens:\n",
    "        i = key_list.index(elem)\n",
    "        vecteur[0][i] += 1\n",
    "    return vdecteur\n",
    "\n",
    "def mesure_cos(query, element, vocabulary):\n",
    "    vectq = np.array(vectorize_query(query, vocabulary))\n",
    "    vectd = np.array(vectorize_document(document, vocabulary))\n",
    "    normq = sqrt(np.dot(vectq, vectq))\n",
    "    normd = sqrt(np.dot(vectd, vectd))\n",
    "    ps = np.dot(vectq, vectd)\n",
    "    return ps / (normq * normd)\n",
    "\n",
    "def match(query, doc_id, vocabulary, prsv, pcosbody, pcostitle, ptags, pvotes, pvues, pcommentaires, preputation):\n",
    "    body=get_text(doc_id,posts)\n",
    "    rcosbody = mesure_cos(query,body,vocabulary)\n",
    "    title=get_title(doc_id,posts)\n",
    "    rcostitle= mesure_cos(query,title,vocabulary)\n",
    "    vectq=np.array(vectorize_element(query,vocabulary))\n",
    "\n",
    "    dtags = get_tags_from_postid(posts, doc_id)\n",
    "    rtags = mesure_cos_element(query, dtags, vocabulary)\n",
    "\n",
    "    for i in range(0, len(vectq)):\n",
    "        for j in range(0, len(dtags)):\n",
    "            if vectq[i] == dtags[j]:\n",
    "                rtags += 1\n",
    "\n",
    "    reputation = log10( 1 + get_reputation(doc_id, posts))\n",
    "\n",
    "    rvotes = log10( 1 + get_votes(posts, doc_id))\n",
    "    RSV_score=get_RSV_score(query, posts,vocabulary, top,doc_id)\n",
    "    # on prend le log car un score avec 10000 likes n'est pas forct 100 fois plus pertinent qu'un post avec  d0 likes\n",
    "\n",
    "    return pcosbody * rcosdoc + ptags * rtags + pvotes * rvotes + preputation * reputation + prsv*RSV_score + pcostitle*rcostitle\n",
    "\n",
    "    for i in range (0,len(posts)): \n",
    "        classement += [(i,atch( query_tk, i , pcos= 1, ptags=1, pvotes=1))]\n",
    "\n",
    "    \n",
    "    classement_trie = sorted(classement, key=lambda x: x[1])\n",
    "\n",
    "    classement = []\n",
    "    # take the top doc_id with the highest match score with the query\n",
    "    for i in range(0, top):\n",
    "        classement += [classement[i][0]]\n",
    "\n",
    "\n",
    "    return classement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9298    [pt, most, basic, relationship, to, describe, ...\n",
      "Name: Body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(get_text(123,posts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas sûr de garder cette partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
