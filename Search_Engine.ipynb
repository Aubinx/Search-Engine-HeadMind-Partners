{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Notebook\n",
    "\n",
    "This notebook is your search engine. \n",
    "\n",
    "For testing your work, we will run each cell. Thus, your code we'll have to fit the structure expected.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "- Install libraries (if you use Colab and needed),\n",
    "- Import the modules,\n",
    "- Declare global variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = \"datascience.stackexchange.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log10,sqrt\n",
    "import re\n",
    "import tkinter as tk\n",
    "import nltk\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filepath):\n",
    "    return pd.read_xml(filepath, parser=\"etree\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "not well-formed (invalid token): line 1, column 29 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3508\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[11], line 2\u001b[0m\n    users = extract_data(filepath=FILEPATH+\"/Users.xml\")\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[10], line 2\u001b[0m in \u001b[1;35mextract_data\u001b[0m\n    return pd.read_xml(filepath, parser=\"etree\", encoding=\"utf8\")\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/pandas/io/xml.py:1118\u001b[0m in \u001b[1;35mread_xml\u001b[0m\n    return _parse(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/pandas/io/xml.py:844\u001b[0m in \u001b[1;35m_parse\u001b[0m\n    data_dicts = p.parse_data()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/pandas/io/xml.py:450\u001b[0m in \u001b[1;35mparse_data\u001b[0m\n    self.xml_doc = self._parse_doc(self.path_or_buffer)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/.local/lib/python3.10/site-packages/pandas/io/xml.py:541\u001b[0m in \u001b[1;35m_parse_doc\u001b[0m\n    document = parse(xml_data, parser=curr_parser)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/usr/lib/python3.10/xml/etree/ElementTree.py:1222\u001b[0m in \u001b[1;35mparse\u001b[0m\n    tree.parse(source, parser)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m/usr/lib/python3.10/xml/etree/ElementTree.py:586\u001b[0;36m in \u001b[0;35mparse\u001b[0;36m\n\u001b[0;31m    parser.feed(data)\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>\u001b[0;36m\u001b[0m\n\u001b[0;31mParseError\u001b[0m\u001b[0;31m:\u001b[0m not well-formed (invalid token): line 1, column 29\n"
     ]
    }
   ],
   "source": [
    "# posts = extract_data(filepath=FILEPATH+\"/Posts.xml\")\n",
    "users = extract_data(filepath=FILEPATH+\"/Users.xml\")\n",
    "comments = extract_data(filepath=FILEPATH+\"/Comments.xml\")\n",
    "votes = extract_data(filepath=FILEPATH+\"/Votes.xml\")\n",
    "badges = extract_data(filepath=FILEPATH+\"/Badges.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def index_data():\n",
    "    # TODO\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "# import re\n",
    "def tokenize_data(string):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string)\n",
    "    return tokenizer.tokenize(str(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenization with transformers bert uncased tokenizer\n",
    "\n",
    "# # make posts smaller\n",
    "# posts = posts.iloc[:1000, :]\n",
    "# posts['Title'] = posts['Title'].apply(tokenize_data)\n",
    "# posts['Body'] = posts['Body'].apply(tokenize_data)\n",
    "# print(len(posts))\n",
    "# load posts\n",
    "file = \"posts.pkl\"\n",
    "with open(file, 'rb') as f:\n",
    "    posts = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [information, retrieval, vs, recommendation, s...\n",
      "1                                [the, idea, behind, drop]\n",
      "2        [maximize, accuracy, with, differential, evolu...\n",
      "3                                                       []\n",
      "4                    [help, using, b, to, rank, sentences]\n",
      "                               ...                        \n",
      "75622    [best, way, to, detect, newly, incoming, an, i...\n",
      "75623    [how, to, represent, and, graphical, display, ...\n",
      "75624                               [rotate, vector, anti]\n",
      "75625    [random, forest, partial, dependence, plot, ef...\n",
      "75626    [can, get, in, a, p, data, return, a, random, ...\n",
      "Name: Title, Length: 75627, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(posts['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed\n",
    "\n",
    "def get_inverted_index_title(posts): # returns inverted index\n",
    "    inverted_index = {}\n",
    "    for i in range(len(posts)):\n",
    "        for word in posts['Title'][i]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {}\n",
    "            if i not in inverted_index[word]:\n",
    "                inverted_index[word][i] = 0\n",
    "            inverted_index[word][i] += 1\n",
    "    return inverted_index\n",
    "\n",
    "def get_inverted_index_body(posts): # returns inverted index\n",
    "    inverted_index = {}\n",
    "    for i in range(len(posts)):\n",
    "        for word in posts['Body'][i]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {}\n",
    "            if i not in inverted_index[word]:\n",
    "                inverted_index[word][i] = 0\n",
    "            inverted_index[word][i] += 1\n",
    "    return inverted_index\n",
    "  \n",
    "inverted_index_body = get_inverted_index_body(posts)\n",
    "inverted_index_title = get_inverted_index_title(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save and Load your Index(es) in Pickle format\n",
    "# def save_index(savepath):\n",
    "#     with open(savepath, 'w') as file:\n",
    "#          pkl.dump(data, file)\n",
    "#          file.close()\n",
    "\n",
    "\n",
    "\n",
    "# def load_index(savepath):\n",
    "#     with open(savepath, 'r') as file:\n",
    "#         data = pkl.load(file)\n",
    "#         file.close()\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'if', 'this', 'is', 'so', 'pi', 'have', 'a', 'hr', 're', 'nor', 'and', 'that', 'can', 'be', 'as', 'of', 'are', 'the', 'by', 'but', 'to', 'with', 'i', 'an', 'on', 'which', 'data', 'or', 'not', 'there', 'model', 'pre', 'code', 'for', 'p', 'it', 'you', 'would', 'like', 'pt', 'using', 'do', 'use', 'one', 'from', 'your']\n",
      "['the', 'with', 'to', 'and', 'for', 'of', 'in', 'how', 'a', 'data', 'is']\n"
     ]
    }
   ],
   "source": [
    "def get_stop_words(inverted_index,k):\n",
    "    stop_words = []\n",
    "    for word in inverted_index:\n",
    "        if len(inverted_index[word]) > len(posts)/k:\n",
    "            stop_words.append(word)\n",
    "    return stop_words\n",
    "\n",
    "stop_words_body = get_stop_words(inverted_index_body,4)\n",
    "stop_words_title = get_stop_words(inverted_index_title,20)\n",
    "print(stop_words_body)\n",
    "print(stop_words_title)\n",
    "# remove stop words from inverted index\n",
    "def remove_stop_words(inverted_index, stop_words):\n",
    "    inverted_index_removed = {}\n",
    "    for word in inverted_index:\n",
    "        if word not in stop_words:\n",
    "            inverted_index_removed[word] = inverted_index[word]\n",
    "    return inverted_index_removed\n",
    "\n",
    "inverted_index_body_removed = remove_stop_words(inverted_index_body, stop_words_body)\n",
    "inverted_index_title_removed = remove_stop_words(inverted_index_title, stop_words_title)\n",
    "\n",
    "# # remove stop words from posts\n",
    "# def remove_stop_words_posts(posts, stop_words_body, stop_words_title):\n",
    "#     posts_removed = posts.copy()\n",
    "#     for i in range(len(posts_removed)):\n",
    "#         posts_removed['Title'][i] = [word for word in posts_removed['Title'][i] if word not in stop_words_title]\n",
    "#         posts_removed['Body'][i] = [word for word in posts_removed['Body'][i] if word not in stop_words_body]\n",
    "#     return posts_removed\n",
    "\n",
    "# posts = remove_stop_words_posts(posts, stop_words_body,stop_words_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(inverted_index_body_removed):\n\u001b[1;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m j, doc \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(inverted_index_body_removed[word]):\n\u001b[0;32m----> 7\u001b[0m         tfidfMatrix_body[i,j] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m log10(inverted_index_body_removed[word][doc])) \u001b[39m*\u001b[39m log10(\u001b[39mlen\u001b[39m(posts)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(inverted_index_body_removed[word]))\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(tfidfMatrix_body)\n\u001b[1;32m     11\u001b[0m \u001b[39m# build an SVD model, n_components = 100 is chosen in random\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# svd_model = TruncatedSVD(n_components=100, \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m#                          algorithm='randomized',\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m#                          n_iter=10, random_state=42)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Latent Semantic indexing\n",
    "\n",
    "\n",
    "tfidfMatrix_body = np.zeros((len(inverted_index_body_removed), len(posts)))\n",
    "for i, word in enumerate(inverted_index_body_removed):\n",
    "    for j, doc in enumerate(inverted_index_body_removed[word]):\n",
    "        tfidfMatrix_body[i,j] = (1 + log10(inverted_index_body_removed[word][doc])) * log10(len(posts)/len(inverted_index_body_removed[word]))\n",
    "\n",
    "print(tfidfMatrix_body)\n",
    "\n",
    "# build an SVD model, n_components = 100 is chosen in random\n",
    "# svd_model = TruncatedSVD(n_components=100, \n",
    "#                          algorithm='randomized',\n",
    "#                          n_iter=10, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_search(query, df=posts,inverted_index, top=5):\n",
    "  # each document get a score\n",
    "  # OKAPI model (BM25)\n",
    "  # print('query : ', query)\n",
    "  # print('top : ', top)\n",
    "  query_words = tokenize_data(query)\n",
    "  k1 = 1.2\n",
    "  b = 0.75\n",
    "  k3 = 1000\n",
    "  # average length of a document\n",
    "  m = df['Body'].apply(lambda x: len(x)).mean()\n",
    "  N = len(df)\n",
    "  RSV_score = {}\n",
    "  # for each post in df :\n",
    "  for _, row in df.iterrows():\n",
    "    # sum over all words in the query and in the post\n",
    "    # length of the post\n",
    "    Ld = len(df)\n",
    "    # term frequency in the query\n",
    "    def tf(word):\n",
    "      return sum([1 for w in query_words if w == word])\n",
    "    def d_f(word):\n",
    "      if word not in inverted_index:\n",
    "        #print(word)\n",
    "        return 0\n",
    "      else:\n",
    "        return len(inverted_index[word])\n",
    "    RSV_score[row['Id']] = sum([(k1+1)*tf(word)/(k1*((1-b)+b*Ld/m)+tf(word))*(k3+1)*tf(word)/(k3+tf(word))*np.log((N-d_f(word)+0.5)/(d_f(word)+0.5)) for word in query_words if word in row['words']])\n",
    "  # return the top Ids of the posts from RSV\n",
    "  sorted_keys = sorted(RSV_score, key=RSV_score.get, reverse=True) # the Id column in the best order\n",
    "  # the values of sorted_keys are values of df[\"Id\"]\n",
    "  # return the top documents in the same order as sorted_keys\n",
    "  new_df = df.copy()\n",
    "  new_df['RSV_score'] = new_df['Id'].apply(lambda x: RSV_score[x])\n",
    "  new_df = new_df.sort_values(by='RSV_score', ascending=False)\n",
    "  return new_df[new_df[\"Id\"].isin(sorted_keys[0:top])]\n",
    "  #new_df[new_df[\"Id\"].isin(sorted_keys[0:top])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Il faut mettre le nombre de vues, le titre et les commentaires\n",
    "def get_tags_from_postid(posts, id):\n",
    "    taglist = posts[posts[\"Id\"] == id][\"Tags\"]\n",
    "    return taglist\n",
    "\n",
    "def get_text(doc_id,posts):\n",
    "    text = posts[posts[\"Id\"] == doc_id][\"Body\"]\n",
    "    return text\n",
    "\n",
    "def get_reputation(doc_id,posts):\n",
    "    # Réputation de l'auteur du post\n",
    "    reputation = users[users[\"Id\"] == int(posts[posts[\"Id\"] == doc_id][\"OwnerUserId\"])][\"Reputation\"].values[0]\n",
    "    return reputation\n",
    "    \n",
    "def get_title(doc_id,posts):\n",
    "    title = posts[posts[\"Id\"] == id][\"Title\"]\n",
    "    return title\n",
    "\n",
    "def get_RSV_score(query, df,inverted_index, top,doc_id):\n",
    "    dataf=proba_search(query,df,inverted_index,top)\n",
    "    RSV_score=dataf[dataf[\"Id\"]==doc_id]['RSV_score']\n",
    "    return RSV_score\n",
    "\n",
    "\n",
    "def doc_tags_association(posts):\n",
    "    association = []\n",
    "    for i in range(0, len(posts)):\n",
    "        association += [(i, get_tags_from_postid(posts, i))]\n",
    "    return association\n",
    "\n",
    "def get_votes(posts, id):\n",
    "    association = doc_tags_association(posts)\n",
    "    return posts[posts[\"Id\"] == association[posts]]\n",
    "\n",
    "def tokenize_avec_repetition(text):\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)\n",
    "    return t.split()\n",
    "\n",
    "def vectorize_element(element, vocabulary):\n",
    "    n = len(vocabulary)\n",
    "    vecteur = np.zeros((1, n))\n",
    "    element_tokens = tokenize_avec_repetition(element)\n",
    "    key_list = list(vocabulary.keys())\n",
    "    for elem in element_tokens:\n",
    "        i = key_list.index(elem)\n",
    "        vecteur[0][i] += 1\n",
    "    return vecteur\n",
    "\n",
    "def match(query, doc_id, vocabulary, pcosbody, pcostitle, ptags, pvotes, pvues, pcommentaires, preputation):\n",
    "    rcosdoc = mesure_cos_element(query,element,vocabulary)\n",
    "    vectq=np.array(vectorize_element(query,vocabulary))\n",
    "\n",
    "    dtags = get_tags_from_postid(posts, doc_id)\n",
    "    rtags = mesure_cos_element(query, dtags, vocabulary)\n",
    "\n",
    "    for i in range(0, len(vectq)):\n",
    "        for j in range(0, len(dtags)):\n",
    "            if vectq[i] == dtags[j]:\n",
    "                rtags += 1\n",
    "\n",
    "    reputation = log10( 1 + get_reputation(doc_id, posts))\n",
    "\n",
    "    rvotes = log10( 1 + get_votes(posts, doc_id))\n",
    "    # on prend le log car un score avec 10000 likes n'est pas forct 100 fois plus pertinent qu'un post avec  d0 likes\n",
    "\n",
    "    return pcos * rcos + ptags * rtags + pvotes * rvotes + preputation * reputation\n",
    "''''\n",
    "def search(query, top =10)\n",
    "    for i in range (0,len(posts)): \n",
    "        classement += [(i,atch( query_tk, i , pcos= 1, ptags=1, pvotes=1))]\n",
    "\n",
    "    \n",
    "    classement_trie = sorted(classement, key=lambda x: x[1])\n",
    "\n",
    "    classement = []\n",
    "    # take the top doc_id with the highest match score with the query\n",
    "    for i in range(0, top):\n",
    "        classement += [classement[i][0]]\n",
    "\n",
    "\n",
    "    return classement\n",
    "'''\n",
    "\n",
    "def search(query, top =10):\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9298    [pt, most, basic, relationship, to, describe, ...\n",
      "Name: Body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(get_text(123,posts))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtkinter\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Tkinter interface for the search engine\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# create a window\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m window \u001b[39m=\u001b[39m Tk()\n\u001b[1;32m      6\u001b[0m window\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mSearch Engine\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m window\u001b[39m.\u001b[39mgeometry(\u001b[39m'\u001b[39m\u001b[39m1000x600\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tk' is not defined"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "def search(query, top =10):\n",
    "    return query\n",
    "\n",
    "# Tkinter interface for the search engine\n",
    "# create a window\n",
    "window = Tk()\n",
    "window.title(\"Search Engine\")\n",
    "window.geometry('1000x600')\n",
    "# create a label\n",
    "lbl = Label(window, text=\"Search Engine\", font=(\"Arial Bold\", 50))\n",
    "lbl.grid(column=0, row=0)\n",
    "# create a label\n",
    "lbl = Label(window, text=\"Enter your query\", font=(\"Arial Bold\", 20))\n",
    "lbl.grid(column=0, row=1)\n",
    "# create a text entry box\n",
    "txt = Entry(window, width=100)\n",
    "txt.grid(column=0, row=2)\n",
    "# call the fonction search when the button is clicked\n",
    "def clicked():\n",
    "    query = txt.get()\n",
    "    search(query, top=10)\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas sûr de garder cette partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
