{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valentingorce/tp_centrale/blob/main/Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiy0xFlZLomY"
      },
      "source": [
        "# Web Information Retrieval\n",
        "## Introduction to search engines\n",
        "\n",
        "### DAY 2: Teacher version\n",
        "### Implementing a search engine\n",
        "\n",
        "The goal of this second session is to implement a first architecture of a search engine on the previously introduced dataset (stackexchange-datascience). If you missed the first session or if you did not saved the dataset, please reload the first session's notebook to download it. \n",
        "\n",
        "If you need some ifnormation about the dataset, it should be available here : https://archive.org/details/stackexchange\n",
        "\n",
        "The notebook is divided into several steps:\n",
        "-\tImplement the indexation\n",
        "-\tImplement the search method\n",
        "-\tDefine a ranking strategy and implement it\n",
        "-\tSuggest some improvements of the search engine\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_WrILaSIJL"
      },
      "source": [
        "## Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0H819R7iJF_y"
      },
      "outputs": [],
      "source": [
        "# !pip install ttable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI2VFiG1SJmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tt import BooleanExpression\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YySiGfQ1SNwT"
      },
      "outputs": [],
      "source": [
        "# # Only if you use Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rq6fLsSSPUn"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = 'datascience.stackexchange.com/'\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4kiMuicy6Du8"
      },
      "source": [
        "**Important :**\n",
        "\n",
        "An Excel file for testing the evaluation part is available in the gitlab repo : evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\n",
        "\n",
        "If you work on Colab, we advice you to push it directly on your Google Drive directory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PzqYFBxYAJN1"
      },
      "source": [
        "# Implement the indexation\n",
        "As you might already know, for a search engine to work properly an index of the documents must be created. Here we will keep it in python, and try to use only common libraries to keep it simple.\n",
        "\n",
        "Once created, the index will be used to match the query with the documents. As a result, there are several ways to build an index, using statistical, boolean, semantic indexation...\n",
        "\n",
        "First of, let's make a naive one that will consist in breaking down each document into a set of the words it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOClVNL5_abL"
      },
      "outputs": [],
      "source": [
        "def extract_words(text):\n",
        "  return text.split(' ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwVgveW6CIAz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# test\n",
        "s = \"The cat is sat on the mat. The dog is laid on the mat.\"\n",
        "words = extract_words(s)\n",
        "assert sorted(words) == ['The', 'The', 'cat', 'dog', 'is', 'is', 'laid', 'mat.', 'mat.', 'on', 'on', 'sat', 'the', 'the']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qTFNPmNJC75C"
      },
      "source": [
        "As you may notice, there are several problems with the previous implementation. First, \"The\" and \"the\" aren't considered the same, the \".\" is kept at the the end of \"mat.\" as any other punctuation character... \n",
        "\n",
        "Re-implement this function with some basic preprocessing to avoid these issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg9aRYX-CTTH"
      },
      "outputs": [],
      "source": [
        "# problems : First, \"The\" and \"the\" aren't considered the same, the \".\" is kept at the the end of \"mat.\" as any other punctuation character... \n",
        "def extract_words(text:str)->list:\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]','',text)\n",
        "  return text.split(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnTSVCA1Fd1q"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "print(sorted(extract_words(s)))\n",
        "assert sorted(extract_words(s))==['cat', 'dog', 'is', 'is', 'laid', 'mat', 'mat', 'on', 'on', 'sat', 'the', 'the', 'the', 'the']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQq7QZKF7mM"
      },
      "source": [
        "Now you sould be able to create your index table. For now we will just make a dataframe with two columns: [raw_text, words]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLO7naGaF0LP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def index_docs(docs:list[str])->pd.DataFrame:\n",
        "  df = pd.DataFrame(docs,columns=['raw_text'])\n",
        "  df['words'] = df['raw_text'].apply(extract_words)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpK3zlftGw_w"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "\n",
        "L = [s, \"Hello World!\", \"Goodbye\", \"How are you?\"]\n",
        "\n",
        "index_docs(L)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "70w-UfpsHktY"
      },
      "source": [
        "Now, let's try it on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46pO8FszG_4s"
      },
      "outputs": [],
      "source": [
        "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")\n",
        "posts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DcaGAngHLK4g"
      },
      "source": [
        "For our first version of the indexation mechanism, we will simply use the \"body\" of the posts. To have a better search engine, the title and other metadata aswell could be used aswell. Finally, not all the XML files have a \"body\" feature, so for the search engine to retrieve information from any of the files you will need to implement another way to index.\n",
        "\n",
        "But first, let's start with \"body\". There is more to preprocess than before, indeed, there are html tags such as \"<p>\" for instance. They are not useful for us, because users won't use them in their queries. So we first need to remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et-7VlXyKuaf"
      },
      "outputs": [],
      "source": [
        "def remove_tags(text:str)->str:\n",
        "  return re.sub(r'<[^>]+>', '', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQAW9pi9MkyZ"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "remove_tags('<p>Hello World!\\nI am making a search engine.<p>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDiFEsPtMszw"
      },
      "outputs": [],
      "source": [
        "clean_posts = posts[['Id','Body']]\n",
        "clean_posts['Clean Body'] = clean_posts['Body'].fillna('').apply(remove_tags)\n",
        "clean_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RbkcTyrNDcJ"
      },
      "outputs": [],
      "source": [
        "clean_posts['words'] = clean_posts['Clean Body'].apply(extract_words)\n",
        "clean_posts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfNgdg7ziCm"
      },
      "source": [
        "## Zipf Law\n",
        "\n",
        "A way of analyzing a corpus is to draw the zipf law"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrts6RVNziLk"
      },
      "outputs": [],
      "source": [
        "# Draw Zipf Law on the Posts Corpus\n",
        "import matplotlib.pyplot as plt\n",
        "# get word count for each word\n",
        "word_count = clean_posts['words'].explode().value_counts()\n",
        "word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(word_count.values)\n",
        "plt.xlabel('Word Rank')\n",
        "plt.ylabel('Word Count')\n",
        "# log-log scale\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "plt.title('Zipf Law')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uOYd2qLpwhHE"
      },
      "source": [
        "## Inverted Index\n",
        "\n",
        "Now, we want to go further on the indexing and build an inverted index. Inverted index is a dictionary where the keys are the words of the vocabulary and the values are the documents containing these words. Reducing the size of the vocabulary is a relevant first step when building an inverted index. Here, we will focus on the creation of the index, we leave you the optimisation steps :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sCV0Ds21g7J"
      },
      "outputs": [],
      "source": [
        "def create_index(posts:pd.DataFrame):\n",
        "  index = {}\n",
        "  for _, row in posts.iterrows():\n",
        "    for word in row['words']:\n",
        "      if word in index:\n",
        "        index[word].add(row['Id'])\n",
        "      else:\n",
        "        index[word] = {row['Id']}\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCPc3tRMZj9R"
      },
      "outputs": [],
      "source": [
        "# inverted_index = create_index(clean_posts.iloc[0:5000])\n",
        "# # save index to pickle\n",
        "# import pickle\n",
        "# with open('inverted_index.pickle', 'wb') as handle:\n",
        "#     pickle.dump(inverted_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load index from pickle\n",
        "import pickle\n",
        "inverted_index = {}\n",
        "with open('inverted_index.pickle', 'rb') as handle:\n",
        "    inverted_index = pickle.load(handle)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q97D2TjBOVZP"
      },
      "source": [
        "#### Well Done, you've indexed the dataset! \n",
        "Don't hesitate to save your indexes in txt or pickle file\n",
        "\n",
        "---\n",
        "# Implement the search method\n",
        "\n",
        "A naive method would be to count the number of words in common between the query and each posts. Then to rank the posts you could directly select the post who maximize the number of common words. Let's implement this approach :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZX8J3Vrq7St"
      },
      "outputs": [],
      "source": [
        "# Implement the word_in_index function \n",
        "# Inputs : a word (str) & a list of words\n",
        "# Output : pandas series of 1 if the word is in the list, else 0\n",
        "\n",
        "def word_in_index(word, word_list_index):\n",
        "  return pd.Series([1 if word in words else 0 for words in word_list_index])\n",
        "\n",
        "# test\n",
        "print(word_in_index('cat', [['cat', 'dog'], ['cat', 'mouse'], ['dog', 'mouse']]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFvxO88LtVi8"
      },
      "outputs": [],
      "source": [
        "# Implement the function which run through a pandas series and count the number of word in common\n",
        "# Use extract_words method, apply method with word_in_index function\n",
        "# Inputs : the query (str) & pandas series of strings\n",
        "# Output : Pandas series counting the number of common words between the query and each string in word_serie\n",
        "\n",
        "def count_common_words(query, word_serie):\n",
        "  query_words = extract_words(query)\n",
        "  return word_serie.apply(lambda x: sum(word_in_index(word, [x]) for word in query_words))\n",
        "\n",
        "# test\n",
        "print(count_common_words('cat dog', pd.Series([['cat', 'dog'], ['cat', 'mouse'], ['dog', 'mouse']])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHzyXeExNWQq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rank_top_query(query, df, top=5):\n",
        "  # get the number of common words between the query and each document\n",
        "  common_words = count_common_words(query, df['words'])\n",
        "  # sort the documents by number of common words\n",
        "  sorted_common_words = common_words.sort_values(by=0, ascending=False)\n",
        "  # return the top documents\n",
        "  return df.iloc[sorted_common_words.index[0:top]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRdErltStZGv"
      },
      "outputs": [],
      "source": [
        "results = rank_top_query(query=\"testing the query in python\", df=clean_posts, top=5) # prends 1min30 pour tourner\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(results)\n",
        "for _, row in results.iterrows():\n",
        "    print(row['Clean Body'])\n",
        "    print('-------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pros:\n",
        "# - easy to implement\n",
        "# - fast to compute (1min30 par requete, bon...)\n",
        "# cons:\n",
        "# - gives the same weight to all words, even common words like \"the\" or \"is\"\n",
        "# - doesn't take into account the order of the words in the query\n",
        "# - doesn't take into account the order of the words in the documents\n",
        "# - doesn't take into account the number of times a word appears in a document\n",
        "# - doesn't take into account the number of times a word appears in the corpus\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JumHiP3txgUb"
      },
      "source": [
        "Testez plusieurs requêtes et critiquez les résultats obtenus.\n",
        "\n",
        "Quels sont les pros and cons de cette méthodes. Vous l'indiquerez sur le rapport avec vos réflexions pour l'améliorer.\n",
        "\n",
        "Next, you have to implement the first improvements you find in the search method to get most relevant results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "nltk.download('stopwords')\n",
        "stop_words =  stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux77Xzftx-kX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def remove_stop_words(l_txt: list) -> list:\n",
        "    return [word for word in l_txt if word not in stop_words]\n",
        "\n",
        "# test\n",
        "print(remove_stop_words(['the', 'cat', 'is', 'on', 'the', 'mat'])) # ['cat', 'mat']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MecCCwzbx8qZ"
      },
      "source": [
        "## Boolean Search\n",
        "\n",
        "Thanks to the ttable library, implement a boolean search method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5JTNdIrVdH9"
      },
      "outputs": [],
      "source": [
        "from tt import BooleanExpression\n",
        "\n",
        "def boolean_search(query, df=clean_posts):\n",
        "  # get the words in the query\n",
        "  expression =  BooleanExpression(query)\n",
        "  # get the posts whose clean body that satisfy the expression\n",
        "  symbols = expression.symbols\n",
        "  # for each post and for each symbol, check if the symbol is in the post, then evaluate the expression\n",
        "  bools = df['Clean Body'].apply(lambda x: [symbol in x for symbol in symbols]).apply(lambda x: expression.evaluate(**dict(zip(symbols,x))))\n",
        "  # return all documents that satisfy the expression\n",
        "  return df.iloc[bools[bools].index]\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  # return the top documents\n",
        "\n",
        "results = boolean_search('java AND NOT python')\n",
        "\n",
        "for _, row in results[0:5].iterrows():\n",
        "    print(row['Clean Body'])\n",
        "    print('-------------------')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N4vFldsAycpB"
      },
      "source": [
        "## Probabilistic search\n",
        "\n",
        "Implement the MIB or BM25 method of searching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(clean_posts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCxjyrldyhUB"
      },
      "outputs": [],
      "source": [
        "def proba_search(query, df=clean_posts, top=5):\n",
        "  # each document get a score\n",
        "  # OKAPI model (BM25)\n",
        "  # print('query : ', query)\n",
        "  # print('top : ', top)\n",
        "  query_words = extract_words(query)\n",
        "  k1 = 1.2\n",
        "  b = 0.75\n",
        "  k3 = 1000\n",
        "  # average length of a document\n",
        "  m = df['Clean Body'].apply(lambda x: len(x)).mean()\n",
        "  N = len(df)\n",
        "  RSV_score = {}\n",
        "  # for each post in df :\n",
        "  for _, row in df.iterrows():\n",
        "    # sum over all words in the query and in the post\n",
        "    # length of the post\n",
        "    Ld = len(row['Clean Body'])\n",
        "    # term frequency in the query\n",
        "    def tf(word):\n",
        "      return sum([1 for w in query_words if w == word])\n",
        "    def d_f(word):\n",
        "      if word not in inverted_index:\n",
        "        #print(word)\n",
        "        return 0\n",
        "      else:\n",
        "        return len(inverted_index[word])\n",
        "    RSV_score[row['Id']] = sum([(k1+1)*tf(word)/(k1*((1-b)+b*Ld/m)+tf(word))*(k3+1)*tf(word)/(k3+tf(word))*np.log((N-d_f(word)+0.5)/(d_f(word)+0.5)) for word in query_words if word in row['words']])\n",
        "  # return the top Ids of the posts from RSV\n",
        "  sorted_keys = sorted(RSV_score, key=RSV_score.get, reverse=True) # the Id column in the best order\n",
        "  # the values of sorted_keys are values of df[\"Id\"]\n",
        "  # return the top documents in the same order as sorted_keys\n",
        "  new_df = df.copy()\n",
        "  new_df['RSV_score'] = new_df['Id'].apply(lambda x: RSV_score[x])\n",
        "  new_df = new_df.sort_values(by='RSV_score', ascending=False)\n",
        "  return new_df[new_df[\"Id\"].isin(sorted_keys[0:top])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test\n",
        "print(\"begin\")\n",
        "results = proba_search('measure performance for multi classification model',top=len(clean_posts))\n",
        "# reset index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(results)\n",
        "\n",
        "# for _, row in results.iterrows():\n",
        "#     print(row['Clean Body'])\n",
        "#     print('-------------------')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x9m_LFo_yog2"
      },
      "source": [
        "Compare the naive method with your improvements and the boolean and probabilistic search. (report)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "arfKMWtLyyxY"
      },
      "source": [
        "# Evaluate the Search\n",
        "\n",
        "Now you implement multiple search methods and you're able to improve it. You have to define metric to compare it objectively.\n",
        "\n",
        "\n",
        "\n",
        "We ask you to implement NDCG (Normalized Discounted Cumulative Gain) from few queries we implement on a dozen of post. We already defined the values of relevance judgement in the xlsx file : . The final score will be the mean quadratic error of the queries.\n",
        "\n",
        "\n",
        "Explication for the xlsx file :\n",
        "\n",
        "We propose you a Excel file with some posts and a mesure of relevancy for the queries\n",
        "\n",
        "- First column is the post Id,\n",
        "- Columns starting by query are the queries you have to test.\n",
        "- The values in this columns are the rank of relevancy of the post in regard with the query.\n",
        "- The missing values indicates you should not take into account the post\n",
        "\n",
        "\n",
        "You will have to criticize this metric and your result in the report. Then you will have to propose some improvements. \n",
        "\n",
        "Thereafter in this week, you will have to compare your different search engines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZxCcftBPagMQ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PostId</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Sentence</th>\n",
              "      <th>Query 1 : mesure performance for multiclassification model</th>\n",
              "      <th>Query 2 : draw neural network</th>\n",
              "      <th>Query 3 : neural network layers</th>\n",
              "      <th>Query 4 : how sklearn working</th>\n",
              "      <th>Query 5 : treat categorical data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6107</td>\n",
              "      <td>What are deconvolutional layers?</td>\n",
              "      <td>I recently read Fully Convolutional Networks f...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15989</td>\n",
              "      <td>Micro Average vs Macro average Performance in ...</td>\n",
              "      <td>I am trying out a multiclass classification se...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13490</td>\n",
              "      <td>How to set class weights for imbalanced classe...</td>\n",
              "      <td>I know that there is a possibility in Keras wi...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12321</td>\n",
              "      <td>What's the difference between fit and fit_tran...</td>\n",
              "      <td>I do not understand the difference between the...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22</td>\n",
              "      <td>K-Means clustering for mixed numeric and categ...</td>\n",
              "      <td>My data set contains a number of numeric attri...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PostId                                              Title  \\\n",
              "0    6107                   What are deconvolutional layers?   \n",
              "1   15989  Micro Average vs Macro average Performance in ...   \n",
              "2   13490  How to set class weights for imbalanced classe...   \n",
              "3   12321  What's the difference between fit and fit_tran...   \n",
              "4      22  K-Means clustering for mixed numeric and categ...   \n",
              "\n",
              "                                      First Sentence  \\\n",
              "0  I recently read Fully Convolutional Networks f...   \n",
              "1  I am trying out a multiclass classification se...   \n",
              "2  I know that there is a possibility in Keras wi...   \n",
              "3  I do not understand the difference between the...   \n",
              "4  My data set contains a number of numeric attri...   \n",
              "\n",
              "   Query 1 : mesure performance for multiclassification model  \\\n",
              "0                                                NaN            \n",
              "1                                                1.0            \n",
              "2                                                3.0            \n",
              "3                                                NaN            \n",
              "4                                                NaN            \n",
              "\n",
              "   Query 2 : draw neural network  Query 3 : neural network layers  \\\n",
              "0                            NaN                              1.0   \n",
              "1                            NaN                              NaN   \n",
              "2                            NaN                              NaN   \n",
              "3                            NaN                              NaN   \n",
              "4                            NaN                              NaN   \n",
              "\n",
              "   Query 4 : how sklearn working  Query 5 : treat categorical data  \n",
              "0                            NaN                               NaN  \n",
              "1                            NaN                               NaN  \n",
              "2                            NaN                               NaN  \n",
              "3                            1.0                               3.0  \n",
              "4                            5.0                               2.0  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read Relevancy CSV\n",
        "import pandas as pd\n",
        "df_relevancy = pd.read_excel(\"evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\")\n",
        "df_relevancy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVxld-Ujy0nN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "    \"\"\"\n",
        "    Calculates the NDCG (Normalized Discounted Cumulative Gain) score for each query in the given relevance dataframe\n",
        "    using the specified search method.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_relevancy : pandas.DataFrame\n",
        "        A dataframe containing the relevance scores for each post and query.\n",
        "        The first column should contain the post IDs, and the remaining columns from the 4th should be named 'query X',\n",
        "        where X is the query number starting from 1.\n",
        "        The values in the columns should be the relevance scores for each post with respect to the corresponding query.\n",
        "    method : function\n",
        "        The search method to use for retrieving the top documents for each query.\n",
        "        The function should take a query string as input and return a pandas.DataFrame containing the top documents.\n",
        "    top : int, optional\n",
        "        The number of top documents to retrieve for each query.\n",
        "        The default value is the total number of posts in the dataset.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        A dictionary containing the NDCG score for each query.\n",
        "        The keys are the query numbers starting from 1, and the values are the corresponding NDCG scores.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def get_ndcg_scores(df_relevancy,method=proba_search,top=len(clean_posts)):\n",
        "    # for each PostId in the relevancy dataframe, get the rank of the post according to the method\n",
        "    querys = {}\n",
        "    for i in range(3, 8):\n",
        "        querys[i] = df_relevancy.columns[i][10:]\n",
        "    method_results = {}\n",
        "    for i in range(3, 8):\n",
        "        method_results[i] = method(query=querys[i],top=top)\n",
        "        #print(method_results[i].head())\n",
        "    # get the score of each post according to the method\n",
        "    rel_preds = {}\n",
        "    for i in range(3, 8):\n",
        "        # the rankings of the posts in PostId\n",
        "        rel_preds[i] = [list(method_results[i][\"Id\"]).index(x) if x in method_results[i][\"Id\"] else top for x in df_relevancy['PostId']]\n",
        "        #print(rel_preds[i])\n",
        "    rel_trues = {}\n",
        "    for i in range(3, 8):\n",
        "        # the rankings of the posts\n",
        "        rel_trues[i] = df_relevancy[df_relevancy.columns[i]].tolist()\n",
        "        #print(rel_trues[i])\n",
        "        rel_trues[i] = [top if np.isnan(x) else x for x in rel_trues[i]]\n",
        "\n",
        "    # calculate the ndcg score for each query\n",
        "    ndcg_scores = {}\n",
        "    for i in range(3, 8):\n",
        "        ndcg_scores[i] = ndcg_score([rel_trues[i]], [rel_preds[i]])\n",
        "    return ndcg_scores\n",
        "    \n",
        "\n",
        "print(get_ndcg_scores(df_relevancy,method=proba_search))\n",
        "   \n",
        "# rel_pred = \n",
        "\n",
        "\n",
        "# table of ndcg for each query\n",
        "# ndcg_table = pd.DataFrame(columns=['Query', 'NDCG'])\n",
        "# for i in range(4, 8):\n",
        "  # ndcg_table = ndcg_table.append({'Query': f'Query {i-3}', 'NDCG': calculate_ndgc(df_relevancy.columns[i])}, ignore_index=True)\n",
        "\n",
        "# print(ndcg_table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
