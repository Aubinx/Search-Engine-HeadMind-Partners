{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valentingorce/tp_centrale/blob/main/Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiy0xFlZLomY"
      },
      "source": [
        "# Web Information Retrieval\n",
        "## Introduction to search engines\n",
        "\n",
        "### DAY 2: Teacher version\n",
        "### Implementing a search engine\n",
        "\n",
        "The goal of this second session is to implement a first architecture of a search engine on the previously introduced dataset (stackexchange-datascience). If you missed the first session or if you did not saved the dataset, please reload the first session's notebook to download it. \n",
        "\n",
        "If you need some ifnormation about the dataset, it should be available here : https://archive.org/details/stackexchange\n",
        "\n",
        "The notebook is divided into several steps:\n",
        "-\tImplement the indexation\n",
        "-\tImplement the search method\n",
        "-\tDefine a ranking strategy and implement it\n",
        "-\tSuggest some improvements of the search engine\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_WrILaSIJL"
      },
      "source": [
        "## Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0H819R7iJF_y"
      },
      "outputs": [],
      "source": [
        "# !pip install ttable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lI2VFiG1SJmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tt import BooleanExpression\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YySiGfQ1SNwT"
      },
      "outputs": [],
      "source": [
        "# # Only if you use Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0rq6fLsSSPUn"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = 'datascience.stackexchange.com/'\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4kiMuicy6Du8"
      },
      "source": [
        "**Important :**\n",
        "\n",
        "An Excel file for testing the evaluation part is available in the gitlab repo : evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\n",
        "\n",
        "If you work on Colab, we advice you to push it directly on your Google Drive directory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PzqYFBxYAJN1"
      },
      "source": [
        "# Implement the indexation\n",
        "As you might already know, for a search engine to work properly an index of the documents must be created. Here we will keep it in python, and try to use only common libraries to keep it simple.\n",
        "\n",
        "Once created, the index will be used to match the query with the documents. As a result, there are several ways to build an index, using statistical, boolean, semantic indexation...\n",
        "\n",
        "First of, let's make a naive one that will consist in breaking down each document into a set of the words it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lOClVNL5_abL"
      },
      "outputs": [],
      "source": [
        "def extract_words(text):\n",
        "  return text.split(' ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QwVgveW6CIAz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# test\n",
        "s = \"The cat is sat on the mat. The dog is laid on the mat.\"\n",
        "words = extract_words(s)\n",
        "assert sorted(words) == ['The', 'The', 'cat', 'dog', 'is', 'is', 'laid', 'mat.', 'mat.', 'on', 'on', 'sat', 'the', 'the']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qTFNPmNJC75C"
      },
      "source": [
        "As you may notice, there are several problems with the previous implementation. First, \"The\" and \"the\" aren't considered the same, the \".\" is kept at the the end of \"mat.\" as any other punctuation character... \n",
        "\n",
        "Re-implement this function with some basic preprocessing to avoid these issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eg9aRYX-CTTH"
      },
      "outputs": [],
      "source": [
        "# problems : First, \"The\" and \"the\" aren't considered the same, the \".\" is kept at the the end of \"mat.\" as any other punctuation character... \n",
        "def extract_words(text:str)->list:\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]','',text)\n",
        "  return text.split(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wnTSVCA1Fd1q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cat', 'dog', 'is', 'is', 'laid', 'mat', 'mat', 'on', 'on', 'sat', 'the', 'the', 'the', 'the']\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "print(sorted(extract_words(s)))\n",
        "assert sorted(extract_words(s))==['cat', 'dog', 'is', 'is', 'laid', 'mat', 'mat', 'on', 'on', 'sat', 'the', 'the', 'the', 'the']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQq7QZKF7mM"
      },
      "source": [
        "Now you sould be able to create your index table. For now we will just make a dataframe with two columns: [raw_text, words]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XLO7naGaF0LP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def index_docs(docs:list[str])->pd.DataFrame:\n",
        "  df = pd.DataFrame(docs,columns=['raw_text'])\n",
        "  df['words'] = df['raw_text'].apply(extract_words)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IpK3zlftGw_w"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_text</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The cat is sat on the mat. The dog is laid on ...</td>\n",
              "      <td>[the, cat, is, sat, on, the, mat, the, dog, is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hello World!</td>\n",
              "      <td>[hello, world]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Goodbye</td>\n",
              "      <td>[goodbye]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How are you?</td>\n",
              "      <td>[how, are, you]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            raw_text  \\\n",
              "0  The cat is sat on the mat. The dog is laid on ...   \n",
              "1                                       Hello World!   \n",
              "2                                            Goodbye   \n",
              "3                                       How are you?   \n",
              "\n",
              "                                               words  \n",
              "0  [the, cat, is, sat, on, the, mat, the, dog, is...  \n",
              "1                                     [hello, world]  \n",
              "2                                          [goodbye]  \n",
              "3                                    [how, are, you]  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "\n",
        "L = [s, \"Hello World!\", \"Goodbye\", \"How are you?\"]\n",
        "\n",
        "index_docs(L)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "70w-UfpsHktY"
      },
      "source": [
        "Now, let's try it on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "46pO8FszG_4s"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>PostTypeId</th>\n",
              "      <th>CreationDate</th>\n",
              "      <th>Score</th>\n",
              "      <th>ViewCount</th>\n",
              "      <th>Body</th>\n",
              "      <th>OwnerUserId</th>\n",
              "      <th>LastActivityDate</th>\n",
              "      <th>Title</th>\n",
              "      <th>Tags</th>\n",
              "      <th>...</th>\n",
              "      <th>ClosedDate</th>\n",
              "      <th>ContentLicense</th>\n",
              "      <th>AcceptedAnswerId</th>\n",
              "      <th>LastEditorUserId</th>\n",
              "      <th>LastEditDate</th>\n",
              "      <th>ParentId</th>\n",
              "      <th>OwnerDisplayName</th>\n",
              "      <th>CommunityOwnedDate</th>\n",
              "      <th>LastEditorDisplayName</th>\n",
              "      <th>FavoriteCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-13T23:58:30.457</td>\n",
              "      <td>9</td>\n",
              "      <td>898.0</td>\n",
              "      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2014-05-14T00:36:31.077</td>\n",
              "      <td>How can I do simple machine learning without h...</td>\n",
              "      <td>&lt;machine-learning&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>2014-05-14T14:40:25.950</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-14T00:11:06.457</td>\n",
              "      <td>4</td>\n",
              "      <td>478.0</td>\n",
              "      <td>&lt;p&gt;As a researcher and instructor, I'm looking...</td>\n",
              "      <td>36.0</td>\n",
              "      <td>2014-05-16T13:45:00.237</td>\n",
              "      <td>What open-source books (or other materials) pr...</td>\n",
              "      <td>&lt;education&gt;&lt;open-source&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>2014-05-14T08:40:54.950</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>2014-05-16T13:45:00.237</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>2014-05-14T00:36:31.077</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;Not sure if this fits the scope of this SE,...</td>\n",
              "      <td>51.0</td>\n",
              "      <td>2014-05-14T00:36:31.077</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>5.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2014-05-14T00:53:43.273</td>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;One book that's freely available is \"The El...</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2014-05-14T00:53:43.273</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>7.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-14T01:25:59.677</td>\n",
              "      <td>26</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>&lt;p&gt;I am sure data science as will be discussed...</td>\n",
              "      <td>66.0</td>\n",
              "      <td>2020-08-16T13:01:33.543</td>\n",
              "      <td>Is Data Science the Same as Data Mining?</td>\n",
              "      <td>&lt;data-mining&gt;&lt;definitions&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2014-06-17T16:17:20.473</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75722</th>\n",
              "      <td>119962</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-04T20:06:06.820</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>&lt;p&gt;I am implementing a neural network of arbit...</td>\n",
              "      <td>147597.0</td>\n",
              "      <td>2023-03-04T20:22:12.523</td>\n",
              "      <td>Back Propagation on arbitrary depth network wi...</td>\n",
              "      <td>&lt;neural-network&gt;&lt;backpropagation&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>147597.0</td>\n",
              "      <td>2023-03-04T20:22:12.523</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75723</th>\n",
              "      <td>119963</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-04T20:12:19.677</td>\n",
              "      <td>0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>&lt;p&gt;I am using KNN for a regression task&lt;/p&gt;\\n&lt;...</td>\n",
              "      <td>147598.0</td>\n",
              "      <td>2023-03-04T20:12:19.677</td>\n",
              "      <td>Evaluation parameter in knn</td>\n",
              "      <td>&lt;regression&gt;&lt;k-nn&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75724</th>\n",
              "      <td>119964</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-05T00:14:12.597</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>&lt;p&gt;I have developed a small encoding algorithm...</td>\n",
              "      <td>44581.0</td>\n",
              "      <td>2023-03-05T00:14:12.597</td>\n",
              "      <td>Can I use zero-padded input and output layers ...</td>\n",
              "      <td>&lt;deep-learning&gt;&lt;convolutional-neural-network&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75725</th>\n",
              "      <td>119965</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-05T00:43:12.213</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>&lt;p&gt;To my understanding, optimizing a model wit...</td>\n",
              "      <td>84437.0</td>\n",
              "      <td>2023-03-05T00:43:12.213</td>\n",
              "      <td>Why does cross validation and hyperparameter t...</td>\n",
              "      <td>&lt;cross-validation&gt;&lt;hyperparameter-tuning&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75726</th>\n",
              "      <td>119966</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-05T03:10:27.593</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>&lt;p&gt;I'm working with a dataset of cars, contain...</td>\n",
              "      <td>147605.0</td>\n",
              "      <td>2023-03-05T03:10:27.593</td>\n",
              "      <td>High metrics value (MAE, MSE, RMSE)</td>\n",
              "      <td>&lt;python&gt;&lt;pandas&gt;&lt;machine-learning-model&gt;&lt;linea...</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75727 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Id  PostTypeId             CreationDate  Score  ViewCount  \\\n",
              "0           5           1  2014-05-13T23:58:30.457      9      898.0   \n",
              "1           7           1  2014-05-14T00:11:06.457      4      478.0   \n",
              "2           9           2  2014-05-14T00:36:31.077      5        NaN   \n",
              "3          10           2  2014-05-14T00:53:43.273     13        NaN   \n",
              "4          14           1  2014-05-14T01:25:59.677     26     1901.0   \n",
              "...       ...         ...                      ...    ...        ...   \n",
              "75722  119962           1  2023-03-04T20:06:06.820      0        8.0   \n",
              "75723  119963           1  2023-03-04T20:12:19.677      0       10.0   \n",
              "75724  119964           1  2023-03-05T00:14:12.597      0        7.0   \n",
              "75725  119965           1  2023-03-05T00:43:12.213      0        5.0   \n",
              "75726  119966           1  2023-03-05T03:10:27.593      0        2.0   \n",
              "\n",
              "                                                    Body  OwnerUserId  \\\n",
              "0      <p>I've always been interested in machine lear...          5.0   \n",
              "1      <p>As a researcher and instructor, I'm looking...         36.0   \n",
              "2      <p>Not sure if this fits the scope of this SE,...         51.0   \n",
              "3      <p>One book that's freely available is \"The El...         22.0   \n",
              "4      <p>I am sure data science as will be discussed...         66.0   \n",
              "...                                                  ...          ...   \n",
              "75722  <p>I am implementing a neural network of arbit...     147597.0   \n",
              "75723  <p>I am using KNN for a regression task</p>\\n<...     147598.0   \n",
              "75724  <p>I have developed a small encoding algorithm...      44581.0   \n",
              "75725  <p>To my understanding, optimizing a model wit...      84437.0   \n",
              "75726  <p>I'm working with a dataset of cars, contain...     147605.0   \n",
              "\n",
              "              LastActivityDate  \\\n",
              "0      2014-05-14T00:36:31.077   \n",
              "1      2014-05-16T13:45:00.237   \n",
              "2      2014-05-14T00:36:31.077   \n",
              "3      2014-05-14T00:53:43.273   \n",
              "4      2020-08-16T13:01:33.543   \n",
              "...                        ...   \n",
              "75722  2023-03-04T20:22:12.523   \n",
              "75723  2023-03-04T20:12:19.677   \n",
              "75724  2023-03-05T00:14:12.597   \n",
              "75725  2023-03-05T00:43:12.213   \n",
              "75726  2023-03-05T03:10:27.593   \n",
              "\n",
              "                                                   Title  \\\n",
              "0      How can I do simple machine learning without h...   \n",
              "1      What open-source books (or other materials) pr...   \n",
              "2                                                   None   \n",
              "3                                                   None   \n",
              "4               Is Data Science the Same as Data Mining?   \n",
              "...                                                  ...   \n",
              "75722  Back Propagation on arbitrary depth network wi...   \n",
              "75723                        Evaluation parameter in knn   \n",
              "75724  Can I use zero-padded input and output layers ...   \n",
              "75725  Why does cross validation and hyperparameter t...   \n",
              "75726                High metrics value (MAE, MSE, RMSE)   \n",
              "\n",
              "                                                    Tags  ...  \\\n",
              "0                                     <machine-learning>  ...   \n",
              "1                               <education><open-source>  ...   \n",
              "2                                                   None  ...   \n",
              "3                                                   None  ...   \n",
              "4                             <data-mining><definitions>  ...   \n",
              "...                                                  ...  ...   \n",
              "75722                  <neural-network><backpropagation>  ...   \n",
              "75723                                 <regression><k-nn>  ...   \n",
              "75724      <deep-learning><convolutional-neural-network>  ...   \n",
              "75725          <cross-validation><hyperparameter-tuning>  ...   \n",
              "75726  <python><pandas><machine-learning-model><linea...  ...   \n",
              "\n",
              "                    ClosedDate  ContentLicense AcceptedAnswerId  \\\n",
              "0      2014-05-14T14:40:25.950    CC BY-SA 3.0              NaN   \n",
              "1      2014-05-14T08:40:54.950    CC BY-SA 3.0             10.0   \n",
              "2                         None    CC BY-SA 3.0              NaN   \n",
              "3                         None    CC BY-SA 3.0              NaN   \n",
              "4                         None    CC BY-SA 3.0             29.0   \n",
              "...                        ...             ...              ...   \n",
              "75722                     None    CC BY-SA 4.0              NaN   \n",
              "75723                     None    CC BY-SA 4.0              NaN   \n",
              "75724                     None    CC BY-SA 4.0              NaN   \n",
              "75725                     None    CC BY-SA 4.0              NaN   \n",
              "75726                     None    CC BY-SA 4.0              NaN   \n",
              "\n",
              "      LastEditorUserId             LastEditDate  ParentId OwnerDisplayName  \\\n",
              "0                  NaN                     None       NaN             None   \n",
              "1                 97.0  2014-05-16T13:45:00.237       NaN             None   \n",
              "2                  NaN                     None       5.0             None   \n",
              "3                  NaN                     None       7.0             None   \n",
              "4                322.0  2014-06-17T16:17:20.473       NaN             None   \n",
              "...                ...                      ...       ...              ...   \n",
              "75722         147597.0  2023-03-04T20:22:12.523       NaN             None   \n",
              "75723              NaN                     None       NaN             None   \n",
              "75724              NaN                     None       NaN             None   \n",
              "75725              NaN                     None       NaN             None   \n",
              "75726              NaN                     None       NaN             None   \n",
              "\n",
              "       CommunityOwnedDate LastEditorDisplayName FavoriteCount  \n",
              "0                    None                  None           NaN  \n",
              "1                    None                  None           NaN  \n",
              "2                    None                  None           NaN  \n",
              "3                    None                  None           NaN  \n",
              "4                    None                  None           NaN  \n",
              "...                   ...                   ...           ...  \n",
              "75722                None                  None           NaN  \n",
              "75723                None                  None           NaN  \n",
              "75724                None                  None           NaN  \n",
              "75725                None                  None           NaN  \n",
              "75726                None                  None           NaN  \n",
              "\n",
              "[75727 rows x 22 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")\n",
        "posts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DcaGAngHLK4g"
      },
      "source": [
        "For our first version of the indexation mechanism, we will simply use the \"body\" of the posts. To have a better search engine, the title and other metadata aswell could be used aswell. Finally, not all the XML files have a \"body\" feature, so for the search engine to retrieve information from any of the files you will need to implement another way to index.\n",
        "\n",
        "But first, let's start with \"body\". There is more to preprocess than before, indeed, there are html tags such as \"<p>\" for instance. They are not useful for us, because users won't use them in their queries. So we first need to remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Et-7VlXyKuaf"
      },
      "outputs": [],
      "source": [
        "def remove_tags(text:str)->str:\n",
        "  return re.sub(r'<[^>]+>', '', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JQAW9pi9MkyZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello World!\\nI am making a search engine.'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "remove_tags('<p>Hello World!\\nI am making a search engine.<p>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lDiFEsPtMszw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19985/2440634092.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clean_posts['Clean Body'] = clean_posts['Body'].fillna('').apply(remove_tags)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Body</th>\n",
              "      <th>Clean Body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n",
              "      <td>I've always been interested in machine learnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>&lt;p&gt;As a researcher and instructor, I'm looking...</td>\n",
              "      <td>As a researcher and instructor, I'm looking fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>&lt;p&gt;Not sure if this fits the scope of this SE,...</td>\n",
              "      <td>Not sure if this fits the scope of this SE, bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>&lt;p&gt;One book that's freely available is \"The El...</td>\n",
              "      <td>One book that's freely available is \"The Eleme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>&lt;p&gt;I am sure data science as will be discussed...</td>\n",
              "      <td>I am sure data science as will be discussed in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75722</th>\n",
              "      <td>119962</td>\n",
              "      <td>&lt;p&gt;I am implementing a neural network of arbit...</td>\n",
              "      <td>I am implementing a neural network of arbitrar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75723</th>\n",
              "      <td>119963</td>\n",
              "      <td>&lt;p&gt;I am using KNN for a regression task&lt;/p&gt;\\n&lt;...</td>\n",
              "      <td>I am using KNN for a regression task\\nIt's lik...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75724</th>\n",
              "      <td>119964</td>\n",
              "      <td>&lt;p&gt;I have developed a small encoding algorithm...</td>\n",
              "      <td>I have developed a small encoding algorithm th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75725</th>\n",
              "      <td>119965</td>\n",
              "      <td>&lt;p&gt;To my understanding, optimizing a model wit...</td>\n",
              "      <td>To my understanding, optimizing a model with k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75726</th>\n",
              "      <td>119966</td>\n",
              "      <td>&lt;p&gt;I'm working with a dataset of cars, contain...</td>\n",
              "      <td>I'm working with a dataset of cars, containing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75727 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Id                                               Body  \\\n",
              "0           5  <p>I've always been interested in machine lear...   \n",
              "1           7  <p>As a researcher and instructor, I'm looking...   \n",
              "2           9  <p>Not sure if this fits the scope of this SE,...   \n",
              "3          10  <p>One book that's freely available is \"The El...   \n",
              "4          14  <p>I am sure data science as will be discussed...   \n",
              "...       ...                                                ...   \n",
              "75722  119962  <p>I am implementing a neural network of arbit...   \n",
              "75723  119963  <p>I am using KNN for a regression task</p>\\n<...   \n",
              "75724  119964  <p>I have developed a small encoding algorithm...   \n",
              "75725  119965  <p>To my understanding, optimizing a model wit...   \n",
              "75726  119966  <p>I'm working with a dataset of cars, contain...   \n",
              "\n",
              "                                              Clean Body  \n",
              "0      I've always been interested in machine learnin...  \n",
              "1      As a researcher and instructor, I'm looking fo...  \n",
              "2      Not sure if this fits the scope of this SE, bu...  \n",
              "3      One book that's freely available is \"The Eleme...  \n",
              "4      I am sure data science as will be discussed in...  \n",
              "...                                                  ...  \n",
              "75722  I am implementing a neural network of arbitrar...  \n",
              "75723  I am using KNN for a regression task\\nIt's lik...  \n",
              "75724  I have developed a small encoding algorithm th...  \n",
              "75725  To my understanding, optimizing a model with k...  \n",
              "75726  I'm working with a dataset of cars, containing...  \n",
              "\n",
              "[75727 rows x 3 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_posts = posts[['Id','Body']]\n",
        "clean_posts['Clean Body'] = clean_posts['Body'].fillna('').apply(remove_tags)\n",
        "clean_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3RbkcTyrNDcJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_19985/2163112268.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clean_posts['words'] = clean_posts['Clean Body'].apply(extract_words)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Body</th>\n",
              "      <th>Clean Body</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n",
              "      <td>I've always been interested in machine learnin...</td>\n",
              "      <td>[ive, always, been, interested, in, machine, l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>&lt;p&gt;As a researcher and instructor, I'm looking...</td>\n",
              "      <td>As a researcher and instructor, I'm looking fo...</td>\n",
              "      <td>[as, a, researcher, and, instructor, im, looki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>&lt;p&gt;Not sure if this fits the scope of this SE,...</td>\n",
              "      <td>Not sure if this fits the scope of this SE, bu...</td>\n",
              "      <td>[not, sure, if, this, fits, the, scope, of, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>&lt;p&gt;One book that's freely available is \"The El...</td>\n",
              "      <td>One book that's freely available is \"The Eleme...</td>\n",
              "      <td>[one, book, thats, freely, available, is, the,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>&lt;p&gt;I am sure data science as will be discussed...</td>\n",
              "      <td>I am sure data science as will be discussed in...</td>\n",
              "      <td>[i, am, sure, data, science, as, will, be, dis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75722</th>\n",
              "      <td>119962</td>\n",
              "      <td>&lt;p&gt;I am implementing a neural network of arbit...</td>\n",
              "      <td>I am implementing a neural network of arbitrar...</td>\n",
              "      <td>[i, am, implementing, a, neural, network, of, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75723</th>\n",
              "      <td>119963</td>\n",
              "      <td>&lt;p&gt;I am using KNN for a regression task&lt;/p&gt;\\n&lt;...</td>\n",
              "      <td>I am using KNN for a regression task\\nIt's lik...</td>\n",
              "      <td>[i, am, using, knn, for, a, regression, task\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75724</th>\n",
              "      <td>119964</td>\n",
              "      <td>&lt;p&gt;I have developed a small encoding algorithm...</td>\n",
              "      <td>I have developed a small encoding algorithm th...</td>\n",
              "      <td>[i, have, developed, a, small, encoding, algor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75725</th>\n",
              "      <td>119965</td>\n",
              "      <td>&lt;p&gt;To my understanding, optimizing a model wit...</td>\n",
              "      <td>To my understanding, optimizing a model with k...</td>\n",
              "      <td>[to, my, understanding, optimizing, a, model, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75726</th>\n",
              "      <td>119966</td>\n",
              "      <td>&lt;p&gt;I'm working with a dataset of cars, contain...</td>\n",
              "      <td>I'm working with a dataset of cars, containing...</td>\n",
              "      <td>[im, working, with, a, dataset, of, cars, cont...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75727 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Id                                               Body  \\\n",
              "0           5  <p>I've always been interested in machine lear...   \n",
              "1           7  <p>As a researcher and instructor, I'm looking...   \n",
              "2           9  <p>Not sure if this fits the scope of this SE,...   \n",
              "3          10  <p>One book that's freely available is \"The El...   \n",
              "4          14  <p>I am sure data science as will be discussed...   \n",
              "...       ...                                                ...   \n",
              "75722  119962  <p>I am implementing a neural network of arbit...   \n",
              "75723  119963  <p>I am using KNN for a regression task</p>\\n<...   \n",
              "75724  119964  <p>I have developed a small encoding algorithm...   \n",
              "75725  119965  <p>To my understanding, optimizing a model wit...   \n",
              "75726  119966  <p>I'm working with a dataset of cars, contain...   \n",
              "\n",
              "                                              Clean Body  \\\n",
              "0      I've always been interested in machine learnin...   \n",
              "1      As a researcher and instructor, I'm looking fo...   \n",
              "2      Not sure if this fits the scope of this SE, bu...   \n",
              "3      One book that's freely available is \"The Eleme...   \n",
              "4      I am sure data science as will be discussed in...   \n",
              "...                                                  ...   \n",
              "75722  I am implementing a neural network of arbitrar...   \n",
              "75723  I am using KNN for a regression task\\nIt's lik...   \n",
              "75724  I have developed a small encoding algorithm th...   \n",
              "75725  To my understanding, optimizing a model with k...   \n",
              "75726  I'm working with a dataset of cars, containing...   \n",
              "\n",
              "                                                   words  \n",
              "0      [ive, always, been, interested, in, machine, l...  \n",
              "1      [as, a, researcher, and, instructor, im, looki...  \n",
              "2      [not, sure, if, this, fits, the, scope, of, th...  \n",
              "3      [one, book, thats, freely, available, is, the,...  \n",
              "4      [i, am, sure, data, science, as, will, be, dis...  \n",
              "...                                                  ...  \n",
              "75722  [i, am, implementing, a, neural, network, of, ...  \n",
              "75723  [i, am, using, knn, for, a, regression, task\\n...  \n",
              "75724  [i, have, developed, a, small, encoding, algor...  \n",
              "75725  [to, my, understanding, optimizing, a, model, ...  \n",
              "75726  [im, working, with, a, dataset, of, cars, cont...  \n",
              "\n",
              "[75727 rows x 4 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_posts['words'] = clean_posts['Clean Body'].apply(extract_words)\n",
        "clean_posts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfNgdg7ziCm"
      },
      "source": [
        "## Zipf Law\n",
        "\n",
        "A way of analyzing a corpus is to draw the zipf law"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Yrts6RVNziLk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "                                           2051547\n",
              "the                                         497900\n",
              "to                                          271649\n",
              "a                                           226675\n",
              "of                                          212782\n",
              "                                            ...   \n",
              "modelcheckpointfilepathbest_weightshdf5          1\n",
              "modeauto\\ncheckpointer                           1\n",
              "min_delta1e8                                     1\n",
              "optimizeropt\\nmonitor                            1\n",
              "004946931214392558\\n\\nam                         1\n",
              "Name: words, Length: 630509, dtype: int64"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Draw Zipf Law on the Posts Corpus\n",
        "import matplotlib.pyplot as plt\n",
        "# get word count for each word\n",
        "word_count = clean_posts['words'].explode().value_counts()\n",
        "word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(word_count.values)\n",
        "plt.xlabel('Word Rank')\n",
        "plt.ylabel('Word Count')\n",
        "# log-log scale\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "plt.title('Zipf Law')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uOYd2qLpwhHE"
      },
      "source": [
        "## Inverted Index\n",
        "\n",
        "Now, we want to go further on the indexing and build an inverted index. Inverted index is a dictionary where the keys are the words of the vocabulary and the values are the documents containing these words. Reducing the size of the vocabulary is a relevant first step when building an inverted index. Here, we will focus on the creation of the index, we leave you the optimisation steps :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-sCV0Ds21g7J"
      },
      "outputs": [],
      "source": [
        "def create_index(posts:pd.DataFrame):\n",
        "  index = {}\n",
        "  for _, row in posts.iterrows():\n",
        "    for word in row['words']:\n",
        "      if word in index:\n",
        "        index[word].add(row['Id'])\n",
        "      else:\n",
        "        index[word] = {row['Id']}\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uCPc3tRMZj9R"
      },
      "outputs": [],
      "source": [
        "# inverted_index = create_index(clean_posts.iloc[0:5000])\n",
        "# # save index to pickle\n",
        "# import pickle\n",
        "# with open('inverted_index.pickle', 'wb') as handle:\n",
        "#     pickle.dump(inverted_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load index from pickle\n",
        "import pickle\n",
        "inverted_index = {}\n",
        "with open('inverted_index.pickle', 'rb') as handle:\n",
        "    inverted_index = pickle.load(handle)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q97D2TjBOVZP"
      },
      "source": [
        "#### Well Done, you've indexed the dataset! \n",
        "Don't hesitate to save your indexes in txt or pickle file\n",
        "\n",
        "---\n",
        "# Implement the search method\n",
        "\n",
        "A naive method would be to count the number of words in common between the query and each posts. Then to rank the posts you could directly select the post who maximize the number of common words. Let's implement this approach :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UZX8J3Vrq7St"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1    1\n",
            "2    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Implement the word_in_index function \n",
        "# Inputs : a word (str) & a list of words\n",
        "# Output : pandas series of 1 if the word is in the list, else 0\n",
        "\n",
        "def word_in_index(word, word_list_index):\n",
        "  return pd.Series([1 if word in words else 0 for words in word_list_index])\n",
        "\n",
        "# test\n",
        "print(word_in_index('cat', [['cat', 'dog'], ['cat', 'mouse'], ['dog', 'mouse']]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GFvxO88LtVi8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   0\n",
            "0  2\n",
            "1  1\n",
            "2  1\n"
          ]
        }
      ],
      "source": [
        "# Implement the function which run through a pandas series and count the number of word in common\n",
        "# Use extract_words method, apply method with word_in_index function\n",
        "# Inputs : the query (str) & pandas series of strings\n",
        "# Output : Pandas series counting the number of common words between the query and each string in word_serie\n",
        "\n",
        "def count_common_words(query, word_serie):\n",
        "  query_words = extract_words(query)\n",
        "  return word_serie.apply(lambda x: sum(word_in_index(word, [x]) for word in query_words))\n",
        "\n",
        "# test\n",
        "print(count_common_words('cat dog', pd.Series([['cat', 'dog'], ['cat', 'mouse'], ['dog', 'mouse']])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "NHzyXeExNWQq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rank_top_query(query, df, top=5):\n",
        "  # get the number of common words between the query and each document\n",
        "  common_words = count_common_words(query, df['words'])\n",
        "  # sort the documents by number of common words\n",
        "  sorted_common_words = common_words.sort_values(by=0, ascending=False)\n",
        "  # return the top documents\n",
        "  return df.iloc[sorted_common_words.index[0:top]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iRdErltStZGv"
      },
      "outputs": [],
      "source": [
        "results = rank_top_query(query=\"testing the query in python\", df=clean_posts, top=5) # prends 1min30 pour tourner\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tl;dr: They markedly differ in many aspects and I can't think Redshift will replace Hadoop.  \n",
            "\n",
            "-Function\n",
            "You can't run anything other than SQL on Redshift. Perhaps most importantly, you can't run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it's more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.\n",
            "\n",
            "-Performance Profile\n",
            "Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I'm using the term indexing very loose here). Therefore, it's great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.\n",
            "\n",
            "-Cost Profile\n",
            "Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon's Elastic Map Reduce). For example, if you are doing OLAP, it's very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.  \n",
            "\n",
            "Having said that, we've replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly for the ease of development. Redshift's Query Engine is based on PostgreSQL and is very mature, compared to Hive's. Its ACID characteristics make it easier to reason about it, and the quicker response time allows more testing to be done. It's a great tool to have, but it won't replace Hadoop.  \n",
            "\n",
            "EDIT:  As for setup complexity, I'd even say it's easier with Hadoop if you use AWS's EMR. Their tools are so mature that it's ridiculously easy to have your Hadoop job running. Tools and mechanisms surrounding Redshift's operation aren't that mature yet. For example, Redshift can't handle trickle loading and thus you have to come up with something that turns that into a batched load, which can add some complexity to your ETL. \n",
            "\n",
            "-------------------\n",
            "The Wolfram Language has a Query function that can traverse data structures and apply functions at different levels of the structure.  I am working with multi-level JSON structures and need a function that has similar functionality as that of Query in the Wolfram Language. \n",
            "\n",
            "Which Python package and function(s) best replicates this?\n",
            "\n",
            "For a minimal working example, say I have the following JSON structure. (String escapes omitted for simplicity)\n",
            "\n",
            "x = {\n",
            "    \"Dims1\":[\n",
            "        {\n",
            "            \"Apple\":{\n",
            "                \"Baking\":[\n",
            "                    \"Pie\",\n",
            "                    \"Tart\"\n",
            "                ],\n",
            "                \"Plant\":\"Tree\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":1.23e1,\n",
            "                    \"2018\":1.12e1\n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"Tomato\":{\n",
            "                \"Cooking\":[\n",
            "                    \"Stew\",\n",
            "                    \"Sauce\"\n",
            "                ],\n",
            "                \"Plant\":\"Vine\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":8.1,\n",
            "                    \"2018\":8.3\n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"Banana\":{\n",
            "                \"Name\":\"Banana\",\n",
            "                \"Baking\":[\n",
            "                    \"Bread\"\n",
            "                ],\n",
            "                \"Cooking\":[\n",
            "                    \"Fried\"\n",
            "                ],\n",
            "                \"Plant\":\"Arborescent\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":0.8,\n",
            "                    \"2018\":0.5\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"Dims2\":[\n",
            "        {\n",
            "            \"Apple\":{\n",
            "                \"Name\":\"Apple\",\n",
            "                \"Baking\":[\n",
            "                    \"Pie\",\n",
            "                    \"Tart\"\n",
            "                ],\n",
            "                \"Plant\":\"Tree\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":1.31e1,\n",
            "                    \"2018\":1.01e1\n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"Sweet Potato\":{\n",
            "                \"Cooking\":[\n",
            "                    \"Fried\",\n",
            "                    \"Steamed\"\n",
            "                ],\n",
            "                \"Baking\":[\n",
            "                    \"Pie\"\n",
            "                ],\n",
            "                \"Plant\":\"Vine\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":1.11e1,\n",
            "                    \"2018\":1.91e1\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "\n",
            "In Wolfram Language I can \n",
            "\n",
            "a = GeneralUtilities`ToAssociations@ImportString[x, \"JSON\"]\n",
            "\n",
            "\n",
            "\n",
            "&lt;|\n",
            " \"Dims1\" -&gt;\n",
            "  {\n",
            "   &lt;|\"Apple\" -&gt;\n",
            "     &lt;|\"Baking\" -&gt; {\"Pie\", \"Tart\"}, \"Plant\" -&gt; \"Tree\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 12.3, \"2018\" -&gt; 11.2|&gt;|&gt;\n",
            "    |&gt;,\n",
            "   &lt;|\"Tomato\" -&gt; \n",
            "     &lt;|\"Cooking\" -&gt; {\"Stew\", \"Sauce\"}, \"Plant\" -&gt; \"Vine\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 8.1, \"2018\" -&gt; 8.3|&gt;|&gt;\n",
            "    |&gt;,\n",
            "   &lt;|\"Banana\" -&gt;\n",
            "     &lt;|\"Name\" -&gt; \"Banana\", \"Baking\" -&gt; {\"Bread\"}, \n",
            "      \"Cooking\" -&gt; {\"Fried\"}, \"Plant\" -&gt; \"Arborescent\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 0.8, \"2018\" -&gt; 0.5|&gt;|&gt;\n",
            "    |&gt;\n",
            "   },\n",
            " \"Dims2\" -&gt;\n",
            "  {\n",
            "   &lt;|\"Apple\" -&gt;\n",
            "     &lt;|\"Name\" -&gt; \"Apple\", \"Baking\" -&gt; {\"Pie\", \"Tart\"}, \n",
            "      \"Plant\" -&gt; \"Tree\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 13.1, \"2018\" -&gt; 10.1|&gt;|&gt;\n",
            "    |&gt;,\n",
            "   &lt;|\"Sweet Potato\" -&gt;\n",
            "     &lt;|\"Cooking\" -&gt; {\"Fried\", \"Steamed\"}, \"Baking\" -&gt; {\"Pie\"}, \n",
            "      \"Plant\" -&gt; \"Vine\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 11.1, \"2018\" -&gt; 19.1|&gt;|&gt;\n",
            "    |&gt;\n",
            "   }\n",
            " |&gt;\n",
            "\n",
            "\n",
            "\n",
            "and then with Query\n",
            "\n",
            "Query[All, All, All, {\"Baking\"}]@a\n",
            "\n",
            "\n",
            "\n",
            "&lt;|\"Dims1\" -&gt; \n",
            "   {&lt;|\"Apple\" -&gt; &lt;|\"Baking\" -&gt; {\"Pie\", \"Tart\"}|&gt;|&gt;, \n",
            "    &lt;|\"Tomato\" -&gt; &lt;|\"Baking\" -&gt; Missing[\"KeyAbsent\", \"Baking\"]|&gt;|&gt;, \n",
            "    &lt;|\"Banana\" -&gt; &lt;|\"Baking\" -&gt; {\"Bread\"}|&gt;|&gt;}, \n",
            "  \"Dims2\" -&gt; \n",
            "   {&lt;|\"Apple\" -&gt; &lt;|\"Baking\" -&gt; {\"Pie\", \"Tart\"}|&gt;|&gt;, \n",
            "    &lt;|\"Sweet Potato\" -&gt; &lt;|\"Baking\" -&gt; {\"Pie\"}|&gt;|&gt;}\n",
            "|&gt;\n",
            "\n",
            "\n",
            "\n",
            "and include functions such as \n",
            "\n",
            "Query[All, Join /* Flatten /* DeleteDuplicates, Values, \"Baking\" /* DeleteMissing]@a\n",
            "\n",
            "\n",
            "\n",
            "&lt;|\"Dims1\" -&gt; {\"Pie\", \"Tart\", \"Bread\"}, \"Dims2\" -&gt; {\"Pie\", \"Tart\"}|&gt;\n",
            "\n",
            "\n",
            "\n",
            "and\n",
            "\n",
            "Query[All, Merge[Total] /* DateListPlot, All, \"Tons\", \n",
            "  KeyMap[DateObject[{FromDigits@#}, \"Year\"] &amp;]]@a\n",
            "\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "How is this done with JSON in Python?\n",
            "\n",
            "-------------------\n",
            "I am studying by now IR system, in the field of valuation of IR system outputs related to a specific query but I need some help to understand it properly. \n",
            "\n",
            "My book states that when an IR system has to be evaluated, we need a test document collection, a set of query examples, a valuation (relevant or not) for each couple of query/document, defined by experts in the field. So we need two measures to know quantitatively if a IR system is good: Precision and Recall. \n",
            "\n",
            "My doubt is related to the following question: Do we use those two measures only if we are testing a IR system or not? \n",
            "\n",
            "I'll explain: before we calculate Precision and Recall related to a specific query example (see above), we need to know how many elements belong to the relevant set, which is impossible if there isn't a valuation (relevant or not) for the query we are using. My book says we can increase Recall in a search engine by using the relevance feedback technique (query expansion and term reweighting): in this case do we assume the Recall value is unknown? \n",
            "\n",
            "For example, everyday many documents are shared on the Internet and Google can find them. So it is impossibile to apply Recall and Precision to this scenario, in which information grows and there is no valuation for every new document for each specific query. It is also impossibile to predict all the possibile queries a user can do on a search engine.\n",
            "\n",
            "-------------------\n",
            "This is how I would do it. However, a DataFrame can be structured in a number of way which best suits your needs. I believe that this method allows for the greatest flexibility because you can easily use grouping functions to restructure this format on the go. \n",
            "\n",
            "First you need to set up your data in a way that is compatible with Python. I use a dictionary of dictionaries\n",
            "\n",
            "cities = {'Lovell_Wyoming':\n",
            "          {'Crimes':\n",
            "           {\n",
            "            'Arson': [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "            'Assaults': [6, 6, 3, 4, 3, 28, 3, 2, 2] ,\n",
            "            'Auto_thefts': [1, 1, 1, 0, 0, 1, 2, 0, 1] ,\n",
            "            'Burglaries': [6, 11, 5, 2, 0, 15, 11, 7, 7] ,\n",
            "            'Murders': [0, 0, 0, 0, 1, 0, 0, 0, 0] ,\n",
            "            'Rapes': [0, 0, 3, 0, 0, 1, 1, 0, 1] ,\n",
            "            'Robberies': [0, 0, 0, 0, 0, 0, 0, 1, 0] ,\n",
            "            'Thefts': [23, 49, 35, 39, 28, 37, 54, 35, 10]\n",
            "           },\n",
            "            'Years': [2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2014]\n",
            "          },\n",
            "\n",
            "         'Wheatland_Wyoming':\n",
            "          {'Crimes':\n",
            "           {\n",
            "            'Arson': [0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0] ,\n",
            "            'Assaults': [9, 2, 6, 5, 6, 6, 2, 4, 2, 4, 3, 11, 5, 4, 8] ,\n",
            "            'Auto_thefts': [4, 8, 3, 3, 4, 4, 5, 3, 4, 6, 4, 8, 12, 7, 3] ,\n",
            "            'Burglaries': [17, 17, 14, 9, 10, 17, 12, 26, 51, 12, 15, 21, 32, 31, 13] ,\n",
            "            'Murders': [1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0] ,\n",
            "            'Rapes': [0, 0, 0, 4, 2, 1, 2, 0, 2, 1, 1, 0, 2, 0, 0] ,\n",
            "            'Robberies': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ,\n",
            "            'Thefts': [109, 95, 146, 81, 108, 100, 82, 85, 106, 128, 48, 85, 66, 56, 47]\n",
            "            },\n",
            "              'Years': [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]\n",
            "            }\n",
            "         }\n",
            "\n",
            "\n",
            "Then compile this data into rows of your DataFrame\n",
            "\n",
            "data = []\n",
            "for city in cities:\n",
            "    for ix, year in enumerate(cities[city]['Years']):\n",
            "        for crime in cities[city]['Crimes']:\n",
            "            temp = {'City':city, \n",
            "                    'Crime':crime, \n",
            "                    'Year':year, \n",
            "                    'Count':cities[city]['Crimes'][crime][ix]}\n",
            "            data.append(temp)\n",
            "\n",
            "\n",
            "Then into the DataFrame structure as.\n",
            "\n",
            "import pandas as pd\n",
            "df = pd.DataFrame(data=data)\n",
            "df\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Queries\n",
            "\n",
            "You can then query this DataFrame in a number of ways for example if you want to know arsons in 2002 you would do \n",
            "\n",
            "df[(df['Crime']=='Arson') &amp; (df['Year']==2002)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Counting\n",
            "\n",
            "You can count the number of arsons throughout the years as\n",
            "\n",
            "df[(df['Crime']=='Arson')].groupby(['City'])['Count'].agg('sum')\n",
            "\n",
            "\n",
            "\n",
            "  City Lovell_Wyoming       1 Wheatland_Wyoming    3 Name: Count, dtype:\n",
            "  int64\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Writing your DataFrame to a CSV file\n",
            "\n",
            "This can be done directly as \n",
            "\n",
            "df.to_csv('filename.csv')\n",
            "\n",
            "\n",
            "-------------------\n",
            "We all know that with the use of sklearn package from python, we can create X_train, X_test, y_train and y_test via this code:\n",
            "\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
            "\n",
            "\n",
            "I want to make sure that each training and testing phase of a multi-class data set, have 66.33/33.33 percent of each class values so the prediction and accuracy would get better. All i want is 66.33 percent of class A in training set and 33.33 percent of Class A in test set. And, so on for other classes, like B, C, D and etc. in a given multi-class data set.\n",
            "\n",
            "Is the code provided enough to achieve this or should i write extra code?\n",
            "\n",
            "Thanks\n",
            "\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "# print(results)\n",
        "for _, row in results.iterrows():\n",
        "    print(row['Clean Body'])\n",
        "    print('-------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pros:\n",
        "# - easy to implement\n",
        "# - fast to compute (1min30 par requete, bon...)\n",
        "# cons:\n",
        "# - gives the same weight to all words, even common words like \"the\" or \"is\"\n",
        "# - doesn't take into account the order of the words in the query\n",
        "# - doesn't take into account the order of the words in the documents\n",
        "# - doesn't take into account the number of times a word appears in a document\n",
        "# - doesn't take into account the number of times a word appears in the corpus\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JumHiP3txgUb"
      },
      "source": [
        "Testez plusieurs requêtes et critiquez les résultats obtenus.\n",
        "\n",
        "Quels sont les pros and cons de cette méthodes. Vous l'indiquerez sur le rapport avec vos réflexions pour l'améliorer.\n",
        "\n",
        "Next, you have to implement the first improvements you find in the search method to get most relevant results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/himmi/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "nltk.download('stopwords')\n",
        "stop_words =  stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ux77Xzftx-kX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cat', 'mat']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def remove_stop_words(l_txt: list) -> list:\n",
        "    return [word for word in l_txt if word not in stop_words]\n",
        "\n",
        "# test\n",
        "print(remove_stop_words(['the', 'cat', 'is', 'on', 'the', 'mat'])) # ['cat', 'mat']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MecCCwzbx8qZ"
      },
      "source": [
        "## Boolean Search\n",
        "\n",
        "Thanks to the ttable library, implement a boolean search method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "q5JTNdIrVdH9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This isn't a full solution, but you may want to look into OrientDB as part of your stack. Orient is a Graph-Document database server written entirely in Java. \n",
            "\n",
            "In graph databases, relationships are considered first class citizens and therefore traversing those relationships can be done pretty quickly. Orient is also a document database which would allow you the kind of schema-free architecture it sounds like you would need. The real reason I suggest Orient, however, is because of its extensiblity. It supports streaming via sockets, and the entire database can be embedded into another application. Finally, it can be scaled efficiently and/or can work entirely through memory. So, with some Java expertise, you can actually run your preset queries against the database in memory.\n",
            "\n",
            "We are doing something similar. In creating an app/site for social science research collaboration, we found ourselves with immensely complex data models. We ended up writing several of the queries using the Gremlin Traversal Language (a subset of Groovy, which is, of course, Java at its heart), and then exposing those queries through the binary connection server of the OrientDB. So, the client opens a TCP socket, sends a short binary message, and the query is executing in Java directly against the in-memory database.\n",
            "\n",
            "OrientDB also supports writing function queries in Javascript, and you can use Node.js to interact directly with an Orient instance.\n",
            "\n",
            "For something of this size, I would want to use Orient in conjunction with Hadoop or something like that. You can also use Orient in conjunction with esper.\n",
            "\n",
            "Consider:\n",
            "An introduction to orient: http://www.sitepoint.com/a-look-at-orientdb-the-graph-document-nosql/\n",
            "\n",
            "Complex, real-time queries: http://www.gft-blog.com/business-trends/leveraging-real-time-scoring-through-bigdata-to-detect-insurance-fraud/\n",
            "\n",
            "A discussion about streaming options with java and orient: https://github.com/orientechnologies/orientdb/issues/1227\n",
            "\n",
            "-------------------\n",
            "As we all know, there are some data indexing techniques, using by well-known indexing apps, like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc. For a No-Sql / Object Oriented Database (which I try to write/play a little around with C#), which technique you suggest?\n",
            "\n",
            "I read about MurMurhash-2 and specially v3 comments say Murmur is very fast. Also Lucene.Net has good comments on it. But what about their memory footprints in general? Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur? Or should I write a special index structure to get the best results?\n",
            "\n",
            "If I try to write my own, then is there any accepted scale for a good indexing, something like 1% of data-node, or 5% of data-node? Any useful hint will be appreciated.\n",
            "\n",
            "-------------------\n",
            "You have three questions to answer and 100 records per month to analyze.\n",
            "\n",
            "Based on this size, I'd recommend doing analysis in a simple SQL database or a spreadsheet to start off with.  The first two questions are fairly easy to figure out.  The third is a little more difficult.\n",
            "\n",
            "I'd definitely add a column for month and group all of that data together into a spreadsheet or database table given the questions you want to answer.\n",
            "\n",
            "question 1. Users who are consistently showing up in this list\n",
            "\n",
            "In excel, this answer should help you out:  https://superuser.com/questions/442653/ms-excel-how-count-occurence-of-item-in-a-list\n",
            "\n",
            "For a SQL database:  https://stackoverflow.com/questions/2516546/select-count-duplicates\n",
            "\n",
            "question 2. Users who are consistently showing up in this list with high risk score \n",
            "\n",
            "This is just adding a little complexity to the above.  For SQL, you would further qualify your query based on a minimum risk score value.\n",
            "\n",
            "In excel, a straight pivot isn't going to work, you'll have to copy the unique values in one column to another, then drag a CountIf function adjacent to each unique value, qualifying the countif function with a minimum risk score.\n",
            "\n",
            "question 3. Users who have/reaching the high risk level very fast.\n",
            "\n",
            "A fast rise in risk level could be defined as the difference between two months being larger than a given value.\n",
            "\n",
            "For each user record you want to know the previous month's threat value, or assume zero as the previous threat value.\n",
            "\n",
            "If that difference is greater than your risk threshold, you want to include it in your report.  If not, they can be filtered from the list.  \n",
            "\n",
            "If I had to do this month after month, I would spend the two hours it might take to automate a report after the first couple of months.  I'd throw all the data in a SQL database and write a quick script in perl or java to iterate through the 100 records, do the calculation, and output the users who crossed the threshold.\n",
            "\n",
            "If I needed it to look pretty, I'd use a reporting tool.  I'm not particularly partial to any of them.\n",
            "\n",
            "If I needed to trend threshold values over time, I'd output the results for all people into a second table add records to that table each month.\n",
            "\n",
            "If I just needed to do it once or twice, figuring out how to do it in excel by adding a new column using VLookUp and some basic math and a filter would probably be the fastest and easiest way to get it done.  I tend to avoid using excel for things I'll need to use with consistency because there are limits that you run into when your data gets sizeable.\n",
            "\n",
            "-------------------\n",
            "Check out this link.\n",
            "\n",
            "Here, they will take you through loading unstructured text to creating a wordcloud.  You can adapt this strategy and instead of creating a wordcloud, you can create a frequency matrix of terms used.  The idea is to take the unstructured text and structure it somehow.  You change everything to lowercase (or uppercase), remove stop words, and find frequent terms for each job function, via Document Term Matrices.  You also have the option of stemming the words.  If you stem words you will be able to detect different forms of words as the same word.  For example, 'programmed' and 'programming' could be stemmed to 'program'.  You can possibly add the occurrence of these frequent terms as a weighted feature in your ML model training.\n",
            "\n",
            "You can also adapt this to frequent phrases, finding common groups of 2-3 words for each job function.\n",
            "\n",
            "Example:\n",
            "\n",
            "1) Load libraries and build the example data\n",
            "\n",
            "library(tm)\n",
            "library(SnowballC)\n",
            "\n",
            "doc1 = \"I am highly skilled in Java Programming.  I have spent 5 years developing bug-tracking systems and creating data managing system applications in C.\"\n",
            "job1 = \"Software Engineer\"\n",
            "doc2 = \"Tested new software releases for major program enhancements.  Designed and executed test procedures and worked with relational databases.  I helped organize and lead meetings and work independently and in a group setting.\"\n",
            "job2 = \"Quality Assurance\"\n",
            "doc3 = \"Developed large and complex web applications for client service center. Lead projects for upcoming releases and interact with consumers.  Perform database design and debugging of current releases.\"\n",
            "job3 = \"Software Engineer\"\n",
            "jobInfo = data.frame(\"text\" = c(doc1,doc2,doc3),\n",
            "                     \"job\" = c(job1,job2,job3))\n",
            "\n",
            "\n",
            "2) Now we do some text structuring. I am positive there are quicker/shorter ways to do the following.\n",
            "\n",
            "# Convert to lowercase\n",
            "jobInfo$text = sapply(jobInfo$text,tolower)\n",
            "\n",
            "# Remove Punctuation\n",
            "jobInfo$text = sapply(jobInfo$text,function(x) gsub(\"[[:punct:]]\",\" \",x))\n",
            "\n",
            "# Remove extra white space\n",
            "jobInfo$text = sapply(jobInfo$text,function(x) gsub(\"[ ]+\",\" \",x))\n",
            "\n",
            "# Remove stop words\n",
            "jobInfo$text = sapply(jobInfo$text, function(x){\n",
            "  paste(setdiff(strsplit(x,\" \")[[1]],stopwords()),collapse=\" \")\n",
            "})\n",
            "\n",
            "# Stem words (Also try without stemming?)\n",
            "jobInfo$text = sapply(jobInfo$text, function(x)  {\n",
            "  paste(setdiff(wordStem(strsplit(x,\" \")[[1]]),\"\"),collapse=\" \")\n",
            "})\n",
            "\n",
            "\n",
            "3) Make a corpus source and document term matrix.\n",
            "\n",
            "# Create Corpus Source\n",
            "jobCorpus = Corpus(VectorSource(jobInfo$text))\n",
            "\n",
            "# Create Document Term Matrix\n",
            "jobDTM = DocumentTermMatrix(jobCorpus)\n",
            "\n",
            "# Create Term Frequency Matrix\n",
            "jobFreq = as.matrix(jobDTM)\n",
            "\n",
            "\n",
            "Now we have the frequency matrix, jobFreq, that is a (3 by x) matrix, 3 entries and X number of words.\n",
            "\n",
            "Where you go from here is up to you.  You can keep only specific (more common) words and use them as features in your model.  Another way is to keep it simple and have a percentage of words used in each job description, say \"java\" would have 80% occurrence in 'software engineer' and only 50% occurrence in 'quality assurance'.\n",
            "\n",
            "Now it's time to go look up why 'assurance' has 1 'r' and 'occurrence' has 2 'r's.\n",
            "\n",
            "-------------------\n",
            "I am a relatively new user to Hadoop (using version 2.4.1). I installed hadoop on my first node without a hitch, but I can't seem to get the Resource Manager to start on my second node. \n",
            "\n",
            "I cleared up some \"shared library\" problems by adding this to yarn-env.sh and hadoop-env.sh:\n",
            "\n",
            "\n",
            "  export HADOOP_HOME=\"/usr/local/hadoop\"\n",
            "  \n",
            "  export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"\n",
            "\n",
            "\n",
            "I also added this to hadoop-env.sh:\n",
            "\n",
            "\n",
            "  export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native\n",
            "\n",
            "\n",
            "based on the advice of this post at horton works http://hortonworks.com/community/forums/topic/hdfs-tmp-dir-issue/ \n",
            "\n",
            "That cleared up all of my error messages; when I run /sbin/start-yarn.sh I get this:\n",
            "\n",
            "\n",
            "  starting yarn daemons\n",
            "  \n",
            "  starting resourcemanager, \n",
            "  logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-HdNode.out\n",
            "  \n",
            "  localhost: starting nodemanager, \n",
            "  logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-HdNode.out\n",
            "\n",
            "\n",
            "The only problem is, JPS says that the Resource Manager isn't running. \n",
            "\n",
            "What's going on here?\n",
            "\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "from tt import BooleanExpression\n",
        "\n",
        "def boolean_search(query, df=clean_posts):\n",
        "  # get the words in the query\n",
        "  expression =  BooleanExpression(query)\n",
        "  # get the posts whose clean body that satisfy the expression\n",
        "  symbols = expression.symbols\n",
        "  # for each post and for each symbol, check if the symbol is in the post, then evaluate the expression\n",
        "  bools = df['Clean Body'].apply(lambda x: [symbol in x for symbol in symbols]).apply(lambda x: expression.evaluate(**dict(zip(symbols,x))))\n",
        "  # return all documents that satisfy the expression\n",
        "  return df.iloc[bools[bools].index]\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  # return the top documents\n",
        "\n",
        "results = boolean_search('java AND NOT python')\n",
        "\n",
        "for _, row in results[0:5].iterrows():\n",
        "    print(row['Clean Body'])\n",
        "    print('-------------------')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N4vFldsAycpB"
      },
      "source": [
        "## Probabilistic search\n",
        "\n",
        "Implement the MIB or BM25 method of searching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           Id                                               Body  \\\n",
            "0           5  <p>I've always been interested in machine lear...   \n",
            "1           7  <p>As a researcher and instructor, I'm looking...   \n",
            "2           9  <p>Not sure if this fits the scope of this SE,...   \n",
            "3          10  <p>One book that's freely available is \"The El...   \n",
            "4          14  <p>I am sure data science as will be discussed...   \n",
            "...       ...                                                ...   \n",
            "75722  119962  <p>I am implementing a neural network of arbit...   \n",
            "75723  119963  <p>I am using KNN for a regression task</p>\\n<...   \n",
            "75724  119964  <p>I have developed a small encoding algorithm...   \n",
            "75725  119965  <p>To my understanding, optimizing a model wit...   \n",
            "75726  119966  <p>I'm working with a dataset of cars, contain...   \n",
            "\n",
            "                                              Clean Body  \\\n",
            "0      I've always been interested in machine learnin...   \n",
            "1      As a researcher and instructor, I'm looking fo...   \n",
            "2      Not sure if this fits the scope of this SE, bu...   \n",
            "3      One book that's freely available is \"The Eleme...   \n",
            "4      I am sure data science as will be discussed in...   \n",
            "...                                                  ...   \n",
            "75722  I am implementing a neural network of arbitrar...   \n",
            "75723  I am using KNN for a regression task\\nIt's lik...   \n",
            "75724  I have developed a small encoding algorithm th...   \n",
            "75725  To my understanding, optimizing a model with k...   \n",
            "75726  I'm working with a dataset of cars, containing...   \n",
            "\n",
            "                                                   words  \n",
            "0      [ive, always, been, interested, in, machine, l...  \n",
            "1      [as, a, researcher, and, instructor, im, looki...  \n",
            "2      [not, sure, if, this, fits, the, scope, of, th...  \n",
            "3      [one, book, thats, freely, available, is, the,...  \n",
            "4      [i, am, sure, data, science, as, will, be, dis...  \n",
            "...                                                  ...  \n",
            "75722  [i, am, implementing, a, neural, network, of, ...  \n",
            "75723  [i, am, using, knn, for, a, regression, task\\n...  \n",
            "75724  [i, have, developed, a, small, encoding, algor...  \n",
            "75725  [to, my, understanding, optimizing, a, model, ...  \n",
            "75726  [im, working, with, a, dataset, of, cars, cont...  \n",
            "\n",
            "[75727 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "print(clean_posts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bCxjyrldyhUB"
      },
      "outputs": [],
      "source": [
        "def proba_search(query, df=clean_posts, top=5):\n",
        "  # each document get a score\n",
        "  # OKAPI model (BM25)\n",
        "  # print('query : ', query)\n",
        "  # print('top : ', top)\n",
        "  query_words = extract_words(query)\n",
        "  k1 = 1.2\n",
        "  b = 0.75\n",
        "  k3 = 1000\n",
        "  # average length of a document\n",
        "  m = df['Clean Body'].apply(lambda x: len(x)).mean()\n",
        "  N = len(df)\n",
        "  RSV_score = {}\n",
        "  # for each post in df :\n",
        "  for _, row in df.iterrows():\n",
        "    # sum over all words in the query and in the post\n",
        "    # length of the post\n",
        "    Ld = len(row['Clean Body'])\n",
        "    # term frequency in the query\n",
        "    def tf(word):\n",
        "      return sum([1 for w in query_words if w == word])\n",
        "    def d_f(word):\n",
        "      if word not in inverted_index:\n",
        "        #print(word)\n",
        "        return 0\n",
        "      else:\n",
        "        return len(inverted_index[word])\n",
        "    RSV_score[row['Id']] = sum([(k1+1)*tf(word)/(k1*((1-b)+b*Ld/m)+tf(word))*(k3+1)*tf(word)/(k3+tf(word))*np.log((N-d_f(word)+0.5)/(d_f(word)+0.5)) for word in query_words if word in row['words']])\n",
        "  # return the top Ids of the posts from RSV\n",
        "  sorted_keys = sorted(RSV_score, key=RSV_score.get, reverse=True) # the Id column in the best order\n",
        "  # the values of sorted_keys are values of df[\"Id\"]\n",
        "  # return the top documents in the same order as sorted_keys\n",
        "  new_df = df.copy()\n",
        "  new_df['RSV_score'] = new_df['Id'].apply(lambda x: RSV_score[x])\n",
        "  new_df = new_df.sort_values(by='RSV_score', ascending=False)\n",
        "  return new_df[new_df[\"Id\"].isin(sorted_keys[0:top])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "begin\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "print(\"begin\")\n",
        "results = proba_search('measure performance for multi classification model',top=len(clean_posts))\n",
        "# reset index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          Id                                               Body  \\\n",
            "42084  65228  <p>AUC - ROC curve is a performance measuremen...   \n",
            "29920  46296  <p>Confusion matrix is generally not considere...   \n",
            "60685  96726  <p>Though it does not exactly measure uncertai...   \n",
            "1944    5233  <p>Please I want to know if there is any SVM R...   \n",
            "51735  80531  <p>I'm interested in <a href=\"https://debug-ml...   \n",
            "...      ...                                                ...   \n",
            "23111  36656  <p>I am wondering why do we use scaling on tra...   \n",
            "23113  36658  <p>train and test datasets should have no over...   \n",
            "23115  36661  <p>I'm trying to mine some data with Rstudio, ...   \n",
            "55729  87556  <p>It seems that your target variable (died or...   \n",
            "0          5  <p>I've always been interested in machine lear...   \n",
            "\n",
            "                                              Clean Body  \\\n",
            "42084  AUC - ROC curve is a performance measurement f...   \n",
            "29920  Confusion matrix is generally not considered a...   \n",
            "60685  Though it does not exactly measure uncertainty...   \n",
            "1944   Please I want to know if there is any SVM R pa...   \n",
            "51735  I'm interested in model debugging and one of t...   \n",
            "...                                                  ...   \n",
            "23111  I am wondering why do we use scaling on train ...   \n",
            "23113  train and test datasets should have no overlap...   \n",
            "23115  I'm trying to mine some data with Rstudio, but...   \n",
            "55729  It seems that your target variable (died or no...   \n",
            "0      I've always been interested in machine learnin...   \n",
            "\n",
            "                                                   words  RSV_score  \n",
            "42084  [auc, , roc, curve, is, a, performance, measur...  29.683522  \n",
            "29920  [confusion, matrix, is, generally, not, consid...  29.627685  \n",
            "60685  [though, it, does, not, exactly, measure, unce...  29.572323  \n",
            "1944   [please, i, want, to, know, if, there, is, any...  29.533097  \n",
            "51735  [im, interested, in, model, debugging, and, on...  28.400729  \n",
            "...                                                  ...        ...  \n",
            "23111  [i, am, wondering, why, do, we, use, scaling, ...   0.000000  \n",
            "23113  [train, and, test, datasets, should, have, no,...   0.000000  \n",
            "23115  [im, trying, to, mine, some, data, with, rstud...   0.000000  \n",
            "55729  [it, seems, that, your, target, variable, died...   0.000000  \n",
            "0      [ive, always, been, interested, in, machine, l...   0.000000  \n",
            "\n",
            "[75727 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "print(results)\n",
        "\n",
        "# for _, row in results.iterrows():\n",
        "#     print(row['Clean Body'])\n",
        "#     print('-------------------')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x9m_LFo_yog2"
      },
      "source": [
        "Compare the naive method with your improvements and the boolean and probabilistic search. (report)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "arfKMWtLyyxY"
      },
      "source": [
        "# Evaluate the Search\n",
        "\n",
        "Now you implement multiple search methods and you're able to improve it. You have to define metric to compare it objectively.\n",
        "\n",
        "\n",
        "\n",
        "We ask you to implement NDCG (Normalized Discounted Cumulative Gain) from few queries we implement on a dozen of post. We already defined the values of relevance judgement in the xlsx file : . The final score will be the mean quadratic error of the queries.\n",
        "\n",
        "\n",
        "Explication for the xlsx file :\n",
        "\n",
        "We propose you a Excel file with some posts and a mesure of relevancy for the queries\n",
        "\n",
        "- First column is the post Id,\n",
        "- Columns starting by query are the queries you have to test.\n",
        "- The values in this columns are the rank of relevancy of the post in regard with the query.\n",
        "- The missing values indicates you should not take into account the post\n",
        "\n",
        "\n",
        "You will have to criticize this metric and your result in the report. Then you will have to propose some improvements. \n",
        "\n",
        "Thereafter in this week, you will have to compare your different search engines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ZxCcftBPagMQ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PostId</th>\n",
              "      <th>Title</th>\n",
              "      <th>First Sentence</th>\n",
              "      <th>Query 1 : mesure performance for multiclassification model</th>\n",
              "      <th>Query 2 : draw neural network</th>\n",
              "      <th>Query 3 : neural network layers</th>\n",
              "      <th>Query 4 : how sklearn working</th>\n",
              "      <th>Query 5 : treat categorical data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6107</td>\n",
              "      <td>What are deconvolutional layers?</td>\n",
              "      <td>I recently read Fully Convolutional Networks f...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15989</td>\n",
              "      <td>Micro Average vs Macro average Performance in ...</td>\n",
              "      <td>I am trying out a multiclass classification se...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13490</td>\n",
              "      <td>How to set class weights for imbalanced classe...</td>\n",
              "      <td>I know that there is a possibility in Keras wi...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12321</td>\n",
              "      <td>What's the difference between fit and fit_tran...</td>\n",
              "      <td>I do not understand the difference between the...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>22</td>\n",
              "      <td>K-Means clustering for mixed numeric and categ...</td>\n",
              "      <td>My data set contains a number of numeric attri...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PostId                                              Title  \\\n",
              "0    6107                   What are deconvolutional layers?   \n",
              "1   15989  Micro Average vs Macro average Performance in ...   \n",
              "2   13490  How to set class weights for imbalanced classe...   \n",
              "3   12321  What's the difference between fit and fit_tran...   \n",
              "4      22  K-Means clustering for mixed numeric and categ...   \n",
              "\n",
              "                                      First Sentence  \\\n",
              "0  I recently read Fully Convolutional Networks f...   \n",
              "1  I am trying out a multiclass classification se...   \n",
              "2  I know that there is a possibility in Keras wi...   \n",
              "3  I do not understand the difference between the...   \n",
              "4  My data set contains a number of numeric attri...   \n",
              "\n",
              "   Query 1 : mesure performance for multiclassification model  \\\n",
              "0                                                NaN            \n",
              "1                                                1.0            \n",
              "2                                                3.0            \n",
              "3                                                NaN            \n",
              "4                                                NaN            \n",
              "\n",
              "   Query 2 : draw neural network  Query 3 : neural network layers  \\\n",
              "0                            NaN                              1.0   \n",
              "1                            NaN                              NaN   \n",
              "2                            NaN                              NaN   \n",
              "3                            NaN                              NaN   \n",
              "4                            NaN                              NaN   \n",
              "\n",
              "   Query 4 : how sklearn working  Query 5 : treat categorical data  \n",
              "0                            NaN                               NaN  \n",
              "1                            NaN                               NaN  \n",
              "2                            NaN                               NaN  \n",
              "3                            1.0                               3.0  \n",
              "4                            5.0                               2.0  "
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read Relevancy CSV\n",
        "df_relevancy = pd.read_excel(\"evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\")\n",
        "df_relevancy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "FVxld-Ujy0nN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[52280, 61610, 59518, 2972, 34327, 1218, 43356, 61420, 58965, 57523, 8702, 32997]\n",
            "[50229, 55117, 54897, 57351, 52459, 6, 2086, 55640, 3639, 3495, 6685, 50923]\n",
            "[9364, 55648, 54954, 57381, 52656, 4432, 2843, 56236, 4871, 4690, 8066, 51285]\n",
            "[20648, 63475, 30767, 32595, 38804, 68232, 36677, 4941, 28784, 40636, 13493, 43362]\n",
            "[58137, 14438, 60138, 201, 444, 61764, 19335, 4704, 60937, 57379, 55025, 388]\n",
            "[nan, 1.0, 3.0, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n",
            "[nan, nan, nan, nan, nan, 2.0, 4.0, nan, 1.0, 3.0, 5.0, nan]\n",
            "[1.0, nan, nan, nan, nan, 5.0, 2.0, nan, 4.0, 3.0, 6.0, nan]\n",
            "[nan, nan, nan, 1.0, 5.0, nan, nan, 2.0, nan, 3.0, 4.0, 7.0]\n",
            "[nan, nan, nan, 3.0, 2.0, nan, nan, nan, nan, nan, 4.0, 1.0]\n",
            "{3: 0.7907373106010506, 4: 0.9999997278906044, 5: 0.999999024471228, 6: 0.8753479921847879, 7: 0.9860441124725677}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "    \"\"\"\n",
        "    Calculates the NDCG (Normalized Discounted Cumulative Gain) score for each query in the given relevance dataframe\n",
        "    using the specified search method.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df_relevancy : pandas.DataFrame\n",
        "        A dataframe containing the relevance scores for each post and query.\n",
        "        The first column should contain the post IDs, and the remaining columns from the 4th should be named 'query X',\n",
        "        where X is the query number starting from 1.\n",
        "        The values in the columns should be the relevance scores for each post with respect to the corresponding query.\n",
        "    method : function\n",
        "        The search method to use for retrieving the top documents for each query.\n",
        "        The function should take a query string as input and return a pandas.DataFrame containing the top documents.\n",
        "    top : int, optional\n",
        "        The number of top documents to retrieve for each query.\n",
        "        The default value is the total number of posts in the dataset.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        A dictionary containing the NDCG score for each query.\n",
        "        The keys are the query numbers starting from 1, and the values are the corresponding NDCG scores.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "def get_ndcg_scores(df_relevancy,method=proba_search,top=len(clean_posts)):\n",
        "    # for each PostId in the relevancy dataframe, get the rank of the post according to the method\n",
        "    querys = {}\n",
        "    for i in range(3, 8):\n",
        "        querys[i] = df_relevancy.columns[i][10:]\n",
        "    method_results = {}\n",
        "    for i in range(3, 8):\n",
        "        method_results[i] = method(query=querys[i],top=top)\n",
        "        #print(method_results[i].head())\n",
        "    # get the score of each post according to the method\n",
        "    rel_preds = {}\n",
        "    for i in range(3, 8):\n",
        "        # the rankings of the posts in PostId\n",
        "        rel_preds[i] = [list(method_results[i][\"Id\"]).index(x) if x in method_results[i][\"Id\"] else top for x in df_relevancy['PostId']]\n",
        "        #print(rel_preds[i])\n",
        "    rel_trues = {}\n",
        "    for i in range(3, 8):\n",
        "        # the rankings of the posts\n",
        "        rel_trues[i] = df_relevancy[df_relevancy.columns[i]].tolist()\n",
        "        #print(rel_trues[i])\n",
        "        rel_trues[i] = [top if np.isnan(x) else x for x in rel_trues[i]]\n",
        "\n",
        "    # calculate the ndcg score for each query\n",
        "    ndcg_scores = {}\n",
        "    for i in range(3, 8):\n",
        "        ndcg_scores[i] = ndcg_score([rel_trues[i]], [rel_preds[i]])\n",
        "    return ndcg_scores\n",
        "    \n",
        "\n",
        "print(get_ndcg_scores(df_relevancy,method=proba_search))\n",
        "   \n",
        "# rel_pred = \n",
        "\n",
        "\n",
        "# table of ndcg for each query\n",
        "# ndcg_table = pd.DataFrame(columns=['Query', 'NDCG'])\n",
        "# for i in range(4, 8):\n",
        "  # ndcg_table = ndcg_table.append({'Query': f'Query {i-3}', 'NDCG': calculate_ndgc(df_relevancy.columns[i])}, ignore_index=True)\n",
        "\n",
        "# print(ndcg_table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
