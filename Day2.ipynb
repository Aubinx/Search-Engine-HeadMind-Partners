{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valentingorce/tp_centrale/blob/main/Day2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiy0xFlZLomY"
      },
      "source": [
        "# Web Information Retrieval\n",
        "## Introduction to search engines\n",
        "\n",
        "### DAY 2: Teacher version\n",
        "### Implementing a search engine\n",
        "\n",
        "The goal of this second session is to implement a first architecture of a search engine on the previously introduced dataset (stackexchange-datascience). If you missed the first session or if you did not saved the dataset, please reload the first session's notebook to download it. \n",
        "\n",
        "If you need some ifnormation about the dataset, it should be available here : https://archive.org/details/stackexchange\n",
        "\n",
        "The notebook is divided into several steps:\n",
        "-\tImplement the indexation\n",
        "-\tImplement the search method\n",
        "-\tDefine a ranking strategy and implement it\n",
        "-\tSuggest some improvements of the search engine\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QH_WrILaSIJL"
      },
      "source": [
        "## Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0H819R7iJF_y"
      },
      "outputs": [],
      "source": [
        "# !pip install ttable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "lI2VFiG1SJmJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tt import BooleanExpression\n",
        "from itertools import product\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YySiGfQ1SNwT"
      },
      "outputs": [],
      "source": [
        "# # Only if you use Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "0rq6fLsSSPUn"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = 'datascience.stackexchange.com/'\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4kiMuicy6Du8"
      },
      "source": [
        "**Important :**\n",
        "\n",
        "An Excel file for testing the evaluation part is available in the gitlab repo : evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\n",
        "\n",
        "If you work on Colab, we advice you to push it directly on your Google Drive directory."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PzqYFBxYAJN1"
      },
      "source": [
        "# Implement the indexation\n",
        "As you might already know, for a search engine to work properly an index of the documents must be created. Here we will keep it in python, and try to use only common libraries to keep it simple.\n",
        "\n",
        "Once created, the index will be used to match the query with the documents. As a result, there are several ways to build an index, using statistical, boolean, semantic indexation...\n",
        "\n",
        "First of, let's make a naive one that will consist in breaking down each document into a set of the words it contains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lOClVNL5_abL"
      },
      "outputs": [],
      "source": [
        "def extract_words(text):\n",
        "  return text.split(' ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QwVgveW6CIAz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# test\n",
        "s = \"The cat is sat on the mat. The dog is laid on the mat.\"\n",
        "words = extract_words(s)\n",
        "assert sorted(words) == ['The', 'The', 'cat', 'dog', 'is', 'is', 'laid', 'mat.', 'mat.', 'on', 'on', 'sat', 'the', 'the']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qTFNPmNJC75C"
      },
      "source": [
        "As you may notice, there are several problems with the previous implementation. First, \"The\" and \"the\" aren't considered the same, the \".\" is kept at the the end of \"mat.\" as any other punctuation character... \n",
        "\n",
        "Re-implement this function with some basic preprocessing to avoid these issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eg9aRYX-CTTH"
      },
      "outputs": [],
      "source": [
        "# problems : First, \"The\" and \"the\" aren't considered the same, the \".\" is kept at the the end of \"mat.\" as any other punctuation character... \n",
        "def extract_words(text:str)->list:\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]','',text)\n",
        "  return text.split(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wnTSVCA1Fd1q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cat', 'dog', 'is', 'is', 'laid', 'mat', 'mat', 'on', 'on', 'sat', 'the', 'the', 'the', 'the']\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "print(sorted(extract_words(s)))\n",
        "assert sorted(extract_words(s))==['cat', 'dog', 'is', 'is', 'laid', 'mat', 'mat', 'on', 'on', 'sat', 'the', 'the', 'the', 'the']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TVQq7QZKF7mM"
      },
      "source": [
        "Now you sould be able to create your index table. For now we will just make a dataframe with two columns: [raw_text, words]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XLO7naGaF0LP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def index_docs(docs:list[str])->pd.DataFrame:\n",
        "  df = pd.DataFrame(docs,columns=['raw_text'])\n",
        "  df['words'] = df['raw_text'].apply(extract_words)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "IpK3zlftGw_w"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_text</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The cat is sat on the mat. The dog is laid on ...</td>\n",
              "      <td>[the, cat, is, sat, on, the, mat, the, dog, is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hello World!</td>\n",
              "      <td>[hello, world]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Goodbye</td>\n",
              "      <td>[goodbye]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How are you?</td>\n",
              "      <td>[how, are, you]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            raw_text  \\\n",
              "0  The cat is sat on the mat. The dog is laid on ...   \n",
              "1                                       Hello World!   \n",
              "2                                            Goodbye   \n",
              "3                                       How are you?   \n",
              "\n",
              "                                               words  \n",
              "0  [the, cat, is, sat, on, the, mat, the, dog, is...  \n",
              "1                                     [hello, world]  \n",
              "2                                          [goodbye]  \n",
              "3                                    [how, are, you]  "
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "\n",
        "L = [s, \"Hello World!\", \"Goodbye\", \"How are you?\"]\n",
        "\n",
        "index_docs(L)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "70w-UfpsHktY"
      },
      "source": [
        "Now, let's try it on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "46pO8FszG_4s"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>PostTypeId</th>\n",
              "      <th>CreationDate</th>\n",
              "      <th>Score</th>\n",
              "      <th>ViewCount</th>\n",
              "      <th>Body</th>\n",
              "      <th>OwnerUserId</th>\n",
              "      <th>LastActivityDate</th>\n",
              "      <th>Title</th>\n",
              "      <th>Tags</th>\n",
              "      <th>...</th>\n",
              "      <th>ClosedDate</th>\n",
              "      <th>ContentLicense</th>\n",
              "      <th>AcceptedAnswerId</th>\n",
              "      <th>LastEditorUserId</th>\n",
              "      <th>LastEditDate</th>\n",
              "      <th>ParentId</th>\n",
              "      <th>OwnerDisplayName</th>\n",
              "      <th>CommunityOwnedDate</th>\n",
              "      <th>LastEditorDisplayName</th>\n",
              "      <th>FavoriteCount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-13T23:58:30.457</td>\n",
              "      <td>9</td>\n",
              "      <td>898.0</td>\n",
              "      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2014-05-14T00:36:31.077</td>\n",
              "      <td>How can I do simple machine learning without h...</td>\n",
              "      <td>&lt;machine-learning&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>2014-05-14T14:40:25.950</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-14T00:11:06.457</td>\n",
              "      <td>4</td>\n",
              "      <td>478.0</td>\n",
              "      <td>&lt;p&gt;As a researcher and instructor, I'm looking...</td>\n",
              "      <td>36.0</td>\n",
              "      <td>2014-05-16T13:45:00.237</td>\n",
              "      <td>What open-source books (or other materials) pr...</td>\n",
              "      <td>&lt;education&gt;&lt;open-source&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>2014-05-14T08:40:54.950</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>2014-05-16T13:45:00.237</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>2014-05-14T00:36:31.077</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;Not sure if this fits the scope of this SE,...</td>\n",
              "      <td>51.0</td>\n",
              "      <td>2014-05-14T00:36:31.077</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>5.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>2014-05-14T00:53:43.273</td>\n",
              "      <td>13</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;One book that's freely available is \"The El...</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2014-05-14T00:53:43.273</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>7.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-14T01:25:59.677</td>\n",
              "      <td>26</td>\n",
              "      <td>1901.0</td>\n",
              "      <td>&lt;p&gt;I am sure data science as will be discussed...</td>\n",
              "      <td>66.0</td>\n",
              "      <td>2020-08-16T13:01:33.543</td>\n",
              "      <td>Is Data Science the Same as Data Mining?</td>\n",
              "      <td>&lt;data-mining&gt;&lt;definitions&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2014-06-17T16:17:20.473</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75722</th>\n",
              "      <td>119962</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-04T20:06:06.820</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>&lt;p&gt;I am implementing a neural network of arbit...</td>\n",
              "      <td>147597.0</td>\n",
              "      <td>2023-03-04T20:22:12.523</td>\n",
              "      <td>Back Propagation on arbitrary depth network wi...</td>\n",
              "      <td>&lt;neural-network&gt;&lt;backpropagation&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>147597.0</td>\n",
              "      <td>2023-03-04T20:22:12.523</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75723</th>\n",
              "      <td>119963</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-04T20:12:19.677</td>\n",
              "      <td>0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>&lt;p&gt;I am using KNN for a regression task&lt;/p&gt;\\n&lt;...</td>\n",
              "      <td>147598.0</td>\n",
              "      <td>2023-03-04T20:12:19.677</td>\n",
              "      <td>Evaluation parameter in knn</td>\n",
              "      <td>&lt;regression&gt;&lt;k-nn&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75724</th>\n",
              "      <td>119964</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-05T00:14:12.597</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>&lt;p&gt;I have developed a small encoding algorithm...</td>\n",
              "      <td>44581.0</td>\n",
              "      <td>2023-03-05T00:14:12.597</td>\n",
              "      <td>Can I use zero-padded input and output layers ...</td>\n",
              "      <td>&lt;deep-learning&gt;&lt;convolutional-neural-network&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75725</th>\n",
              "      <td>119965</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-05T00:43:12.213</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>&lt;p&gt;To my understanding, optimizing a model wit...</td>\n",
              "      <td>84437.0</td>\n",
              "      <td>2023-03-05T00:43:12.213</td>\n",
              "      <td>Why does cross validation and hyperparameter t...</td>\n",
              "      <td>&lt;cross-validation&gt;&lt;hyperparameter-tuning&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75726</th>\n",
              "      <td>119966</td>\n",
              "      <td>1</td>\n",
              "      <td>2023-03-05T03:10:27.593</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>&lt;p&gt;I'm working with a dataset of cars, contain...</td>\n",
              "      <td>147605.0</td>\n",
              "      <td>2023-03-05T03:10:27.593</td>\n",
              "      <td>High metrics value (MAE, MSE, RMSE)</td>\n",
              "      <td>&lt;python&gt;&lt;pandas&gt;&lt;machine-learning-model&gt;&lt;linea...</td>\n",
              "      <td>...</td>\n",
              "      <td>None</td>\n",
              "      <td>CC BY-SA 4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75727 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Id  PostTypeId             CreationDate  Score  ViewCount  \\\n",
              "0           5           1  2014-05-13T23:58:30.457      9      898.0   \n",
              "1           7           1  2014-05-14T00:11:06.457      4      478.0   \n",
              "2           9           2  2014-05-14T00:36:31.077      5        NaN   \n",
              "3          10           2  2014-05-14T00:53:43.273     13        NaN   \n",
              "4          14           1  2014-05-14T01:25:59.677     26     1901.0   \n",
              "...       ...         ...                      ...    ...        ...   \n",
              "75722  119962           1  2023-03-04T20:06:06.820      0        8.0   \n",
              "75723  119963           1  2023-03-04T20:12:19.677      0       10.0   \n",
              "75724  119964           1  2023-03-05T00:14:12.597      0        7.0   \n",
              "75725  119965           1  2023-03-05T00:43:12.213      0        5.0   \n",
              "75726  119966           1  2023-03-05T03:10:27.593      0        2.0   \n",
              "\n",
              "                                                    Body  OwnerUserId  \\\n",
              "0      <p>I've always been interested in machine lear...          5.0   \n",
              "1      <p>As a researcher and instructor, I'm looking...         36.0   \n",
              "2      <p>Not sure if this fits the scope of this SE,...         51.0   \n",
              "3      <p>One book that's freely available is \"The El...         22.0   \n",
              "4      <p>I am sure data science as will be discussed...         66.0   \n",
              "...                                                  ...          ...   \n",
              "75722  <p>I am implementing a neural network of arbit...     147597.0   \n",
              "75723  <p>I am using KNN for a regression task</p>\\n<...     147598.0   \n",
              "75724  <p>I have developed a small encoding algorithm...      44581.0   \n",
              "75725  <p>To my understanding, optimizing a model wit...      84437.0   \n",
              "75726  <p>I'm working with a dataset of cars, contain...     147605.0   \n",
              "\n",
              "              LastActivityDate  \\\n",
              "0      2014-05-14T00:36:31.077   \n",
              "1      2014-05-16T13:45:00.237   \n",
              "2      2014-05-14T00:36:31.077   \n",
              "3      2014-05-14T00:53:43.273   \n",
              "4      2020-08-16T13:01:33.543   \n",
              "...                        ...   \n",
              "75722  2023-03-04T20:22:12.523   \n",
              "75723  2023-03-04T20:12:19.677   \n",
              "75724  2023-03-05T00:14:12.597   \n",
              "75725  2023-03-05T00:43:12.213   \n",
              "75726  2023-03-05T03:10:27.593   \n",
              "\n",
              "                                                   Title  \\\n",
              "0      How can I do simple machine learning without h...   \n",
              "1      What open-source books (or other materials) pr...   \n",
              "2                                                   None   \n",
              "3                                                   None   \n",
              "4               Is Data Science the Same as Data Mining?   \n",
              "...                                                  ...   \n",
              "75722  Back Propagation on arbitrary depth network wi...   \n",
              "75723                        Evaluation parameter in knn   \n",
              "75724  Can I use zero-padded input and output layers ...   \n",
              "75725  Why does cross validation and hyperparameter t...   \n",
              "75726                High metrics value (MAE, MSE, RMSE)   \n",
              "\n",
              "                                                    Tags  ...  \\\n",
              "0                                     <machine-learning>  ...   \n",
              "1                               <education><open-source>  ...   \n",
              "2                                                   None  ...   \n",
              "3                                                   None  ...   \n",
              "4                             <data-mining><definitions>  ...   \n",
              "...                                                  ...  ...   \n",
              "75722                  <neural-network><backpropagation>  ...   \n",
              "75723                                 <regression><k-nn>  ...   \n",
              "75724      <deep-learning><convolutional-neural-network>  ...   \n",
              "75725          <cross-validation><hyperparameter-tuning>  ...   \n",
              "75726  <python><pandas><machine-learning-model><linea...  ...   \n",
              "\n",
              "                    ClosedDate  ContentLicense AcceptedAnswerId  \\\n",
              "0      2014-05-14T14:40:25.950    CC BY-SA 3.0              NaN   \n",
              "1      2014-05-14T08:40:54.950    CC BY-SA 3.0             10.0   \n",
              "2                         None    CC BY-SA 3.0              NaN   \n",
              "3                         None    CC BY-SA 3.0              NaN   \n",
              "4                         None    CC BY-SA 3.0             29.0   \n",
              "...                        ...             ...              ...   \n",
              "75722                     None    CC BY-SA 4.0              NaN   \n",
              "75723                     None    CC BY-SA 4.0              NaN   \n",
              "75724                     None    CC BY-SA 4.0              NaN   \n",
              "75725                     None    CC BY-SA 4.0              NaN   \n",
              "75726                     None    CC BY-SA 4.0              NaN   \n",
              "\n",
              "      LastEditorUserId             LastEditDate  ParentId OwnerDisplayName  \\\n",
              "0                  NaN                     None       NaN             None   \n",
              "1                 97.0  2014-05-16T13:45:00.237       NaN             None   \n",
              "2                  NaN                     None       5.0             None   \n",
              "3                  NaN                     None       7.0             None   \n",
              "4                322.0  2014-06-17T16:17:20.473       NaN             None   \n",
              "...                ...                      ...       ...              ...   \n",
              "75722         147597.0  2023-03-04T20:22:12.523       NaN             None   \n",
              "75723              NaN                     None       NaN             None   \n",
              "75724              NaN                     None       NaN             None   \n",
              "75725              NaN                     None       NaN             None   \n",
              "75726              NaN                     None       NaN             None   \n",
              "\n",
              "       CommunityOwnedDate LastEditorDisplayName FavoriteCount  \n",
              "0                    None                  None           NaN  \n",
              "1                    None                  None           NaN  \n",
              "2                    None                  None           NaN  \n",
              "3                    None                  None           NaN  \n",
              "4                    None                  None           NaN  \n",
              "...                   ...                   ...           ...  \n",
              "75722                None                  None           NaN  \n",
              "75723                None                  None           NaN  \n",
              "75724                None                  None           NaN  \n",
              "75725                None                  None           NaN  \n",
              "75726                None                  None           NaN  \n",
              "\n",
              "[75727 rows x 22 columns]"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")\n",
        "posts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DcaGAngHLK4g"
      },
      "source": [
        "For our first version of the indexation mechanism, we will simply use the \"body\" of the posts. To have a better search engine, the title and other metadata aswell could be used aswell. Finally, not all the XML files have a \"body\" feature, so for the search engine to retrieve information from any of the files you will need to implement another way to index.\n",
        "\n",
        "But first, let's start with \"body\". There is more to preprocess than before, indeed, there are html tags such as \"<p>\" for instance. They are not useful for us, because users won't use them in their queries. So we first need to remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Et-7VlXyKuaf"
      },
      "outputs": [],
      "source": [
        "def remove_tags(text:str)->str:\n",
        "  return re.sub(r'<[^>]+>', '', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "JQAW9pi9MkyZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello World!\\nI am making a search engine.'"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "remove_tags('<p>Hello World!\\nI am making a search engine.<p>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lDiFEsPtMszw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_33718/2440634092.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clean_posts['Clean Body'] = clean_posts['Body'].fillna('').apply(remove_tags)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Body</th>\n",
              "      <th>Clean Body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n",
              "      <td>I've always been interested in machine learnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>&lt;p&gt;As a researcher and instructor, I'm looking...</td>\n",
              "      <td>As a researcher and instructor, I'm looking fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>&lt;p&gt;Not sure if this fits the scope of this SE,...</td>\n",
              "      <td>Not sure if this fits the scope of this SE, bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>&lt;p&gt;One book that's freely available is \"The El...</td>\n",
              "      <td>One book that's freely available is \"The Eleme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>&lt;p&gt;I am sure data science as will be discussed...</td>\n",
              "      <td>I am sure data science as will be discussed in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75722</th>\n",
              "      <td>119962</td>\n",
              "      <td>&lt;p&gt;I am implementing a neural network of arbit...</td>\n",
              "      <td>I am implementing a neural network of arbitrar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75723</th>\n",
              "      <td>119963</td>\n",
              "      <td>&lt;p&gt;I am using KNN for a regression task&lt;/p&gt;\\n&lt;...</td>\n",
              "      <td>I am using KNN for a regression task\\nIt's lik...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75724</th>\n",
              "      <td>119964</td>\n",
              "      <td>&lt;p&gt;I have developed a small encoding algorithm...</td>\n",
              "      <td>I have developed a small encoding algorithm th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75725</th>\n",
              "      <td>119965</td>\n",
              "      <td>&lt;p&gt;To my understanding, optimizing a model wit...</td>\n",
              "      <td>To my understanding, optimizing a model with k...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75726</th>\n",
              "      <td>119966</td>\n",
              "      <td>&lt;p&gt;I'm working with a dataset of cars, contain...</td>\n",
              "      <td>I'm working with a dataset of cars, containing...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75727 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Id                                               Body  \\\n",
              "0           5  <p>I've always been interested in machine lear...   \n",
              "1           7  <p>As a researcher and instructor, I'm looking...   \n",
              "2           9  <p>Not sure if this fits the scope of this SE,...   \n",
              "3          10  <p>One book that's freely available is \"The El...   \n",
              "4          14  <p>I am sure data science as will be discussed...   \n",
              "...       ...                                                ...   \n",
              "75722  119962  <p>I am implementing a neural network of arbit...   \n",
              "75723  119963  <p>I am using KNN for a regression task</p>\\n<...   \n",
              "75724  119964  <p>I have developed a small encoding algorithm...   \n",
              "75725  119965  <p>To my understanding, optimizing a model wit...   \n",
              "75726  119966  <p>I'm working with a dataset of cars, contain...   \n",
              "\n",
              "                                              Clean Body  \n",
              "0      I've always been interested in machine learnin...  \n",
              "1      As a researcher and instructor, I'm looking fo...  \n",
              "2      Not sure if this fits the scope of this SE, bu...  \n",
              "3      One book that's freely available is \"The Eleme...  \n",
              "4      I am sure data science as will be discussed in...  \n",
              "...                                                  ...  \n",
              "75722  I am implementing a neural network of arbitrar...  \n",
              "75723  I am using KNN for a regression task\\nIt's lik...  \n",
              "75724  I have developed a small encoding algorithm th...  \n",
              "75725  To my understanding, optimizing a model with k...  \n",
              "75726  I'm working with a dataset of cars, containing...  \n",
              "\n",
              "[75727 rows x 3 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_posts = posts[['Id','Body']]\n",
        "clean_posts['Clean Body'] = clean_posts['Body'].fillna('').apply(remove_tags)\n",
        "clean_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3RbkcTyrNDcJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_33718/2163112268.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clean_posts['words'] = clean_posts['Clean Body'].apply(extract_words)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Body</th>\n",
              "      <th>Clean Body</th>\n",
              "      <th>words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>&lt;p&gt;I've always been interested in machine lear...</td>\n",
              "      <td>I've always been interested in machine learnin...</td>\n",
              "      <td>[ive, always, been, interested, in, machine, l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>&lt;p&gt;As a researcher and instructor, I'm looking...</td>\n",
              "      <td>As a researcher and instructor, I'm looking fo...</td>\n",
              "      <td>[as, a, researcher, and, instructor, im, looki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>&lt;p&gt;Not sure if this fits the scope of this SE,...</td>\n",
              "      <td>Not sure if this fits the scope of this SE, bu...</td>\n",
              "      <td>[not, sure, if, this, fits, the, scope, of, th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10</td>\n",
              "      <td>&lt;p&gt;One book that's freely available is \"The El...</td>\n",
              "      <td>One book that's freely available is \"The Eleme...</td>\n",
              "      <td>[one, book, thats, freely, available, is, the,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>&lt;p&gt;I am sure data science as will be discussed...</td>\n",
              "      <td>I am sure data science as will be discussed in...</td>\n",
              "      <td>[i, am, sure, data, science, as, will, be, dis...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75722</th>\n",
              "      <td>119962</td>\n",
              "      <td>&lt;p&gt;I am implementing a neural network of arbit...</td>\n",
              "      <td>I am implementing a neural network of arbitrar...</td>\n",
              "      <td>[i, am, implementing, a, neural, network, of, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75723</th>\n",
              "      <td>119963</td>\n",
              "      <td>&lt;p&gt;I am using KNN for a regression task&lt;/p&gt;\\n&lt;...</td>\n",
              "      <td>I am using KNN for a regression task\\nIt's lik...</td>\n",
              "      <td>[i, am, using, knn, for, a, regression, task\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75724</th>\n",
              "      <td>119964</td>\n",
              "      <td>&lt;p&gt;I have developed a small encoding algorithm...</td>\n",
              "      <td>I have developed a small encoding algorithm th...</td>\n",
              "      <td>[i, have, developed, a, small, encoding, algor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75725</th>\n",
              "      <td>119965</td>\n",
              "      <td>&lt;p&gt;To my understanding, optimizing a model wit...</td>\n",
              "      <td>To my understanding, optimizing a model with k...</td>\n",
              "      <td>[to, my, understanding, optimizing, a, model, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75726</th>\n",
              "      <td>119966</td>\n",
              "      <td>&lt;p&gt;I'm working with a dataset of cars, contain...</td>\n",
              "      <td>I'm working with a dataset of cars, containing...</td>\n",
              "      <td>[im, working, with, a, dataset, of, cars, cont...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75727 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Id                                               Body  \\\n",
              "0           5  <p>I've always been interested in machine lear...   \n",
              "1           7  <p>As a researcher and instructor, I'm looking...   \n",
              "2           9  <p>Not sure if this fits the scope of this SE,...   \n",
              "3          10  <p>One book that's freely available is \"The El...   \n",
              "4          14  <p>I am sure data science as will be discussed...   \n",
              "...       ...                                                ...   \n",
              "75722  119962  <p>I am implementing a neural network of arbit...   \n",
              "75723  119963  <p>I am using KNN for a regression task</p>\\n<...   \n",
              "75724  119964  <p>I have developed a small encoding algorithm...   \n",
              "75725  119965  <p>To my understanding, optimizing a model wit...   \n",
              "75726  119966  <p>I'm working with a dataset of cars, contain...   \n",
              "\n",
              "                                              Clean Body  \\\n",
              "0      I've always been interested in machine learnin...   \n",
              "1      As a researcher and instructor, I'm looking fo...   \n",
              "2      Not sure if this fits the scope of this SE, bu...   \n",
              "3      One book that's freely available is \"The Eleme...   \n",
              "4      I am sure data science as will be discussed in...   \n",
              "...                                                  ...   \n",
              "75722  I am implementing a neural network of arbitrar...   \n",
              "75723  I am using KNN for a regression task\\nIt's lik...   \n",
              "75724  I have developed a small encoding algorithm th...   \n",
              "75725  To my understanding, optimizing a model with k...   \n",
              "75726  I'm working with a dataset of cars, containing...   \n",
              "\n",
              "                                                   words  \n",
              "0      [ive, always, been, interested, in, machine, l...  \n",
              "1      [as, a, researcher, and, instructor, im, looki...  \n",
              "2      [not, sure, if, this, fits, the, scope, of, th...  \n",
              "3      [one, book, thats, freely, available, is, the,...  \n",
              "4      [i, am, sure, data, science, as, will, be, dis...  \n",
              "...                                                  ...  \n",
              "75722  [i, am, implementing, a, neural, network, of, ...  \n",
              "75723  [i, am, using, knn, for, a, regression, task\\n...  \n",
              "75724  [i, have, developed, a, small, encoding, algor...  \n",
              "75725  [to, my, understanding, optimizing, a, model, ...  \n",
              "75726  [im, working, with, a, dataset, of, cars, cont...  \n",
              "\n",
              "[75727 rows x 4 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_posts['words'] = clean_posts['Clean Body'].apply(extract_words)\n",
        "clean_posts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qWfNgdg7ziCm"
      },
      "source": [
        "## Zipf Law\n",
        "\n",
        "A way of analyzing a corpus is to draw the zipf law"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Yrts6RVNziLk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "                                           2051547\n",
              "the                                         497900\n",
              "to                                          271649\n",
              "a                                           226675\n",
              "of                                          212782\n",
              "                                            ...   \n",
              "modelcheckpointfilepathbest_weightshdf5          1\n",
              "modeauto\\ncheckpointer                           1\n",
              "min_delta1e8                                     1\n",
              "optimizeropt\\nmonitor                            1\n",
              "004946931214392558\\n\\nam                         1\n",
              "Name: words, Length: 630509, dtype: int64"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Draw Zipf Law on the Posts Corpus\n",
        "import matplotlib.pyplot as plt\n",
        "# get word count for each word\n",
        "word_count = clean_posts['words'].explode().value_counts()\n",
        "word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(word_count.values)\n",
        "plt.xlabel('Word Rank')\n",
        "plt.ylabel('Word Count')\n",
        "# log-log scale\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "plt.title('Zipf Law')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uOYd2qLpwhHE"
      },
      "source": [
        "## Inverted Index\n",
        "\n",
        "Now, we want to go further on the indexing and build an inverted index. Inverted index is a dictionary where the keys are the words of the vocabulary and the values are the documents containing these words. Reducing the size of the vocabulary is a relevant first step when building an inverted index. Here, we will focus on the creation of the index, we leave you the optimisation steps :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-sCV0Ds21g7J"
      },
      "outputs": [],
      "source": [
        "def create_index(posts:pd.DataFrame):\n",
        "  index = {}\n",
        "  for _, row in posts.iterrows():\n",
        "    for word in row['words']:\n",
        "      if word in index:\n",
        "        index[word].add(row['Id'])\n",
        "      else:\n",
        "        index[word] = {row['Id']}\n",
        "  return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uCPc3tRMZj9R"
      },
      "outputs": [],
      "source": [
        "# inverted_index = create_index(clean_posts.iloc[0:5000])\n",
        "# # save index to pickle\n",
        "# import pickle\n",
        "# with open('inverted_index.pickle', 'wb') as handle:\n",
        "#     pickle.dump(inverted_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load index from pickle\n",
        "import pickle\n",
        "inverted_index = {}\n",
        "with open('inverted_index.pickle', 'rb') as handle:\n",
        "    inverted_index = pickle.load(handle)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q97D2TjBOVZP"
      },
      "source": [
        "#### Well Done, you've indexed the dataset! \n",
        "Don't hesitate to save your indexes in txt or pickle file\n",
        "\n",
        "---\n",
        "# Implement the search method\n",
        "\n",
        "A naive method would be to count the number of words in common between the query and each posts. Then to rank the posts you could directly select the post who maximize the number of common words. Let's implement this approach :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UZX8J3Vrq7St"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1    1\n",
            "2    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Implement the word_in_index function \n",
        "# Inputs : a word (str) & a list of words\n",
        "# Output : pandas series of 1 if the word is in the list, else 0\n",
        "\n",
        "def word_in_index(word, word_list_index):\n",
        "  return pd.Series([1 if word in words else 0 for words in word_list_index])\n",
        "\n",
        "# test\n",
        "print(word_in_index('cat', [['cat', 'dog'], ['cat', 'mouse'], ['dog', 'mouse']]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "GFvxO88LtVi8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   0\n",
            "0  2\n",
            "1  1\n",
            "2  1\n"
          ]
        }
      ],
      "source": [
        "# Implement the function which run through a pandas series and count the number of word in common\n",
        "# Use extract_words method, apply method with word_in_index function\n",
        "# Inputs : the query (str) & pandas series of strings\n",
        "# Output : Pandas series counting the number of common words between the query and each string in word_serie\n",
        "\n",
        "def count_common_words(query, word_serie):\n",
        "  query_words = extract_words(query)\n",
        "  return word_serie.apply(lambda x: sum(word_in_index(word, [x]) for word in query_words))\n",
        "\n",
        "# test\n",
        "print(count_common_words('cat dog', pd.Series([['cat', 'dog'], ['cat', 'mouse'], ['dog', 'mouse']])))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "NHzyXeExNWQq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def rank_top_query(query, df, top=5):\n",
        "  # get the number of common words between the query and each document\n",
        "  common_words = count_common_words(query, df['words'])\n",
        "  # sort the documents by number of common words\n",
        "  sorted_common_words = common_words.sort_values(by=0, ascending=False)\n",
        "  # return the top documents\n",
        "  return df.iloc[sorted_common_words.index[0:top]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "iRdErltStZGv"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[101], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m rank_top_query(query\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtesting the query in python\u001b[39;49m\u001b[39m\"\u001b[39;49m, df\u001b[39m=\u001b[39;49mclean_posts, top\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
            "Cell \u001b[0;32mIn[91], line 3\u001b[0m, in \u001b[0;36mrank_top_query\u001b[0;34m(query, df, top)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrank_top_query\u001b[39m(query, df, top\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m):\n\u001b[1;32m      2\u001b[0m   \u001b[39m# get the number of common words between the query and each document\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m   common_words \u001b[39m=\u001b[39m count_common_words(query, df[\u001b[39m'\u001b[39;49m\u001b[39mwords\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      4\u001b[0m   \u001b[39m# sort the documents by number of common words\u001b[39;00m\n\u001b[1;32m      5\u001b[0m   sorted_common_words \u001b[39m=\u001b[39m common_words\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[82], line 8\u001b[0m, in \u001b[0;36mcount_common_words\u001b[0;34m(query, word_serie)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount_common_words\u001b[39m(query, word_serie):\n\u001b[1;32m      7\u001b[0m   query_words \u001b[39m=\u001b[39m extract_words(query)\n\u001b[0;32m----> 8\u001b[0m   \u001b[39mreturn\u001b[39;00m word_serie\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39msum\u001b[39;49m(word_in_index(word, [x]) \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m query_words))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "Cell \u001b[0;32mIn[82], line 8\u001b[0m, in \u001b[0;36mcount_common_words.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount_common_words\u001b[39m(query, word_serie):\n\u001b[1;32m      7\u001b[0m   query_words \u001b[39m=\u001b[39m extract_words(query)\n\u001b[0;32m----> 8\u001b[0m   \u001b[39mreturn\u001b[39;00m word_serie\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39msum\u001b[39;49m(word_in_index(word, [x]) \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m query_words))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/arraylike.py:102\u001b[0m, in \u001b[0;36mOpsMixin.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__add__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__add__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arith_method(other, operator\u001b[39m.\u001b[39;49madd)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:6259\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_arith_method\u001b[39m(\u001b[39mself\u001b[39m, other, op):\n\u001b[1;32m   6258\u001b[0m     \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39malign_method_SERIES(\u001b[39mself\u001b[39m, other)\n\u001b[0;32m-> 6259\u001b[0m     \u001b[39mreturn\u001b[39;00m base\u001b[39m.\u001b[39;49mIndexOpsMixin\u001b[39m.\u001b[39;49m_arith_method(\u001b[39mself\u001b[39;49m, other, op)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/base.py:1324\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1321\u001b[0m rvalues \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mmaybe_prepare_scalar_for_op(rvalues, lvalues\u001b[39m.\u001b[39mshape)\n\u001b[1;32m   1322\u001b[0m rvalues \u001b[39m=\u001b[39m ensure_wrapped_if_datetimelike(rvalues)\n\u001b[0;32m-> 1324\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1325\u001b[0m     result \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[1;32m   1327\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(result, name\u001b[39m=\u001b[39mres_name)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/_ufunc_config.py:435\u001b[0m, in \u001b[0;36merrstate.__exit__\u001b[0;34m(self, *exc_info)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mexc_info):\n\u001b[0;32m--> 435\u001b[0m     seterr(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moldstate)\n\u001b[1;32m    436\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _Unspecified:\n\u001b[1;32m    437\u001b[0m         seterrcall(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moldcall)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/_ufunc_config.py:32\u001b[0m, in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m     22\u001b[0m _errdict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m: ERR_IGNORE,\n\u001b[1;32m     23\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m: ERR_WARN,\n\u001b[1;32m     24\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m: ERR_RAISE,\n\u001b[1;32m     25\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcall\u001b[39m\u001b[39m\"\u001b[39m: ERR_CALL,\n\u001b[1;32m     26\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mprint\u001b[39m\u001b[39m\"\u001b[39m: ERR_PRINT,\n\u001b[1;32m     27\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m\"\u001b[39m: ERR_LOG}\n\u001b[1;32m     29\u001b[0m _errdict_rev \u001b[39m=\u001b[39m {value: key \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m _errdict\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m---> 32\u001b[0m \u001b[39m@set_module\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseterr\u001b[39m(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, divide\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, over\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, under\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, invalid\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     34\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m    Set how floating-point errors are handled.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     pyvals \u001b[39m=\u001b[39m umath\u001b[39m.\u001b[39mgeterrobj()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "results = rank_top_query(query=\"testing the query in python\", df=clean_posts, top=5) # prends 1min30 pour tourner\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tl;dr: They markedly differ in many aspects and I can't think Redshift will replace Hadoop.  \n",
            "\n",
            "-Function\n",
            "You can't run anything other than SQL on Redshift. Perhaps most importantly, you can't run any type of custom functions on Redshift. In Hadoop you can, using many languages (Java, Python, Ruby.. you name it). For example, NLP in Hadoop is easy, while it's more or less impossible in Redshift. I.e. there are lots of things you can do in Hadoop but not on Redshift. This is probably the most important difference.\n",
            "\n",
            "-Performance Profile\n",
            "Query execution on Redshift is in most cases significantly more efficient than on Hadoop. However, this efficiency comes from the indexing that is done when the data is loaded into Redshift (I'm using the term indexing very loose here). Therefore, it's great if you load your data once and execute multiple queries, but if you want to execute only one query for example, you might actually lose out in performance overall.\n",
            "\n",
            "-Cost Profile\n",
            "Which solution wins out in cost depends on the situation (like performance), but you probably need quite a lot of queries in order to make it cheaper than Hadoop (more specifically Amazon's Elastic Map Reduce). For example, if you are doing OLAP, it's very likely that Redshift comes out cheaper. If you do daily batch ETLs, Hadoop is more likely to come out cheaper.  \n",
            "\n",
            "Having said that, we've replaced part of our ETL that was done in Hive to Redshift, and it was a pretty great experience; mostly for the ease of development. Redshift's Query Engine is based on PostgreSQL and is very mature, compared to Hive's. Its ACID characteristics make it easier to reason about it, and the quicker response time allows more testing to be done. It's a great tool to have, but it won't replace Hadoop.  \n",
            "\n",
            "EDIT:  As for setup complexity, I'd even say it's easier with Hadoop if you use AWS's EMR. Their tools are so mature that it's ridiculously easy to have your Hadoop job running. Tools and mechanisms surrounding Redshift's operation aren't that mature yet. For example, Redshift can't handle trickle loading and thus you have to come up with something that turns that into a batched load, which can add some complexity to your ETL. \n",
            "\n",
            "-------------------\n",
            "The Wolfram Language has a Query function that can traverse data structures and apply functions at different levels of the structure.  I am working with multi-level JSON structures and need a function that has similar functionality as that of Query in the Wolfram Language. \n",
            "\n",
            "Which Python package and function(s) best replicates this?\n",
            "\n",
            "For a minimal working example, say I have the following JSON structure. (String escapes omitted for simplicity)\n",
            "\n",
            "x = {\n",
            "    \"Dims1\":[\n",
            "        {\n",
            "            \"Apple\":{\n",
            "                \"Baking\":[\n",
            "                    \"Pie\",\n",
            "                    \"Tart\"\n",
            "                ],\n",
            "                \"Plant\":\"Tree\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":1.23e1,\n",
            "                    \"2018\":1.12e1\n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"Tomato\":{\n",
            "                \"Cooking\":[\n",
            "                    \"Stew\",\n",
            "                    \"Sauce\"\n",
            "                ],\n",
            "                \"Plant\":\"Vine\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":8.1,\n",
            "                    \"2018\":8.3\n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"Banana\":{\n",
            "                \"Name\":\"Banana\",\n",
            "                \"Baking\":[\n",
            "                    \"Bread\"\n",
            "                ],\n",
            "                \"Cooking\":[\n",
            "                    \"Fried\"\n",
            "                ],\n",
            "                \"Plant\":\"Arborescent\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":0.8,\n",
            "                    \"2018\":0.5\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    ],\n",
            "    \"Dims2\":[\n",
            "        {\n",
            "            \"Apple\":{\n",
            "                \"Name\":\"Apple\",\n",
            "                \"Baking\":[\n",
            "                    \"Pie\",\n",
            "                    \"Tart\"\n",
            "                ],\n",
            "                \"Plant\":\"Tree\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":1.31e1,\n",
            "                    \"2018\":1.01e1\n",
            "                }\n",
            "            }\n",
            "        },\n",
            "        {\n",
            "            \"Sweet Potato\":{\n",
            "                \"Cooking\":[\n",
            "                    \"Fried\",\n",
            "                    \"Steamed\"\n",
            "                ],\n",
            "                \"Baking\":[\n",
            "                    \"Pie\"\n",
            "                ],\n",
            "                \"Plant\":\"Vine\",\n",
            "                \"Tons\":{\n",
            "                    \"2017\":1.11e1,\n",
            "                    \"2018\":1.91e1\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "\n",
            "In Wolfram Language I can \n",
            "\n",
            "a = GeneralUtilities`ToAssociations@ImportString[x, \"JSON\"]\n",
            "\n",
            "\n",
            "\n",
            "&lt;|\n",
            " \"Dims1\" -&gt;\n",
            "  {\n",
            "   &lt;|\"Apple\" -&gt;\n",
            "     &lt;|\"Baking\" -&gt; {\"Pie\", \"Tart\"}, \"Plant\" -&gt; \"Tree\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 12.3, \"2018\" -&gt; 11.2|&gt;|&gt;\n",
            "    |&gt;,\n",
            "   &lt;|\"Tomato\" -&gt; \n",
            "     &lt;|\"Cooking\" -&gt; {\"Stew\", \"Sauce\"}, \"Plant\" -&gt; \"Vine\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 8.1, \"2018\" -&gt; 8.3|&gt;|&gt;\n",
            "    |&gt;,\n",
            "   &lt;|\"Banana\" -&gt;\n",
            "     &lt;|\"Name\" -&gt; \"Banana\", \"Baking\" -&gt; {\"Bread\"}, \n",
            "      \"Cooking\" -&gt; {\"Fried\"}, \"Plant\" -&gt; \"Arborescent\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 0.8, \"2018\" -&gt; 0.5|&gt;|&gt;\n",
            "    |&gt;\n",
            "   },\n",
            " \"Dims2\" -&gt;\n",
            "  {\n",
            "   &lt;|\"Apple\" -&gt;\n",
            "     &lt;|\"Name\" -&gt; \"Apple\", \"Baking\" -&gt; {\"Pie\", \"Tart\"}, \n",
            "      \"Plant\" -&gt; \"Tree\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 13.1, \"2018\" -&gt; 10.1|&gt;|&gt;\n",
            "    |&gt;,\n",
            "   &lt;|\"Sweet Potato\" -&gt;\n",
            "     &lt;|\"Cooking\" -&gt; {\"Fried\", \"Steamed\"}, \"Baking\" -&gt; {\"Pie\"}, \n",
            "      \"Plant\" -&gt; \"Vine\",\n",
            "      \"Tons\" -&gt; &lt;|\"2017\" -&gt; 11.1, \"2018\" -&gt; 19.1|&gt;|&gt;\n",
            "    |&gt;\n",
            "   }\n",
            " |&gt;\n",
            "\n",
            "\n",
            "\n",
            "and then with Query\n",
            "\n",
            "Query[All, All, All, {\"Baking\"}]@a\n",
            "\n",
            "\n",
            "\n",
            "&lt;|\"Dims1\" -&gt; \n",
            "   {&lt;|\"Apple\" -&gt; &lt;|\"Baking\" -&gt; {\"Pie\", \"Tart\"}|&gt;|&gt;, \n",
            "    &lt;|\"Tomato\" -&gt; &lt;|\"Baking\" -&gt; Missing[\"KeyAbsent\", \"Baking\"]|&gt;|&gt;, \n",
            "    &lt;|\"Banana\" -&gt; &lt;|\"Baking\" -&gt; {\"Bread\"}|&gt;|&gt;}, \n",
            "  \"Dims2\" -&gt; \n",
            "   {&lt;|\"Apple\" -&gt; &lt;|\"Baking\" -&gt; {\"Pie\", \"Tart\"}|&gt;|&gt;, \n",
            "    &lt;|\"Sweet Potato\" -&gt; &lt;|\"Baking\" -&gt; {\"Pie\"}|&gt;|&gt;}\n",
            "|&gt;\n",
            "\n",
            "\n",
            "\n",
            "and include functions such as \n",
            "\n",
            "Query[All, Join /* Flatten /* DeleteDuplicates, Values, \"Baking\" /* DeleteMissing]@a\n",
            "\n",
            "\n",
            "\n",
            "&lt;|\"Dims1\" -&gt; {\"Pie\", \"Tart\", \"Bread\"}, \"Dims2\" -&gt; {\"Pie\", \"Tart\"}|&gt;\n",
            "\n",
            "\n",
            "\n",
            "and\n",
            "\n",
            "Query[All, Merge[Total] /* DateListPlot, All, \"Tons\", \n",
            "  KeyMap[DateObject[{FromDigits@#}, \"Year\"] &amp;]]@a\n",
            "\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "How is this done with JSON in Python?\n",
            "\n",
            "-------------------\n",
            "I am studying by now IR system, in the field of valuation of IR system outputs related to a specific query but I need some help to understand it properly. \n",
            "\n",
            "My book states that when an IR system has to be evaluated, we need a test document collection, a set of query examples, a valuation (relevant or not) for each couple of query/document, defined by experts in the field. So we need two measures to know quantitatively if a IR system is good: Precision and Recall. \n",
            "\n",
            "My doubt is related to the following question: Do we use those two measures only if we are testing a IR system or not? \n",
            "\n",
            "I'll explain: before we calculate Precision and Recall related to a specific query example (see above), we need to know how many elements belong to the relevant set, which is impossible if there isn't a valuation (relevant or not) for the query we are using. My book says we can increase Recall in a search engine by using the relevance feedback technique (query expansion and term reweighting): in this case do we assume the Recall value is unknown? \n",
            "\n",
            "For example, everyday many documents are shared on the Internet and Google can find them. So it is impossibile to apply Recall and Precision to this scenario, in which information grows and there is no valuation for every new document for each specific query. It is also impossibile to predict all the possibile queries a user can do on a search engine.\n",
            "\n",
            "-------------------\n",
            "This is how I would do it. However, a DataFrame can be structured in a number of way which best suits your needs. I believe that this method allows for the greatest flexibility because you can easily use grouping functions to restructure this format on the go. \n",
            "\n",
            "First you need to set up your data in a way that is compatible with Python. I use a dictionary of dictionaries\n",
            "\n",
            "cities = {'Lovell_Wyoming':\n",
            "          {'Crimes':\n",
            "           {\n",
            "            'Arson': [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "            'Assaults': [6, 6, 3, 4, 3, 28, 3, 2, 2] ,\n",
            "            'Auto_thefts': [1, 1, 1, 0, 0, 1, 2, 0, 1] ,\n",
            "            'Burglaries': [6, 11, 5, 2, 0, 15, 11, 7, 7] ,\n",
            "            'Murders': [0, 0, 0, 0, 1, 0, 0, 0, 0] ,\n",
            "            'Rapes': [0, 0, 3, 0, 0, 1, 1, 0, 1] ,\n",
            "            'Robberies': [0, 0, 0, 0, 0, 0, 0, 1, 0] ,\n",
            "            'Thefts': [23, 49, 35, 39, 28, 37, 54, 35, 10]\n",
            "           },\n",
            "            'Years': [2002, 2003, 2005, 2006, 2007, 2008, 2009, 2010, 2014]\n",
            "          },\n",
            "\n",
            "         'Wheatland_Wyoming':\n",
            "          {'Crimes':\n",
            "           {\n",
            "            'Arson': [0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0] ,\n",
            "            'Assaults': [9, 2, 6, 5, 6, 6, 2, 4, 2, 4, 3, 11, 5, 4, 8] ,\n",
            "            'Auto_thefts': [4, 8, 3, 3, 4, 4, 5, 3, 4, 6, 4, 8, 12, 7, 3] ,\n",
            "            'Burglaries': [17, 17, 14, 9, 10, 17, 12, 26, 51, 12, 15, 21, 32, 31, 13] ,\n",
            "            'Murders': [1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0] ,\n",
            "            'Rapes': [0, 0, 0, 4, 2, 1, 2, 0, 2, 1, 1, 0, 2, 0, 0] ,\n",
            "            'Robberies': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ,\n",
            "            'Thefts': [109, 95, 146, 81, 108, 100, 82, 85, 106, 128, 48, 85, 66, 56, 47]\n",
            "            },\n",
            "              'Years': [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]\n",
            "            }\n",
            "         }\n",
            "\n",
            "\n",
            "Then compile this data into rows of your DataFrame\n",
            "\n",
            "data = []\n",
            "for city in cities:\n",
            "    for ix, year in enumerate(cities[city]['Years']):\n",
            "        for crime in cities[city]['Crimes']:\n",
            "            temp = {'City':city, \n",
            "                    'Crime':crime, \n",
            "                    'Year':year, \n",
            "                    'Count':cities[city]['Crimes'][crime][ix]}\n",
            "            data.append(temp)\n",
            "\n",
            "\n",
            "Then into the DataFrame structure as.\n",
            "\n",
            "import pandas as pd\n",
            "df = pd.DataFrame(data=data)\n",
            "df\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Queries\n",
            "\n",
            "You can then query this DataFrame in a number of ways for example if you want to know arsons in 2002 you would do \n",
            "\n",
            "df[(df['Crime']=='Arson') &amp; (df['Year']==2002)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Counting\n",
            "\n",
            "You can count the number of arsons throughout the years as\n",
            "\n",
            "df[(df['Crime']=='Arson')].groupby(['City'])['Count'].agg('sum')\n",
            "\n",
            "\n",
            "\n",
            "  City Lovell_Wyoming       1 Wheatland_Wyoming    3 Name: Count, dtype:\n",
            "  int64\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Writing your DataFrame to a CSV file\n",
            "\n",
            "This can be done directly as \n",
            "\n",
            "df.to_csv('filename.csv')\n",
            "\n",
            "\n",
            "-------------------\n",
            "We all know that with the use of sklearn package from python, we can create X_train, X_test, y_train and y_test via this code:\n",
            "\n",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
            "\n",
            "\n",
            "I want to make sure that each training and testing phase of a multi-class data set, have 66.33/33.33 percent of each class values so the prediction and accuracy would get better. All i want is 66.33 percent of class A in training set and 33.33 percent of Class A in test set. And, so on for other classes, like B, C, D and etc. in a given multi-class data set.\n",
            "\n",
            "Is the code provided enough to achieve this or should i write extra code?\n",
            "\n",
            "Thanks\n",
            "\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "# print(results)\n",
        "for _, row in results.iterrows():\n",
        "    print(row['Clean Body'])\n",
        "    print('-------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pros:\n",
        "# - easy to implement\n",
        "# - fast to compute (1min30 par requete, bon...)\n",
        "# cons:\n",
        "# - gives the same weight to all words, even common words like \"the\" or \"is\"\n",
        "# - doesn't take into account the order of the words in the query\n",
        "# - doesn't take into account the order of the words in the documents\n",
        "# - doesn't take into account the number of times a word appears in a document\n",
        "# - doesn't take into account the number of times a word appears in the corpus\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JumHiP3txgUb"
      },
      "source": [
        "Testez plusieurs requêtes et critiquez les résultats obtenus.\n",
        "\n",
        "Quels sont les pros and cons de cette méthodes. Vous l'indiquerez sur le rapport avec vos réflexions pour l'améliorer.\n",
        "\n",
        "Next, you have to implement the first improvements you find in the search method to get most relevant results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/himmi/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        " \n",
        "nltk.download('stopwords')\n",
        "stop_words =  stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ux77Xzftx-kX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['cat', 'mat']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def remove_stop_words(l_txt: list) -> list:\n",
        "    return [word for word in l_txt if word not in stop_words]\n",
        "\n",
        "# test\n",
        "print(remove_stop_words(['the', 'cat', 'is', 'on', 'the', 'mat'])) # ['cat', 'mat']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MecCCwzbx8qZ"
      },
      "source": [
        "## Boolean Search\n",
        "\n",
        "Thanks to the ttable library, implement a boolean search method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "q5JTNdIrVdH9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This isn't a full solution, but you may want to look into OrientDB as part of your stack. Orient is a Graph-Document database server written entirely in Java. \n",
            "\n",
            "In graph databases, relationships are considered first class citizens and therefore traversing those relationships can be done pretty quickly. Orient is also a document database which would allow you the kind of schema-free architecture it sounds like you would need. The real reason I suggest Orient, however, is because of its extensiblity. It supports streaming via sockets, and the entire database can be embedded into another application. Finally, it can be scaled efficiently and/or can work entirely through memory. So, with some Java expertise, you can actually run your preset queries against the database in memory.\n",
            "\n",
            "We are doing something similar. In creating an app/site for social science research collaboration, we found ourselves with immensely complex data models. We ended up writing several of the queries using the Gremlin Traversal Language (a subset of Groovy, which is, of course, Java at its heart), and then exposing those queries through the binary connection server of the OrientDB. So, the client opens a TCP socket, sends a short binary message, and the query is executing in Java directly against the in-memory database.\n",
            "\n",
            "OrientDB also supports writing function queries in Javascript, and you can use Node.js to interact directly with an Orient instance.\n",
            "\n",
            "For something of this size, I would want to use Orient in conjunction with Hadoop or something like that. You can also use Orient in conjunction with esper.\n",
            "\n",
            "Consider:\n",
            "An introduction to orient: http://www.sitepoint.com/a-look-at-orientdb-the-graph-document-nosql/\n",
            "\n",
            "Complex, real-time queries: http://www.gft-blog.com/business-trends/leveraging-real-time-scoring-through-bigdata-to-detect-insurance-fraud/\n",
            "\n",
            "A discussion about streaming options with java and orient: https://github.com/orientechnologies/orientdb/issues/1227\n",
            "\n",
            "-------------------\n",
            "As we all know, there are some data indexing techniques, using by well-known indexing apps, like Lucene (for java) or Lucene.NET (for .NET), MurMurHash, B+Tree etc. For a No-Sql / Object Oriented Database (which I try to write/play a little around with C#), which technique you suggest?\n",
            "\n",
            "I read about MurMurhash-2 and specially v3 comments say Murmur is very fast. Also Lucene.Net has good comments on it. But what about their memory footprints in general? Is there any efficient solution which uses less footprint (and of course if faster is preferable) than Lucene or Murmur? Or should I write a special index structure to get the best results?\n",
            "\n",
            "If I try to write my own, then is there any accepted scale for a good indexing, something like 1% of data-node, or 5% of data-node? Any useful hint will be appreciated.\n",
            "\n",
            "-------------------\n",
            "You have three questions to answer and 100 records per month to analyze.\n",
            "\n",
            "Based on this size, I'd recommend doing analysis in a simple SQL database or a spreadsheet to start off with.  The first two questions are fairly easy to figure out.  The third is a little more difficult.\n",
            "\n",
            "I'd definitely add a column for month and group all of that data together into a spreadsheet or database table given the questions you want to answer.\n",
            "\n",
            "question 1. Users who are consistently showing up in this list\n",
            "\n",
            "In excel, this answer should help you out:  https://superuser.com/questions/442653/ms-excel-how-count-occurence-of-item-in-a-list\n",
            "\n",
            "For a SQL database:  https://stackoverflow.com/questions/2516546/select-count-duplicates\n",
            "\n",
            "question 2. Users who are consistently showing up in this list with high risk score \n",
            "\n",
            "This is just adding a little complexity to the above.  For SQL, you would further qualify your query based on a minimum risk score value.\n",
            "\n",
            "In excel, a straight pivot isn't going to work, you'll have to copy the unique values in one column to another, then drag a CountIf function adjacent to each unique value, qualifying the countif function with a minimum risk score.\n",
            "\n",
            "question 3. Users who have/reaching the high risk level very fast.\n",
            "\n",
            "A fast rise in risk level could be defined as the difference between two months being larger than a given value.\n",
            "\n",
            "For each user record you want to know the previous month's threat value, or assume zero as the previous threat value.\n",
            "\n",
            "If that difference is greater than your risk threshold, you want to include it in your report.  If not, they can be filtered from the list.  \n",
            "\n",
            "If I had to do this month after month, I would spend the two hours it might take to automate a report after the first couple of months.  I'd throw all the data in a SQL database and write a quick script in perl or java to iterate through the 100 records, do the calculation, and output the users who crossed the threshold.\n",
            "\n",
            "If I needed it to look pretty, I'd use a reporting tool.  I'm not particularly partial to any of them.\n",
            "\n",
            "If I needed to trend threshold values over time, I'd output the results for all people into a second table add records to that table each month.\n",
            "\n",
            "If I just needed to do it once or twice, figuring out how to do it in excel by adding a new column using VLookUp and some basic math and a filter would probably be the fastest and easiest way to get it done.  I tend to avoid using excel for things I'll need to use with consistency because there are limits that you run into when your data gets sizeable.\n",
            "\n",
            "-------------------\n",
            "Check out this link.\n",
            "\n",
            "Here, they will take you through loading unstructured text to creating a wordcloud.  You can adapt this strategy and instead of creating a wordcloud, you can create a frequency matrix of terms used.  The idea is to take the unstructured text and structure it somehow.  You change everything to lowercase (or uppercase), remove stop words, and find frequent terms for each job function, via Document Term Matrices.  You also have the option of stemming the words.  If you stem words you will be able to detect different forms of words as the same word.  For example, 'programmed' and 'programming' could be stemmed to 'program'.  You can possibly add the occurrence of these frequent terms as a weighted feature in your ML model training.\n",
            "\n",
            "You can also adapt this to frequent phrases, finding common groups of 2-3 words for each job function.\n",
            "\n",
            "Example:\n",
            "\n",
            "1) Load libraries and build the example data\n",
            "\n",
            "library(tm)\n",
            "library(SnowballC)\n",
            "\n",
            "doc1 = \"I am highly skilled in Java Programming.  I have spent 5 years developing bug-tracking systems and creating data managing system applications in C.\"\n",
            "job1 = \"Software Engineer\"\n",
            "doc2 = \"Tested new software releases for major program enhancements.  Designed and executed test procedures and worked with relational databases.  I helped organize and lead meetings and work independently and in a group setting.\"\n",
            "job2 = \"Quality Assurance\"\n",
            "doc3 = \"Developed large and complex web applications for client service center. Lead projects for upcoming releases and interact with consumers.  Perform database design and debugging of current releases.\"\n",
            "job3 = \"Software Engineer\"\n",
            "jobInfo = data.frame(\"text\" = c(doc1,doc2,doc3),\n",
            "                     \"job\" = c(job1,job2,job3))\n",
            "\n",
            "\n",
            "2) Now we do some text structuring. I am positive there are quicker/shorter ways to do the following.\n",
            "\n",
            "# Convert to lowercase\n",
            "jobInfo$text = sapply(jobInfo$text,tolower)\n",
            "\n",
            "# Remove Punctuation\n",
            "jobInfo$text = sapply(jobInfo$text,function(x) gsub(\"[[:punct:]]\",\" \",x))\n",
            "\n",
            "# Remove extra white space\n",
            "jobInfo$text = sapply(jobInfo$text,function(x) gsub(\"[ ]+\",\" \",x))\n",
            "\n",
            "# Remove stop words\n",
            "jobInfo$text = sapply(jobInfo$text, function(x){\n",
            "  paste(setdiff(strsplit(x,\" \")[[1]],stopwords()),collapse=\" \")\n",
            "})\n",
            "\n",
            "# Stem words (Also try without stemming?)\n",
            "jobInfo$text = sapply(jobInfo$text, function(x)  {\n",
            "  paste(setdiff(wordStem(strsplit(x,\" \")[[1]]),\"\"),collapse=\" \")\n",
            "})\n",
            "\n",
            "\n",
            "3) Make a corpus source and document term matrix.\n",
            "\n",
            "# Create Corpus Source\n",
            "jobCorpus = Corpus(VectorSource(jobInfo$text))\n",
            "\n",
            "# Create Document Term Matrix\n",
            "jobDTM = DocumentTermMatrix(jobCorpus)\n",
            "\n",
            "# Create Term Frequency Matrix\n",
            "jobFreq = as.matrix(jobDTM)\n",
            "\n",
            "\n",
            "Now we have the frequency matrix, jobFreq, that is a (3 by x) matrix, 3 entries and X number of words.\n",
            "\n",
            "Where you go from here is up to you.  You can keep only specific (more common) words and use them as features in your model.  Another way is to keep it simple and have a percentage of words used in each job description, say \"java\" would have 80% occurrence in 'software engineer' and only 50% occurrence in 'quality assurance'.\n",
            "\n",
            "Now it's time to go look up why 'assurance' has 1 'r' and 'occurrence' has 2 'r's.\n",
            "\n",
            "-------------------\n",
            "I am a relatively new user to Hadoop (using version 2.4.1). I installed hadoop on my first node without a hitch, but I can't seem to get the Resource Manager to start on my second node. \n",
            "\n",
            "I cleared up some \"shared library\" problems by adding this to yarn-env.sh and hadoop-env.sh:\n",
            "\n",
            "\n",
            "  export HADOOP_HOME=\"/usr/local/hadoop\"\n",
            "  \n",
            "  export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"\n",
            "\n",
            "\n",
            "I also added this to hadoop-env.sh:\n",
            "\n",
            "\n",
            "  export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native\n",
            "\n",
            "\n",
            "based on the advice of this post at horton works http://hortonworks.com/community/forums/topic/hdfs-tmp-dir-issue/ \n",
            "\n",
            "That cleared up all of my error messages; when I run /sbin/start-yarn.sh I get this:\n",
            "\n",
            "\n",
            "  starting yarn daemons\n",
            "  \n",
            "  starting resourcemanager, \n",
            "  logging to /usr/local/hadoop/logs/yarn-hduser-resourcemanager-HdNode.out\n",
            "  \n",
            "  localhost: starting nodemanager, \n",
            "  logging to /usr/local/hadoop/logs/yarn-hduser-nodemanager-HdNode.out\n",
            "\n",
            "\n",
            "The only problem is, JPS says that the Resource Manager isn't running. \n",
            "\n",
            "What's going on here?\n",
            "\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "from tt import BooleanExpression\n",
        "\n",
        "def boolean_search(query, df=clean_posts):\n",
        "  # get the words in the query\n",
        "  expression =  BooleanExpression(query)\n",
        "  # get the posts whose clean body that satisfy the expression\n",
        "  symbols = expression.symbols\n",
        "  # for each post and for each symbol, check if the symbol is in the post, then evaluate the expression\n",
        "  bools = df['Clean Body'].apply(lambda x: [symbol in x for symbol in symbols]).apply(lambda x: expression.evaluate(**dict(zip(symbols,x))))\n",
        "  # return all documents that satisfy the expression\n",
        "  return df.iloc[bools[bools].index]\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "  # return the top documents\n",
        "\n",
        "results = boolean_search('java AND NOT python')\n",
        "\n",
        "for _, row in results[0:5].iterrows():\n",
        "    print(row['Clean Body'])\n",
        "    print('-------------------')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N4vFldsAycpB"
      },
      "source": [
        "## Probabilistic search\n",
        "\n",
        "Implement the MIB or BM25 method of searching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "bCxjyrldyhUB"
      },
      "outputs": [],
      "source": [
        "def probabilistic_search(query,df=clean_posts):\n",
        "  # each document get a score\n",
        "  # the score is the probability that the document is returned given the query (P(D|Q)) divides by the probability that the document is returned given it doesn't match the query (P(D|¬Q))\n",
        "\n",
        "  scores = []\n",
        "  for _, row in df.iterrows():\n",
        "    # get the words in the document\n",
        "    words = row['words']\n",
        "    # get the number of words in the document\n",
        "    n_words = len(words)\n",
        "    # get the number of words in the document that are in the query\n",
        "    n_words_in_query = len([word for word in words if word in query])\n",
        "    # get the number of words in the document that are not in the query\n",
        "    n_words_not_in_query = n_words - n_words_in_query\n",
        "    # get the number of words in the query\n",
        "    n_words_in_query = len(query)\n",
        "    # get the number of words in the query that are in the document\n",
        "    n_words_in_query_in_doc = len([word for word in query if word in words])\n",
        "    # get the number of words in the query that are not in the document\n",
        "    n_words_in_query_not_in_doc = n_words_in_query - n_words_in_query_in_doc\n",
        "    # compute the probability that the document is returned given the query (P(D|Q))\n",
        "    p_doc_given_query = (n_words_in_query_in_doc + 1) / (n_words_in_query + 2)\n",
        "    # compute the probability that the document is returned given it doesn't match the query (P(D|¬Q))\n",
        "    p_doc_given_not_query = (n_words_in_query_not_in_doc + 1) / (n_words_not_in_query + 2)\n",
        "    # compute the score\n",
        "    score = p_doc_given_query / p_doc_given_not_query\n",
        "    # add the score to the list of scores\n",
        "    scores.append(score)\n",
        "  # add the scores to the dataframe\n",
        "  df['score'] = scores\n",
        "  # sort the dataframe by score\n",
        "  sorted_df = df.sort_values(by='score', ascending=False)\n",
        "  # return the top documents\n",
        "  return sorted_df[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_33718/3056723717.py:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  clean_posts['score'] = scores\n"
          ]
        }
      ],
      "source": [
        "# test\n",
        "results = probabilistic_search('how to do a query with python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm trying to create a neural network that can learn how to write text character by character from the book David Copperfield (via Project Gutenburg). \n",
            "\n",
            "It starts great, then forgets punctuation around epoch 25 and devolves into nonsense at epoch 26. I've been trying to find a starting point where I can start attacking this problem. I've read research papers on the concept of clipping the gradient to stop it from vanishing or exploding, but I'm having a hard time finding a way to first visualize the gradient and what's going wrong and then how to clip it at the most appropriate value. \n",
            "\n",
            "I've saved checkpoint models from all fifty epochs. \n",
            "\n",
            "I've tried using a clipping the gradient at 5 based on a research paper about gradient clipping in LSTMs and it didn't change anything. I don't have to budget to find the optimum value through experimentation, but if that's the only way to do it I'll make it work.  \n",
            "\n",
            "I've been working on this a long time, but I'm self-taught and feel a a bit out of my depth here. A nudge in the right direction from a subject matter expert would be greatly appreciated.\n",
            "\n",
            "Epoch 20:\n",
            "\n",
            "\n",
            "  and that I saw Traddles the service in the house, person a sort of something and so great hand of the counter being a prospect of Mr. Omer, had replied to go and so moment as a case so refersing the house of his man as distracting him on the door and last hearing her after for the streng to a head, and Mr. Micawber, that many beer and when I was so consequent when the man. I can be so looked to keep me all the black desiring the constinging and enearly to his part of more arms were in the scound as my delight, that she asked the time and herself of what I had been with the windows years of such a green eye of the\n",
            "  most night was a dear, but that there are you? I began to make dear, she was not that a present words, long a back hand with a better was many to a relief in search had one been what he was to say that the hand sitting weeks, and made with a present time, and my face, so he sat no more about the mount, and set of the time of petitution and the door was in a proceeding the spirit,\n",
            "\n",
            "\n",
            "Epoch 25: \n",
            "\n",
            "\n",
            "  sir, I am willing to the beautiful thought who was not her to the best flew for a son, when I said, said I. When she had to speak the strong spirits of this comprier of an and bill time of the resist to my street at the garden with it, and that he before my for the sense of the room in the old evening in the time with a real arms child would take it a little books to the send with a shining the care that I cant have seen it a good shoulder of the morning for a moment in a time of a good mist from the expect of laughing so like a deport for one of the carrier of that success of a spoke with the wild of the table just over and thing being pretending in the proposition that there took a far in my devoted towards me what he went to a sense of the stare to the breditol for the great contriout, and that I were done in the shadow and man of our fathers dream at once with me, as to say, but he said a man. I think it was one far that he had been over the desire that was not loved in the grief that could says more\n",
            "\n",
            "\n",
            "Epoch 26:\n",
            "\n",
            "\n",
            "  spi  ,  i ed o eoa t  i    i.  nhsaae     s  t?h  h t    h    n\n",
            "  t  cha,p   t r   ieto  t wo\n",
            "  a hw   ne s  uawpai  ,y  na  a    e  t dttte  t?atsh      oh h  i au a    a a   ddn e haf e  s t.rooe  wt   etdt s\n",
            "  t a   ,,  i t e a h e\n",
            "     a, dt os   rhr   tis        m  elrii    e a ao   ty   otatp   ya  r t a  ty o  he,  ee , s s  i.\n",
            "  hn    e  o  te  a o o se  s h u     te  senea    e tt        s d  ew , ie s  I ee     ihi hnts  a an r rv otso t   a    eshne     tta o  tt\n",
            "    e  o  m l   h arnt led n    sa a a    e  h  tww n  ee   t     h   ha  ,tdeh  t e  nntt  i atnr,e     wt eee     hb a     t  n  oea    a     e    ei t -.     de  e a e s e n   atehh   h e  a s   ef   e n  to  hr   d , eh  In.i o st watn t  htih e io    tt a x h   s s ,  e   s  r ohsmtldal  er e\n",
            "  n, t,dtthan,t   h s hdhe   oa  oh tbh t  ot isn  o  e tr      et ttnm an  a    ot  ng c a ds   tr .s\n",
            "  t h    a t\n",
            "  t    ewehso         n sr o e  se   i   e e   httst    b    i tt  .  t n I   he o   w so tt t,w sttt  nta t ai   o  \n",
            "\n",
            "\n",
            "Code Follows: \n",
            "(I started with Tensorflow, but switched to TFLearn for simplicity. I'm willing to learn any frameworks that have the tools to solve the problem, though. I'm a self-taught student, so learning is really the only objective here.) \n",
            "\n",
            "import time \n",
            "start_script_time = time.time()\n",
            "\n",
            "\n",
            "import numpy as np\n",
            "import tflearn\n",
            "import random\n",
            "import pickle\n",
            "''' create data ''' \n",
            "\n",
            "log_file = 'dickens_log.txt'\n",
            "def my_log(text, filename=log_file):\n",
            "    text = str(text)\n",
            "    print(text)\n",
            "    with open(filename, 'a', newline='\\n') as file:\n",
            "        file.write(text + '\\n')\n",
            "\n",
            "\n",
            "try:\n",
            "    book_name = 'as_loaded.txt'\n",
            "    book = open(book_name, errors='ignore',\n",
            "                encoding='ascii', newline='\\n').read() \n",
            "except:\n",
            "    book_name = 'copperfield.txt'\n",
            "    book = open(book_name, errors='ignore',\n",
            "                encoding='utf-8', newline='\\n').read()\n",
            "    #book = book.replace('\\r', '')\n",
            "    #book = book.replace('\\n', ' ')\n",
            "    with open('as_loaded.txt', 'w', newline='\\n') as file:\n",
            "        file.write(book)\n",
            "\n",
            "# make smaller slice for quickly testing code on CPU \n",
            "# book = book[0:1500]\n",
            "# del(book_name)\n",
            "\n",
            "# length of strings in the training set\n",
            "string_length = 30\n",
            "\n",
            "def process_book(book, string_length, redundant_step=3):\n",
            "\n",
            "    # Remember to pickle to dictionary as a binary. This is pretty critical for loading your model on a different machine than you trained on. \n",
            "    try:\n",
            "        pickle_ld = open('charDict.pi', 'rb')\n",
            "        charDict = pickle.load(pickle_ld)\n",
            "        pickle_ld.close()\n",
            "    except:\n",
            "        # dictionary of character-number pairs\n",
            "        chars = sorted(list(set(book)))\n",
            "        charDict = dict((c, i) for i, c in enumerate(chars))\n",
            "        #charDict.pop('\\r')\n",
            "        pickle_sv = open('charDict.pi', 'wb')\n",
            "        pickle.dump(charDict, pickle_sv)\n",
            "        pickle_sv.close()\n",
            "\n",
            "    len_chars = len(charDict)\n",
            "\n",
            "    # train is a string input and target is the \n",
            "    # expected next character     \n",
            "    train = []\n",
            "    target = []\n",
            "    for i in range(0, len(book)-string_length, redundant_step):\n",
            "        train.append(book[i:i+string_length])\n",
            "        target.append(book[i+string_length])\n",
            "\n",
            "    # create containers for data with appropriate dimensions\n",
            "    # 3D (n_samples, sample_size, n_categories)\n",
            "    X = np.zeros((len(train), string_length, len_chars), dtype=np.bool)\n",
            "    # 2D (n_samples, n_categories)\n",
            "    y = np.zeros((len(train), len_chars), dtype=np.bool)\n",
            "\n",
            "    # fill arrays\n",
            "    for i, string in enumerate(train):\n",
            "        for j, char in enumerate(string):\n",
            "            # X is a sparse 3D tensor where a 1 value signals \n",
            "            # that a information is present in 3rd dimension index\n",
            "            X[i, j, charDict[char]] = 1\n",
            "        y[i, charDict[target[i]]] = 1\n",
            "\n",
            "    return charDict, X, y\n",
            "\n",
            "charDict, X, y = process_book(book, string_length)\n",
            "\n",
            "''' build the network ''' \n",
            "\n",
            "# number of hidden layers in each LSTM layer\n",
            "lstm_hidden = 512\n",
            "drop_rate = 0.5\n",
            "\n",
            "net = tflearn.input_data(shape=(None, string_length, len(charDict)))\n",
            "\n",
            "# input shape is the length of the strings by the number of characters\n",
            "# leading None is necessary if no placeholders \n",
            "net = tflearn.lstm(net, lstm_hidden, return_seq=True)\n",
            "net = tflearn.dropout(net, drop_rate)\n",
            "\n",
            "# You have to use a separate dropout layer. There's a glitch where tflean\n",
            "# will drop out all the time, not just during training, making prediction\n",
            "# impossible. \n",
            "net = tflearn.lstm(net, lstm_hidden, return_seq=True)\n",
            "net = tflearn.dropout(net, drop_rate)\n",
            "\n",
            "net = tflearn.lstm(net, lstm_hidden, return_seq=False)\n",
            "net = tflearn.dropout(net, drop_rate)\n",
            "\n",
            "net = tflearn.fully_connected(net, len(charDict), activation='softmax')\n",
            "\n",
            "net = tflearn.regression(net, optimizer='adam', \n",
            "                         loss='categorical_crossentropy', \n",
            "                         learning_rate=0.005)\n",
            "\n",
            "# https://www.quora.com/What-is-gradient-clipping-and-why-is-it-necessary \n",
            "model = tflearn.SequenceGenerator(net, dictionary=charDict, \n",
            "                                  seq_maxlen=string_length,\n",
            "                                  clip_gradients=5,\n",
            "                                  checkpoint_path='model_checkpoint_v3')\n",
            "\n",
            "\n",
            "my_log('Character dictionary for ' + book_name)\n",
            "my_log(charDict)\n",
            "my_log('charDict length: ' + str(len(charDict)))\n",
            "my_log('&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;&amp;')\n",
            "\n",
            "def random_seed_test(book, temp=0.5, gen_length=300):\n",
            "    my_log('#######################')    \n",
            "    seed_no = random.randint(0, len(book) - string_length)\n",
            "    seed = book[seed_no : seed_no + string_length]\n",
            "    my_log('(temp ' + str(temp) + ') ' + 'Seed: \"' + seed + '\"')\n",
            "    my_log('++++++++++++++++++++++')  \n",
            "    my_log(model.generate(seq_length=gen_length, temperature=temp, \n",
            "                              seq_seed=seed))\n",
            "    my_log('#######################') \n",
            "\n",
            "\n",
            "\n",
            "# If you train one epoch at a time in a loop, you can get an idea \n",
            "# of how the model progressed. With other ML problems, error rate and \n",
            "# accuracy reveal a lot, but with this problem performance is subjective. \n",
            "for epoch in range(50):\n",
            "    start_epoch = time.time()\n",
            "    my_log('======================================================')\n",
            "    my_log('Begin epoch %d' % (epoch+1))\n",
            "    model.fit(X, y, validation_set=0.1, batch_size=128, n_epoch=1)\n",
            "    my_log('End epoch %d' % (epoch+1))\n",
            "    epoch_time = time.time() - start_epoch\n",
            "    my_log('This epoch took ' + str(epoch_time) + ' seconds.')\n",
            "    random_seed_test(book, temp=0.5, gen_length=1000)\n",
            "    random_seed_test(book, temp=0.75, gen_length=1000)\n",
            "    random_seed_test(book, temp=1.0, gen_length=1000)\n",
            "    my_log('End epoch %d' % (epoch+1))\n",
            "    my_log('======================================================')\n",
            "\n",
            "\n",
            "full_time = time.time() - start_script_time\n",
            "my_log('This program took ' + str(full_time) + ' seconds.')\n",
            "\n",
            "model.save('dickens_compute_4.model')\n",
            "\n",
            "my_log('finished')\n",
            "\n",
            "\n",
            "-------------------\n",
            "I am creating the LSTM with just numpy and plotting the loss with pyplot. I have checked the derivatives again and again however have not found a mistake. The entire code with the main function can be viewed at https://github.com/lanttu1243/LSTM\n",
            "Here are the backpropagation functions:\n",
            "def backpropagation_output_layer(self, targets: list[int]) -&gt; dict[int, np.ndarray]:\n",
            "    dout: dict[int, np.ndarray] = {}\n",
            "    for t in reversed(range(len(targets))):\n",
            "        # Backpropagation into y\n",
            "        dy = np.copy(self.ps[t])\n",
            "\n",
            "        dy[targets[t]] -= 1\n",
            "\n",
            "        self.dBy += dy\n",
            "        self.dWy += dy @ self.hs[t].T\n",
            "\n",
            "        dout[t] = self.Wy.T @ dy\n",
            "\n",
            "    return dout\n",
            "\n",
            "def backpropagation_lstm_layer(self, dh: dict[int, np.ndarray]) -&gt; dict[int, np.ndarray]:\n",
            "\n",
            "    dcnext: np.ndarray = np.zeros((self.hidden_size, 1))\n",
            "    dx: dict[int, np.ndarray] = {}\n",
            "    for t in reversed(range(len(dh))):\n",
            "\n",
            "        dout_dh = dh[t]\n",
            "        dh_dct = dout_dh * self.og[t] + dcnext\n",
            "        dh_do = self.cs[t]\n",
            "\n",
            "        dct_dct_1 = dh_dct * self.fg[t]\n",
            "        dct_df = dh_dct * self.cs[t-1]\n",
            "        dct_di = dh_dct * self.mg[t]\n",
            "        dct_dm = dh_dct * self.ig[t]\n",
            "\n",
            "        df_dzf = dct_df * (self.fg[t] * (1 - self.fg[t]))\n",
            "        self.dWf += df_dzf @ self.conc[t].T\n",
            "        self.dBf += df_dzf\n",
            "        dzf_dx = self.Wf.T @ df_dzf\n",
            "\n",
            "        di_dzi = dct_di * (self.ig[t] * (1 - self.ig[t]))\n",
            "        self.dWi += di_dzi @ self.conc[t].T\n",
            "        self.dBi += di_dzi\n",
            "        dzi_dx = self.Wi.T @ di_dzi\n",
            "\n",
            "        dm_dzm = dct_dm * (1 - np.square(self.mg[t]))\n",
            "        self.dWm += dm_dzm @ self.conc[t].T\n",
            "        self.Bm += dm_dzm\n",
            "        dzm_dx = self.Wm.T @ dm_dzm\n",
            "\n",
            "        do_dzo = dh_do * (self.og[t] * (1 - self.og[t]))\n",
            "        self.dWo += do_dzo @ self.conc[t].T\n",
            "        self.Bo += do_dzo\n",
            "        dzo_dx = self.Wo.T @ do_dzo\n",
            "\n",
            "        dx[t] = (dzo_dx + dzm_dx + dzf_dx + dzi_dx)[:self.input_size, :]\n",
            "\n",
            "    return dx\n",
            "\n",
            "def adagrad(self) -&gt; None:\n",
            "    self.mWf += np.square(self.dWf)\n",
            "    self.mWi += np.square(self.dWi)\n",
            "    self.mWo += np.square(self.dWo)\n",
            "    self.mWm += np.square(self.dWm)\n",
            "\n",
            "    self.mBf += np.square(self.dBf)\n",
            "    self.mBi += np.square(self.dBi)\n",
            "    self.mBo += np.square(self.dBo)\n",
            "    self.mBm += np.square(self.dBm)\n",
            "\n",
            "    self.mWy += np.square(self.dWy)\n",
            "    self.mBy += np.square(self.dBy)\n",
            "\n",
            "    self.Wf -= (self.learning_rate / np.sqrt(np.diagonal(self.mWf).copy() + 1e-8)) @ self.dWf\n",
            "    self.Wi -= (self.learning_rate / np.sqrt(np.diagonal(self.mWi).copy() + 1e-8)) @ self.dWi\n",
            "    self.Wo -= (self.learning_rate / np.sqrt(np.diagonal(self.mWo).copy() + 1e-8)) @ self.dWo\n",
            "    self.Wm -= (self.learning_rate / np.sqrt(np.diagonal(self.mWm).copy() + 1e-8)) @ self.dWm\n",
            "\n",
            "    self.Bf -= (self.learning_rate / np.sqrt(self.mBf + 1e-8)) * self.dBf\n",
            "    self.Bi -= (self.learning_rate / np.sqrt(self.mBi + 1e-8)) * self.dBi\n",
            "    self.Bo -= (self.learning_rate / np.sqrt(self.mBo + 1e-8)) * self.dBo\n",
            "    self.Bm -= (self.learning_rate / np.sqrt(self.mBm + 1e-8)) * self.dBm\n",
            "\n",
            "    self.Wy -= (self.learning_rate / np.sqrt(np.diagonal(self.mWy).copy() + 1e-8)) @ self.dWy\n",
            "    self.By -= (self.learning_rate / np.sqrt(self.mBy + 1e-8)) * self.dBy\n",
            "\n",
            "Here is the entire code:\n",
            "import numpy as np\n",
            "from matplotlib import pyplot as plt\n",
            "\n",
            "class LSTMLayer:\n",
            "\n",
            "    def __init__(self,\n",
            "                 layer_number: int,\n",
            "                 input_size: int,\n",
            "                 hidden_size: int,\n",
            "                 learning_rate: float,\n",
            "                 output_size: int = 0):\n",
            "        # Initialising the static parameters for the layer\n",
            "        self.hidden_size: int = hidden_size  # Hidden size is the number of neurons in a layer\n",
            "        self.input_size: int = input_size  # Input size is the number of inputs into the layer\n",
            "        self.learning_rate: float = learning_rate  # Learning rate tells the model how quickly to adjust parameters\n",
            "        self.smooth_loss: float = 0  # Initialising the loss to zero\n",
            "        self.number: int = layer_number  # The number of the layer in the network\n",
            "        # If the layer is the last layer this tells the size of\n",
            "        # the output often the same as the input for the first layer\n",
            "        self.output_size: int = output_size\n",
            "        # define the variables used in the layer\n",
            "        self.xs: dict[int, np.ndarray] = {}  # Input\n",
            "        self.hs: dict[int, np.ndarray] = {}  # Hidden state\n",
            "        self.fg: dict[int, np.ndarray] = {}  # Forget gate\n",
            "        self.ig: dict[int, np.ndarray] = {}  # Input gate\n",
            "        self.og: dict[int, np.ndarray] = {}  # Output gate\n",
            "        self.mg: dict[int, np.ndarray] = {}  # Memory gate\n",
            "        self.cs: dict[int, np.ndarray] = {}  # Cell gate\n",
            "        self.ys: dict[int, np.ndarray] = {}  # Output\n",
            "        self.ps: dict[int, np.ndarray] = {}  # Probability vector\n",
            "        self.conc: dict[int, np.ndarray] = {}  # Concatenation of xs[t] and hs[t-1]\n",
            "\n",
            "        # Initialize weights and biases for the lstm unit\n",
            "        self.Wf: np.ndarray = np.random.randn(self.hidden_size, self.hidden_size + self.input_size) * 0.0001\n",
            "        self.Wi: np.ndarray = np.random.randn(self.hidden_size, self.hidden_size + self.input_size) * 0.0001\n",
            "        self.Wo: np.ndarray = np.random.randn(self.hidden_size, self.hidden_size + self.input_size) * 0.0001\n",
            "        self.Wm: np.ndarray = np.random.randn(self.hidden_size, self.hidden_size + self.input_size) * 0.0001\n",
            "\n",
            "        self.Bf: np.ndarray = np.zeros((self.hidden_size, 1))\n",
            "        self.Bi: np.ndarray = np.zeros((self.hidden_size, 1))\n",
            "        self.Bo: np.ndarray = np.zeros((self.hidden_size, 1))\n",
            "        self.Bm: np.ndarray = np.zeros((self.hidden_size, 1))\n",
            "\n",
            "        # initialize weights and biases for output layer\n",
            "        self.Wy: np.ndarray = np.random.randn(self.output_size, self.hidden_size)\n",
            "        self.By: np.ndarray = np.zeros((self.output_size, 1))\n",
            "\n",
            "        # define variables to store the parameter updates to\n",
            "        self.dWf: np.ndarray = np.zeros_like(self.Wf)\n",
            "        self.dWi: np.ndarray = np.zeros_like(self.Wi)\n",
            "        self.dWo: np.ndarray = np.zeros_like(self.Wo)\n",
            "        self.dWm: np.ndarray = np.zeros_like(self.Wm)\n",
            "\n",
            "        self.dBf: np.ndarray = np.zeros_like(self.Bf)\n",
            "        self.dBi: np.ndarray = np.zeros_like(self.Bi)\n",
            "        self.dBo: np.ndarray = np.zeros_like(self.Bo)\n",
            "        self.dBm: np.ndarray = np.zeros_like(self.Bm)\n",
            "\n",
            "        self.dWy: np.ndarray = np.zeros_like(self.Wy)\n",
            "        self.dBy: np.ndarray = np.zeros_like(self.By)\n",
            "\n",
            "        # define variables for Adagrad memories\n",
            "        self.mWf: np.ndarray = np.zeros_like(self.Wf)\n",
            "        self.mWi: np.ndarray = np.zeros_like(self.Wi)\n",
            "        self.mWo: np.ndarray = np.zeros_like(self.Wo)\n",
            "        self.mWm: np.ndarray = np.zeros_like(self.Wm)\n",
            "\n",
            "        self.mBf: np.ndarray = np.zeros_like(self.Bf)\n",
            "        self.mBi: np.ndarray = np.zeros_like(self.Bi)\n",
            "        self.mBo: np.ndarray = np.zeros_like(self.Bo)\n",
            "        self.mBm: np.ndarray = np.zeros_like(self.Bm)\n",
            "\n",
            "        self.mWy: np.ndarray = np.zeros_like(self.Wy)\n",
            "        self.mBy: np.ndarray = np.zeros_like(self.By)\n",
            "\n",
            "        # define variables for derivative checking\n",
            "        self.cWf: np.ndarray = np.zeros_like(self.Wf)\n",
            "        self.cWi: np.ndarray = np.zeros_like(self.Wi)\n",
            "        self.cWo: np.ndarray = np.zeros_like(self.Wo)\n",
            "        self.cWm: np.ndarray = np.zeros_like(self.Wm)\n",
            "\n",
            "        self.cBf: np.ndarray = np.zeros_like(self.Bf)\n",
            "        self.cBi: np.ndarray = np.zeros_like(self.Bi)\n",
            "        self.cBo: np.ndarray = np.zeros_like(self.Bo)\n",
            "        self.cBm: np.ndarray = np.zeros_like(self.Bm)\n",
            "\n",
            "        self.cWy: np.ndarray = np.zeros_like(self.Wy)\n",
            "        self.cBy: np.ndarray = np.zeros_like(self.By)\n",
            "\n",
            "        self.prev_hidden: np.ndarray = np.zeros((self.hidden_size, 1))\n",
            "        self.prev_cell: np.ndarray = np.zeros((self.hidden_size, 1))\n",
            "\n",
            "    def __str__(self):\n",
            "        # Return the layer as a string\n",
            "        if self.output_size != 0:\n",
            "            out_size = self.output_size\n",
            "        else:\n",
            "            out_size = self.hidden_size\n",
            "        return f&quot;LSTM(Layer no. {self.number}, input size: {self.input_size},&quot; \\\n",
            "               f&quot;hidden size: {self.hidden_size}, output size: {out_size})&quot;\n",
            "\n",
            "    @staticmethod\n",
            "    def sigmoid(x: np.ndarray) -&gt; np.ndarray:\n",
            "        return 1 / (1 + np.exp(- x, dtype=&quot;float64&quot;))\n",
            "\n",
            "    @staticmethod\n",
            "    def sigmoid_derivative(x: np.ndarray) -&gt; np.ndarray:\n",
            "        return x * (1 - x)\n",
            "\n",
            "    @staticmethod\n",
            "    def tanh_derivative(x: np.ndarray) -&gt; np.ndarray:\n",
            "        return 1 - x * x\n",
            "\n",
            "    def initialise_layer(self) -&gt; None:\n",
            "        # initialise the parameters for the iteration to empty or zero\n",
            "        self.xs, self.hs, self.fg, self.ig, self.og, self.mg, self.cs = {}, {}, {}, {}, {}, {}, {}\n",
            "        self.ys, self.ps = {}, {}\n",
            "        self.conc = {}\n",
            "\n",
            "        for i in [self.dWf, self.dWi, self.dWo, self.dWm, self.dBf, self.dBi, self.dBo, self.dBm, self.dWy, self.dBy]:\n",
            "            i.fill(0)\n",
            "\n",
            "    def format_text_input(self, inputs: list[int]) -&gt; None:\n",
            "        for t, k in enumerate(inputs):\n",
            "            self.xs[t] = np.zeros((self.input_size, 1))\n",
            "            self.xs[t][inputs[t]] = 1\n",
            "\n",
            "    def format_non_text_input(self, inputs: dict[int, np.ndarray]) -&gt; None:\n",
            "        for t, k in enumerate(inputs):\n",
            "            self.xs[t] = inputs[k]\n",
            "\n",
            "    def forward_pass(self) -&gt; dict[int, np.ndarray]:\n",
            "        self.hs[-1] = self.prev_hidden\n",
            "        self.cs[-1] = self.prev_cell\n",
            "        for t, k in enumerate(self.xs):\n",
            "            self.conc[t] = np.concatenate((self.xs[t], self.hs[t - 1]), axis=0)\n",
            "            self.fg[t] = self.sigmoid(self.Wf @ self.conc[t] + self.Bf)\n",
            "            self.ig[t] = self.sigmoid(self.Wi @ self.conc[t] + self.Bi)\n",
            "            self.og[t] = self.sigmoid(self.Wo @ self.conc[t] + self.Bo)\n",
            "            self.mg[t] = np.tanh(self.Wm @ self.conc[t] + self.Bm)\n",
            "\n",
            "            self.cs[t] = self.fg[t] * self.cs[t - 1] + self.ig[t] * self.mg[t]\n",
            "            self.hs[t] = self.og[t] * self.cs[t]\n",
            "        self.prev_hidden = self.hs[len(self.xs) - 1]\n",
            "        return self.hs\n",
            "\n",
            "    def output_layer(self) -&gt; dict[int, np.ndarray]:\n",
            "        hid: dict[int, np.ndarray] = self.forward_pass().copy()\n",
            "        hid.pop(-1)\n",
            "        for t, k in enumerate(hid):\n",
            "            self.ys[t] = self.Wy @ self.hs[t] + self.By\n",
            "            yt = self.ys[t] - np.max(self.ys[t])\n",
            "            self.ps[t] = np.exp(yt) / np.sum(np.exp(yt))\n",
            "        return self.ps\n",
            "\n",
            "    def backpropagation_output_layer(self, targets: list[int]) -&gt; dict[int, np.ndarray]:\n",
            "        dout: dict[int, np.ndarray] = {}\n",
            "        for t in reversed(range(len(targets))):\n",
            "            # Backpropagation into y\n",
            "            dy = np.copy(self.ps[t])\n",
            "\n",
            "            dy[targets[t]] -= 1\n",
            "\n",
            "            self.dBy += dy\n",
            "            self.dWy += dy @ self.hs[t].T\n",
            "\n",
            "            dout[t] = self.Wy.T @ dy\n",
            "\n",
            "        return dout\n",
            "\n",
            "    def backpropagation_lstm_layer(self, dh: dict[int, np.ndarray]) -&gt; dict[int, np.ndarray]:\n",
            "\n",
            "        dcnext: np.ndarray = np.zeros((self.hidden_size, 1))\n",
            "        dx: dict[int, np.ndarray] = {}\n",
            "        for t in reversed(range(len(dh))):\n",
            "\n",
            "            dout_dh = dh[t]\n",
            "            dh_dct = dout_dh * self.og[t] + dcnext\n",
            "            dh_do = self.cs[t]\n",
            "\n",
            "            dct_dct_1 = dh_dct * self.fg[t]\n",
            "            dct_df = dh_dct * self.cs[t-1]\n",
            "            dct_di = dh_dct * self.mg[t]\n",
            "            dct_dm = dh_dct * self.ig[t]\n",
            "\n",
            "            df_dzf = dct_df * (self.fg[t] * (1 - self.fg[t]))\n",
            "            self.dWf += df_dzf @ self.conc[t].T\n",
            "            self.dBf += df_dzf\n",
            "            dzf_dx = self.Wf.T @ df_dzf\n",
            "\n",
            "            di_dzi = dct_di * (self.ig[t] * (1 - self.ig[t]))\n",
            "            self.dWi += di_dzi @ self.conc[t].T\n",
            "            self.dBi += di_dzi\n",
            "            dzi_dx = self.Wi.T @ di_dzi\n",
            "\n",
            "            dm_dzm = dct_dm * (1 - np.square(self.mg[t]))\n",
            "            self.dWm += dm_dzm @ self.conc[t].T\n",
            "            self.Bm += dm_dzm\n",
            "            dzm_dx = self.Wm.T @ dm_dzm\n",
            "\n",
            "            do_dzo = dh_do * (self.og[t] * (1 - self.og[t]))\n",
            "            self.dWo += do_dzo @ self.conc[t].T\n",
            "            self.Bo += do_dzo\n",
            "            dzo_dx = self.Wo.T @ do_dzo\n",
            "\n",
            "            dx[t] = (dzo_dx + dzm_dx + dzf_dx + dzi_dx)[:self.input_size, :]\n",
            "\n",
            "        return dx\n",
            "\n",
            "    def adagrad(self) -&gt; None:\n",
            "        self.mWf += np.square(self.dWf)\n",
            "        self.mWi += np.square(self.dWi)\n",
            "        self.mWo += np.square(self.dWo)\n",
            "        self.mWm += np.square(self.dWm)\n",
            "\n",
            "        self.mBf += np.square(self.dBf)\n",
            "        self.mBi += np.square(self.dBi)\n",
            "        self.mBo += np.square(self.dBo)\n",
            "        self.mBm += np.square(self.dBm)\n",
            "\n",
            "        self.mWy += np.square(self.dWy)\n",
            "        self.mBy += np.square(self.dBy)\n",
            "\n",
            "        self.Wf -= (self.learning_rate / np.sqrt(np.diagonal(self.mWf).copy() + 1e-8)) @ self.dWf\n",
            "        self.Wi -= (self.learning_rate / np.sqrt(np.diagonal(self.mWi).copy() + 1e-8)) @ self.dWi\n",
            "        self.Wo -= (self.learning_rate / np.sqrt(np.diagonal(self.mWo).copy() + 1e-8)) @ self.dWo\n",
            "        self.Wm -= (self.learning_rate / np.sqrt(np.diagonal(self.mWm).copy() + 1e-8)) @ self.dWm\n",
            "\n",
            "        self.Bf -= (self.learning_rate / np.sqrt(self.mBf + 1e-8)) * self.dBf\n",
            "        self.Bi -= (self.learning_rate / np.sqrt(self.mBi + 1e-8)) * self.dBi\n",
            "        self.Bo -= (self.learning_rate / np.sqrt(self.mBo + 1e-8)) * self.dBo\n",
            "        self.Bm -= (self.learning_rate / np.sqrt(self.mBm + 1e-8)) * self.dBm\n",
            "\n",
            "        self.Wy -= (self.learning_rate / np.sqrt(np.diagonal(self.mWy).copy() + 1e-8)) @ self.dWy\n",
            "        self.By -= (self.learning_rate / np.sqrt(self.mBy + 1e-8)) * self.dBy\n",
            "\n",
            "    def sgd(self) -&gt; None:\n",
            "        self.Wf -= self.learning_rate * self.dWf\n",
            "        self.Wi -= self.learning_rate * self.dWi\n",
            "        self.Wo -= self.learning_rate * self.dWo\n",
            "        self.Wm -= self.learning_rate * self.dWm\n",
            "\n",
            "        self.Bf -= self.learning_rate * self.dBf\n",
            "        self.Bi -= self.learning_rate * self.dBi\n",
            "        self.Bo -= self.learning_rate * self.dBo\n",
            "        self.Bm -= self.learning_rate * self.dBm\n",
            "\n",
            "        self.Wy -= self.learning_rate * self.dWy\n",
            "        self.By -= self.learning_rate * self.dBy\n",
            "\n",
            "    def loss(self, targets: list[int]) -&gt; float:\n",
            "        # Calculate Loss\n",
            "        loss = 0\n",
            "        for t in range(len(targets)):\n",
            "            loss += - np.log(self.ps[t][targets[t]] + 1e-5)\n",
            "        self.smooth_loss = 0.999 * self.smooth_loss + 0.001 * loss\n",
            "        return self.smooth_loss\n",
            "\n",
            "    def forward_check(self, wf, wi, wo, wm, bf, bi, bo, bm) -&gt; dict[int, np.ndarray]:\n",
            "        hs, cs, fg, ig, og, mg, cs, = {}, {}, {}, {}, {}, {}, {}\n",
            "        # Initialise h_t-1 and c_t-1 to zero-arrays\n",
            "        hs[-1] = self.prev_hidden\n",
            "        cs[-1] = self.prev_cell\n",
            "        # forward pass with one variable slightly changed\n",
            "        for t, k in enumerate(self.xs):\n",
            "            # No need to initialise self.conc as it exists from forward pass\n",
            "            fg[t] = self.sigmoid(wf @ self.conc[t] + bf)\n",
            "            ig[t] = self.sigmoid(wi @ self.conc[t] + bi)\n",
            "            og[t] = self.sigmoid(wo @ self.conc[t] + bo)\n",
            "            mg[t] = np.tanh(wm @ self.conc[t] + bm)\n",
            "\n",
            "            cs[t] = fg[t] * cs[t - 1] + ig[t] * mg[t]\n",
            "            hs[t] = og[t] * cs[t]\n",
            "        return hs.pop(-1)\n",
            "\n",
            "    def output_check(self, wy, by):\n",
            "        hid: dict[int, np.ndarray] = self.forward_pass().copy()\n",
            "        ys, hs, ps = {}, {}, {}\n",
            "        hid.pop(-1)\n",
            "        hs = hid\n",
            "        for t, k in enumerate(hid):\n",
            "            ys[t] = wy @ self.hs[t] + by\n",
            "            yt = ys[t] - np.max(ys[t])\n",
            "            ps[t] = np.exp(yt) / np.sum(np.exp(yt))\n",
            "        return ps\n",
            "\n",
            "    def grad_check(self) -&gt; float:\n",
            "        # Function for calculating the hidden states to check the derivatives\n",
            "        def h(variable: str = &quot;default&quot;, d=1e-20):\n",
            "            match variable.lower():\n",
            "                case &quot;wf&quot;:\n",
            "                    return self.forward_check(wf=self.Wf + d, wi=self.Wi, wo=self.Wo, wm=self.Wm, bf=self.Bf,\n",
            "                                              bi=self.Bi, bo=self.Bo, bm=self.Bm)\n",
            "                case &quot;wi&quot;:\n",
            "                    return self.forward_check(wf=self.Wf, wi=self.Wi + d, wo=self.Wo, wm=self.Wm, bf=self.Bf,\n",
            "                                              bi=self.Bi, bo=self.Bo, bm=self.Bm)\n",
            "                case &quot;wo&quot;:\n",
            "                    return self.forward_check(wf=self.Wf, wi=self.Wi, wo=self.Wo + d, wm=self.Wm, bf=self.Bf,\n",
            "                                              bi=self.Bi, bo=self.Bo, bm=self.Bm)\n",
            "                case &quot;wm&quot;:\n",
            "                    return self.forward_check(wf=self.Wf, wi=self.Wi, wo=self.Wo, wm=self.Wm + d, bf=self.Bf,\n",
            "                                              bi=self.Bi, bo=self.Bo, bm=self.Bm)\n",
            "                case &quot;bf&quot;:\n",
            "                    return self.forward_check(wf=self.Wf, wi=self.Wi, wo=self.Wo, wm=self.Wm, bf=self.Bf + d,\n",
            "                                              bi=self.Bi, bo=self.Bo, bm=self.Bm)\n",
            "                case &quot;bi&quot;:\n",
            "                    return self.forward_check(wf=self.Wf, wi=self.Wi, wo=self.Wo, wm=self.Wm, bf=self.Bf,\n",
            "                                              bi=self.Bi + d, bo=self.Bo, bm=self.Bm)\n",
            "                case &quot;bo&quot;:\n",
            "                    return self.forward_check(wf=self.Wf, wi=self.Wi, wo=self.Wo, wm=self.Wm, bf=self.Bf, bi=self.Bi,\n",
            "                                              bo=self.Bo + d, bm=self.Bm)\n",
            "                case &quot;bm&quot;:\n",
            "                    return self.forward_check(wf=self.Wf, wi=self.Wi, wo=self.Wo, wm=self.Wm, bf=self.Bf, bi=self.Bi,\n",
            "                                              bo=self.Bo, bm=self.Bm + d)\n",
            "                case &quot;default&quot;:\n",
            "                    return self.forward_check(wf=self.Wf, wi=self.Wi, wo=self.Wo, wm=self.Wm, bf=self.Bf, bi=self.Bi,\n",
            "                                              bo=self.Bo, bm=self.Bm)\n",
            "\n",
            "        dx = 1e-7\n",
            "        if self.output_size != 0:\n",
            "            for i, j, k in zip(self.output_check(self.Wy + dx, self.By), self.output_check(self.Wy, self.By + dx),\n",
            "                               self.output_check(self.Wy, self.By)):\n",
            "                self.cWy += (i - k) / dx\n",
            "                self.cBy += (j - k) / dx\n",
            "\n",
            "        for t, j in enumerate(h(d=dx)):\n",
            "            self.cWf += (h(&quot;wf&quot;, d=dx)[t] - j) / dx\n",
            "            self.cWi += (h(&quot;wi&quot;, d=dx)[t] - j) / dx\n",
            "            self.cWo += (h(&quot;wo&quot;, d=dx)[t] - j) / dx\n",
            "            self.cWm += (h(&quot;wm&quot;, d=dx)[t] - j) / dx\n",
            "            self.cBf += (h(&quot;bf&quot;, d=dx)[t] - j) / dx\n",
            "            self.cBi += (h(&quot;bi&quot;, d=dx)[t] - j) / dx\n",
            "            self.cBo += (h(&quot;bo&quot;, d=dx)[t] - j) / dx\n",
            "            self.cBm += (h(&quot;bm&quot;, d=dx)[t] - j) / dx\n",
            "\n",
            "        dcwf = np.average(np.absolute(self.cWf - self.dWf))\n",
            "        dcwi = np.average(np.absolute(self.cWi - self.dWi))\n",
            "        dcwo = np.average(np.absolute(self.cWo - self.dWo))\n",
            "        dcwm = np.average(np.absolute(self.cWm - self.dWm))\n",
            "        dcbf = np.average(np.absolute(self.cBf - self.dBf))\n",
            "        dcbi = np.average(np.absolute(self.cBi - self.dBi))\n",
            "        dcbo = np.average(np.absolute(self.cBo - self.dBo))\n",
            "        dcbm = np.average(np.absolute(self.cBm - self.dBm))\n",
            "        print(f&quot;Layer: {self.number}, Wf-error: {dcwf:.2e}&quot;)\n",
            "        print(f&quot;Layer: {self.number}, Wi-error: {dcwi:.2e}&quot;)\n",
            "        print(f&quot;Layer: {self.number}, Wo-error: {dcwo:.2e}&quot;)\n",
            "        print(f&quot;Layer: {self.number}, Wm-error: {dcwm:.2e}&quot;)\n",
            "        print(f&quot;Layer: {self.number}, Bf-error: {dcbf:.2e}&quot;)\n",
            "        print(f&quot;Layer: {self.number}, Bi-error: {dcbi:.2e}&quot;)\n",
            "        print(f&quot;Layer: {self.number}, Bo-error: {dcbo:.2e}&quot;)\n",
            "        print(f&quot;Layer: {self.number}, Bm-error: {dcbm:.2e}&quot;)\n",
            "        sum_diff = dcwf + dcwi + dcwo + dcwm + dcbf + dcbi + dcbo + dcbm\n",
            "        if self.output_size != 0:\n",
            "            print(f&quot;Output Wy-error: {np.average(np.absolute(self.cWy - self.dWy)):.2e}&quot;)\n",
            "            print(f&quot;Output By-error: {np.average(np.absolute(self.cBy - self.dBy)):.2e}&quot;)\n",
            "        return sum_diff\n",
            "\n",
            "    def sample__input(self, x0: int) -&gt; np.ndarray:\n",
            "        x = np.zeros((self.input_size, 1))\n",
            "        x[x0] = 1\n",
            "        return x\n",
            "\n",
            "    def sample_lstm(self, x: np.ndarray, h_prev: np.ndarray, c: np.ndarray) -&gt; (np.ndarray, np.ndarray):\n",
            "        x = np.concatenate((x, h_prev), axis=0)\n",
            "        f = self.sigmoid(self.Wf @ x + self.Bf)\n",
            "        i = self.sigmoid(self.Wi @ x + self.Bi)\n",
            "        o = np.tanh(self.Wo @ x + self.Bo)\n",
            "        m = self.sigmoid(self.Wm @ x + self.Bm)\n",
            "        c = f * c + i * m\n",
            "        h = o * c\n",
            "        return h, c\n",
            "\n",
            "    def sample_out(self, h: np.ndarray) -&gt; int:\n",
            "        y = self.Wy @ h + self.By\n",
            "        y = y - np.max(y)\n",
            "        p = (np.exp(y) / np.sum(np.exp(y))).ravel()\n",
            "        try:\n",
            "            ix = int(np.random.choice(range(self.output_size), size=1, p=p))\n",
            "        except ValueError as e:\n",
            "            print(e)\n",
            "            print(p)\n",
            "            ix = 0\n",
            "        return ix\n",
            "\n",
            "    def change_output(self, output_size: int) -&gt; None:\n",
            "        self.output_size = output_size\n",
            "        self.Wy = np.random.randn(self.output_size, self.hidden_size)\n",
            "        self.By = np.random.randn(self.output_size, 1)\n",
            "\n",
            "class LSTM:\n",
            "    def __init__(self, layer_sizes: list[int], sequence_length: int, learning_rate: float, training_data: str):\n",
            "        # Training data\n",
            "        self.data: str = training_data\n",
            "        # Length of a single sequence\n",
            "        self.sequence_length: int = sequence_length\n",
            "        # Formatted training data as a list of characters\n",
            "        self.data_set: list[str] = sorted(list(set(training_data)))\n",
            "        # The learning rate\n",
            "        self.learning_rate: float = learning_rate\n",
            "        # The amount of different characters in the data set\n",
            "        self.dict_size = len(self.data_set)\n",
            "        # Converting the characters into integers and vice-versa\n",
            "        self.char_to_ix = {ch: i for i, ch in enumerate(self.data_set)}\n",
            "        self.ix_to_char = {i: ch for i, ch in enumerate(self.data_set)}\n",
            "        # The sizes of the different layers\n",
            "        self.layer_sizes: list[int] = layer_sizes\n",
            "        # Initialising the previous hidden and cell states\n",
            "        self.prev_hidde = [np.zeros((i, 1)) for i in self.layer_sizes]\n",
            "        self.prev_cells = [np.zeros((i, 1)) for i in self.layer_sizes]\n",
            "        prev_size = self.layer_sizes[0]\n",
            "        self.layers = []\n",
            "        # Initialise the layers to a list\n",
            "        for layer, size in enumerate(self.layer_sizes):\n",
            "            if layer == 0:\n",
            "                # First layer parameters: Vocab size and hidden size\n",
            "                self.layers.append(LSTMLayer(layer, self.dict_size, size, self.learning_rate))\n",
            "            elif layer == len(layer_sizes) - 1:\n",
            "                # Last layer previous hidden size and output is the vocab size\n",
            "                self.layers.append(LSTMLayer(layer, prev_size, size, self.learning_rate, output_size=self.dict_size))\n",
            "                prev_size = size\n",
            "            else:\n",
            "                # Initialising the rest of the layers\n",
            "                self.layers.append(LSTMLayer(layer, prev_size, size, self.learning_rate))\n",
            "                prev_size = size\n",
            "\n",
            "        print(f&quot;Layers of the network with dictionary of {self.dict_size} characters:&quot;)\n",
            "        for i in self.layers:\n",
            "            print(f&quot;\\t{i}&quot;)\n",
            "\n",
            "        self.loss: list[float] = [0]\n",
            "\n",
            "    def initialise_layers(self) -&gt; None:\n",
            "        # Initialise the parameters for each layer before an iteration\n",
            "        for layer in self.layers:\n",
            "            layer.initialise_layer()\n",
            "\n",
            "    def train(self):\n",
            "        # Initialising the counters n the iteration p the sequence counter\n",
            "        n, p = 0, 0\n",
            "\n",
            "        while True:\n",
            "            # Check if p fits inside the data\n",
            "            if p &gt;= len(self.data):\n",
            "                p = 0\n",
            "            # Set the training data for the iteration\n",
            "            iter_in = [self.char_to_ix[i] for i in self.data[p: p + self.sequence_length]]\n",
            "            iter_out = [self.char_to_ix[i] for i in self.data[p + 1: p + self.sequence_length + 1]]\n",
            "\n",
            "            self.initialise_layers()\n",
            "            out: dict[int, np.ndarray] = {}\n",
            "            for layerN, layer in enumerate(self.layers):\n",
            "                if layerN == 0:\n",
            "                    layer.format_text_input(iter_in)\n",
            "                    out: dict[int, np.ndarray] = layer.forward_pass()\n",
            "                elif layerN == len(self.layers) - 1:\n",
            "                    layer.format_non_text_input(out)\n",
            "                    out = layer.output_layer()\n",
            "                    self.loss.append(float(layer.loss(iter_out)))\n",
            "                elif layerN &gt;= len(self.layers) - 1:\n",
            "                    continue\n",
            "                else:\n",
            "                    layer.format_non_text_input(out)\n",
            "                    out = layer.forward_pass()\n",
            "\n",
            "            dh = {}\n",
            "            # Backpropagation\n",
            "            for layerN, layer in enumerate(reversed(self.layers)):\n",
            "                if layerN == 0:\n",
            "                    dy = layer.backpropagation_output_layer(iter_out)\n",
            "                    dh = layer.backpropagation_lstm_layer(dy)\n",
            "                else:\n",
            "                    dh = layer.backpropagation_lstm_layer(dh)\n",
            "            if n % 1000 == 0:\n",
            "                for layer in self.layers:\n",
            "                    layer.grad_check()\n",
            "            # Update parameters\n",
            "            for layer in self.layers:\n",
            "                layer.sgd()\n",
            "            # Print out the sample and the details\n",
            "            if n % 50 == 0:\n",
            "                print(self.details(n), &quot;\\n&quot;)\n",
            "                plt.plot(self.loss)\n",
            "                plt.savefig(&quot;lossLSTM.png&quot;)\n",
            "                plt.close()\n",
            "\n",
            "            if n % 100 == 0:\n",
            "                #print(f&quot;Sample: \\n[{self.sample(100)}]\\n&quot;)\n",
            "                pass\n",
            "\n",
            "            if n % 1000 == 0:\n",
            "                print(self.sample(1000))\n",
            "            # Increment n and p at the end of the iteration\n",
            "            n += 1\n",
            "            p += self.sequence_length\n",
            "\n",
            "    def sample(self, size):\n",
            "        # Randomise the first character\n",
            "        rand_x = np.random.randint(0, self.dict_size)\n",
            "        x = self.layers[0].sample__input(rand_x)\n",
            "        chars = [int(rand_x)]\n",
            "        text = &quot;&quot;.join(self.ix_to_char[chars[0]])\n",
            "        txt = &quot;&quot;\n",
            "        hiddens = [np.zeros((i.hidden_size, 1)) for i in self.layers]\n",
            "        cells = [np.zeros((i.hidden_size, 1)) for i in self.layers]\n",
            "        for n in range(size):\n",
            "            for i, layer in enumerate(self.layers):\n",
            "                x = layer.sample_lstm(x, hiddens[i], cells[i])\n",
            "                hiddens[i] = x[0]\n",
            "                cells[i] = x[1]\n",
            "                x = x[0]\n",
            "            y = self.layers[-1].sample_out(x)\n",
            "            x = self.layers[0].sample__input(y)\n",
            "            chars.append(self.ix_to_char[y])\n",
            "            text += chars[-1]\n",
            "            txt = &quot;&quot;\n",
            "            for char in text:\n",
            "                if char == &quot;\\n&quot;:\n",
            "                    txt += &quot;[/linebreak]&quot;\n",
            "                else:\n",
            "                    txt += char\n",
            "        return txt\n",
            "\n",
            "    def details(self, n):\n",
            "        print(f&quot;n: {n}, loss: {self.loss[-1]}, timestep sample: [{self.sample(30)}]&quot;)\n",
            "\n",
            "\n",
            "-------------------\n",
            "The reason your OCR program is not recognizing anything is that the font size is very small. If you resize your image to increase its size, you will obtain better results. For example, using ImageMagick to apply a fixed threshold to remove the background on your first image and increase its size:\n",
            "\n",
            "convert -density 500 -threshold 40% 29-1891a.gif -resize 250% output.tiff\n",
            "\n",
            "After this, tesseract does a reasonable job:\n",
            "\n",
            "tesseract output.tiff output test.config \n",
            "\n",
            "I included a configuration file test.config to tesseract in order to restrict the permitted characters. In this way, we don't obtain mistaken unicode characters in our text.\n",
            "\n",
            "test.config\n",
            "\n",
            "tessedit_char_whitelist ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-\n",
            "\n",
            "\n",
            "This is the result:\n",
            "\n",
            "HOWARD COUNTY 10011607\n",
            "\n",
            "NALL JOHN E 0-686196 1 LT F05 DALE MARVIN E 30037267 PVT DON SANSJNG DAVID 10505969 PFC\n",
            "NIXSON EONOND R 10077072 CPL KIA DANIEL NILBORN N 30637064 PVT FOB SAUNDER6 NEWTON L 0-401709 CAPT\n",
            "NOBLES STEPNEN E 30502791 PVT 0N0 DAVIS IR D 30433764 PVT 0N0 SNELTON CHARLES R 14001742 VT\n",
            "OSBURN SILLY F 02008518 2 LT DNB BAVIS JINKIE L 18214208 5 SC KIA SHELTON CHARLIE 33423953 7335\n",
            "PELTON J L 30105473 5 SC RNA 0 DEAN CHARLES 02066314 3 LT 0N0 SMERAN CARLTON A JR 0-510441 2 LT\n",
            "GUEZADA FRANK 5 10014939 PFC KIA IDIXON JOHN N 6202114 PVT 00R SIBLEV JESSE 30037241 PFC\n",
            "RAMIREZ JOSE T 30347914 PPC KIA DREADIN RAYMOND 6950964 PVT KIA SLATER JERONE E JR 0-605266 1 LT\n",
            "OSE THO S 0 30345100 PPC NTA - DUNN GARLAND J 30037202 PVT 0N0 SMITH 00 SE 30509104 PPC\n",
            "R035 ORREN C 16015376 PVT ONE 1 ELDER GERALD P 0-387706 2 LT KIA SNITN JASPER 7 33037355 7 SC\n",
            "RUTLEDCE CARL R 38107116 PVTELPNAA JEARHER MARVIN R 30279097 CPL DON SMITN JERALO D 10005000 CPL\n",
            "SCUDDAY BERNIE L 0-603906 1 LT KIA - FINDLEY PAUL A 50536010 PFC OMB SMITH NELDON A 5550735 PFC\n",
            "SNITH ROBERT L 10015524 507 ONE- ELINC ROY T I 0-724730 CAPT F05 S THEY 4295431 5 5\n",
            "SNITH TRAVIS L 30670530 PVT IDUN FORD HERRELL E 0-665672 CAPT E00 STEPN NSON JESSIE P 10006200 PVT\n",
            "SNEED ROY A 13076927 S 80 ONE GARRETT TOMMY 30220694 PFC KIA SULLIVAN 31 01525125 2 L\n",
            "SOUTN CARL 0 JR 30343379 PVT OMB CATLOR R T 20012665 SOT KIA SNINDELL VERL O 30037252 7005\n",
            "STEVENS JANESCO 10015557 PVT DNR CLASSCOCN CNARLES J 6571000 5 SO ONO TAYLOR ARTHUR V 33531479 PFC\n",
            "STENA D JO 0-723036 2 LT POL 7 00 A 10007027 PVT DOM ITOHN E N A JR 30035251 5 SO\n",
            "UTTON ARVI 6360431 AV C 0N6 COSSETT JANES N 30436055 CPL KIAI ALKER J NEE H 30117597 SOT\n",
            "IALBOTT CHARLES 9 38342083 PFC KIA GRESN J 18126923 5 SO KIA NALKER RALPH L 203126071PPC\n",
            "TUCKER JAMES 30848763 PVT MIA CRIFFIS WILLIAM J 6270119 PFC 0N0 NALLACE JESSE A 30043713 T SC\n",
            "-TUCKER STERLING P 33341143 RPCI KIA CROSS ELERY C 0-390713 3 LT DNR NARREN CHARLES D 30110253 5 50\n",
            "NAOSNORTN PAUL P 30345079 5 SC ONE HAKNONBS ROBERT M 38433940 3 SC KIA MASHINCTON HENRY 30299100 PFC\n",
            "NALNER JAN S H JR 0-696023 2 LT P0 HANDLSY JAMES J JR 0-417955 1 LT ONE EEMS NINPRED E 20017930 CPL\n",
            "NEBB GLEN 30343104 PVT -K1 7 NANET FARRELL 8 30012597 SCT KIA NNITE DE NIS 10124415 PFC\n",
            "NRAT JAMES H 30067743 TE05 KIA NARCIE PRANCNARD 6370053 1 SC NIA NMITE MARVIN J 30430974 PFC\n",
            "NRIOHT NAILAND 0 30609570 PFC KIA NARKEY VEWCEN 30424190 PVT KIA NOODARD BILLY E 510217472 T 5\n",
            "HARRIS PRBS 38111537 PFC OKIA NRICNT EILL 0-725355 CAPT\n",
            "I HARRISO DUKE N JR 18055432 AV 0 ONE YOST TNURNAN R 6273153 PVT\n",
            "HENDRIX JOHN N JR 10217563 PVT 0N8\n",
            "HICKERSDN JACK 04431540 1 LT 9N8\n",
            "HUDSPETH COUNTY 3536305354 A CHEW 55555 W\n",
            "9 0776 A -\n",
            "- JOHNSTON LONNIE O 0-754906 3 LT KIA H INSON COUNTY\n",
            "ONES GEORBE N 01173315 1 LT ONE UTCH\n",
            "JUMPER ISAAC H 30605960 PVT KIA I\n",
            "A9515 HAN C -7 7 3 LT N5 LONG OSCAR D 30812573 5 SC KIA ALEXANDER BOYD A 0-690739 2 LT\n",
            "EARDNERNE3SEPN H 3L5E3355 3 L7 335 LVTLE JOHN E 30433946 PVT KIA EALDNIN JANES H 0-407114 CAPT\n",
            "NORALES ALFREDO L 18015539 3 CG P05 MACK HULET 00057345 1 LT NIA EICCERSTAPP C N 34530001 PC\n",
            "54N1952 LOCAS N 39441440 PFC K14 NAJORS TRUSTT J 00410046 2 LT KIA BRITTON JAMES H 18036279 8 SC\n",
            "ROBLES VICENTE 30570733 PFC 0N5 MASON DICE 30203437 CPL NIA EULLARD CAR 5 30607623 PVT\n",
            "SANCNE2 ANGEL R 38310341 PVT ONE MASON HALTER P 0-671673 3 LT NIA C IN ROBE T E 33105799 TECS\n",
            "VALLES RENICIO 0 30441430 PIC DON NASSEV JESSE D 30435424 PVT KIA COHA K LL03 N 10077256 PVT\n",
            "NC CLENDON J H 30117530 PPC ONE EVANS L 0 37392406 TECS\n",
            "NC HNORTER C R 30300391 PVT KIA FIRLEY MARVIN L 38572402 PVT\n",
            "4 HILLNAN ODEAN R T-000345 FL 0 DNB IPIELDS JAKES 35711546 PVT\n",
            "HIL10N JACKSON 7 13030591 PFC 933 ORADDY ROY L 39112920 PFC\n",
            "HUNT COUNTY HOORE ARLON D 6295620 9 SC KIA GRANT 30572401 PVT\n",
            "MORRISON DURHARD D 0-519411 3 LT KIA HANNA EVERETT T 10104243 S SC\n",
            "- LNULLINS GERALD D 6295622 T 50 NIA HANSARD SAMUEL 2 04754619 1 LT\n",
            "NEAL HOMER M 30609200 PVT KIA THARVEY JOHN B 0-602116 2 LT\n",
            "ALANIS VICENTE 30894642 VT KIA MEAL RAVN 39 9 1759550 3 NECOAL MARTIN J 6396940 PFC\n",
            "ALLEN TRUHAN L 0-123936 1 LT KIA NELSON 1 0 30431073 PPET ONE HILL RAYMONO 8 30050142 PVT\n",
            "ALLEY NILBUR K 01703993 2 LT KIA NELSON TRAVIS C 38204649 PVT DOH HOP JACK H D 30342393 PFC\n",
            "BENCH CLARENCE A 30003600 PFC KIA INEWLAND OTIS T T-000346 FL 0 F051 HDFF ROBERT C 30345133 T 50\n",
            "BENNETT EVERETT N 0-692130 LT 0N5 NICNOLSON DALE- 10005042 PVT KIA JONES JANES 0 30343505 PVT\n",
            "I I\n",
            "A NIXON LOYD 5041674 TECS KIA KAPPELMN MC 0-360687 CAPT\n",
            "BLACKHELL E C 38357321 307 KIA PARKER N 33130N V 52959524 3 L7 KIA KECANS TIM JR 38342898 FC\n",
            "ERITT BASIL JR 36002307 PFC K14 PATTERSON THOMAS H 18136913 SCT 9N5 KENNINER EARL 0 30401408 SGT\n",
            "BROHN SHOE 8 38634396 PFC KIA PERRI JA 5 0-562333 1 LT KIA LANTRON EDWARD L 19190302 5 SO\n",
            "BURNS VIRCIL P 38035701 PRC KIA PETTICREH FRED 0 16215700 S 50 NIA LESNER ROLLIE H 30050735 AV C\n",
            "BUTLER GEORGE A 37259433 1 SC KIA PHILLIPS C L JR 0-431139 CAPT KIA NC CARTY TOURNAN P 30711042 PFC\n",
            "CAMERON VANDSLL C 30634570 PPCI KIA PILCRTN CLYDE 30115026 V NIA NC GLENDON JACK 30330395 PFC\n",
            "CARTER LEO RD K 30119959 AV 0 ONE PO D EUCENE N 4 30431074 PRC KIA NC NINNET K E 30340151 PVT\n",
            "CARTER OILLIAH F 38685535 PFC 0N8 PRESLEY NILLIAN H 30037547 PVT 0N5 NC QUEEN JAMES Y 02055059 2 LT\n",
            "CREEK LOTD 6379416 A SC DNB PRICE PREDRICK P 30409331 CPL 100W NOTEN GEORGE N 30304695 PFC\n",
            "CLARK JOHN 18317907 CPL KIA PURCELL SAMUEL N 30017905 PVT 0N5 PANNELL NOLENIA 6227623 1 SC\n",
            "COLLINS RAYMOND N 30137040 PFC OOH RAILIPF NARDEN A 30037006 CPL 00H PIERCE FELIX 0 30335390 5 SC\n",
            "CREAKER SAH BL N 18006101 PVT KIA HAYNES NILLIAN T 7-954739 PL 0 9N5 PIETZSCH JAI E 0-426961 2 LT\n",
            "CRIDER NARLAN 0 10154794 SCT KIA REED JDRN L 30529414 PVT POL RTER NALLACE N JR 38193560 CPL\n",
            "DALE CHARLEY 8 I 38049500 3 SC KIA ROI NILLIS 5 01393956 1 LT KIA PRESCOTT DAVID L JR 0-707191 2 LT\n",
            "05 231\n",
            "\n",
            "\n",
            "\n",
            "D KZXXN D\n",
            "NNNNH 2\n",
            "XXLA-A- Q\n",
            "\n",
            "2\n",
            "33\n",
            "\n",
            "UR R\n",
            "2055\n",
            "35\n",
            "\n",
            "\n",
            "As you can see, there are many mistakes, but at least we are obtaining reasonable results. In any case, you don't want to do this because even if you were able to get perfect recognition, some of your lines would be placed in the wrong county because tesseract is looking at your document as a big paragraph. I recommend you to use the vertical lines to segment your image into 3 parts and then preprocess each one of them. You can even try to concatenate the parts vertically and perform OCR in a single page. This also applies to the second image.\n",
            "\n",
            "By the way, the resolution of your image is not great, so if you can get a better image, that is going to make a big difference. \n",
            "\n",
            "-------------------\n",
            "Will I overfit my LSTM if I train it via the sliding-window approach? Why do people not seem to use it for LSTMs?\n",
            "For a simplified example, assume that we have to predict the sequence of characters:\n",
            "A B C D E F G H I J K L M N O P Q R S T U V W X Y Z\n",
            "\n",
            "Is it bad (or better?) if I keep training my LSTM with the following minibatches:\n",
            "A B C D E F G H I J K L M N, backprop, erase the cell\n",
            "\n",
            "B C D E F G H I J K L M N O, backprop, erase the cell\n",
            "\n",
            " .... and so on, shifting by 1 every time?\n",
            "\n",
            "Previously, I always trained it as:\n",
            "A B C D E F G H I J K L M N,  backprop, erase the cell\n",
            "\n",
            "O P Q R S T U V W X Y Z,  backprop, erase the cell\n",
            "\n",
            "\n",
            "Instead of shifting by one, would it be better to slide the window by 2 entries instead, etc? What would that mean (in terms of precision/overfitting)?\n",
            "\n",
            "Also, if I were to do the sliding-window approach in a Feed-forward network, would it result in overfitting? I would assume yes, because the network is exposed to the same information regions for a very long time. For example, it is exposed to E F G H I J K for a long time.\n",
            "\n",
            "Edit:\n",
            "Please remember that cell state is erased between training batches, so the LSTM will have a &quot;hammer to head&quot; at these times. It's unable to remember what was before OPQRSTUVWXYZ. This means that the LSTM is unable to ever learn that &quot;O&quot; follows the &quot;M&quot;.\n",
            "So, I thought (thus my entire question), why not to give it intermediate (overlapping) batch in between...and in that case why not use multiple overlapping minibatches - to me this would provide a smoother training? Ultimately, that would mean a sliding window for an LSTM.\n",
            "\n",
            "Some useful info I've found after answer was accepted:\n",
            "from here\n",
            "\n",
            "The first word of the English translation is probably highly correlated with the first word of the source sentence. But that means decoder has to consider information from 50 steps ago, and that information needs to be somehow encoded in the vector. Recurrent Neural Networks are known to have problems dealing with such long-range dependencies. In theory, architectures like LSTMs should be able to deal with this, but in practice long-range dependencies are still problematic.\n",
            "For example, researchers have found that reversing the source sequence\n",
            "(feeding it backwards into the encoder) produces significantly better\n",
            "results because it shortens the path from the decoder to the relevant\n",
            "parts of the encoder. Similarly, feeding an input sequence twice also\n",
            "seems to help a network to better memorize things. For example, if one training example is &quot;John went home&quot;, you would give &quot;John went home John went home&quot; to the network as one input.\n",
            "\n",
            "Edit after accepting the answer:\n",
            "Several months after, I am more inclined to use the sliding window approach, as it uses the data better. But in that case you probably don't want to train BCDEFGHIJKLMNO right after ABCDEFGHIJKLMNO.  Instead, shuffle your examples, to gradually and uniformly &quot;brush-in&quot; all of the information into your LSTM.  Give it HIJKLMNOPQRSTU after ABCDEFGHIJKLMNO etc.  That's directly related to Catastrophic Forgetting. As always, monitor the Validation and Test set closely, and stop as soon as you see their errors steadily increasing\n",
            "Also, the &quot;hammer to head&quot; issue can be improved, by using Synthetic Gradients. See its benefit here: (linked answer discusses its benefit of long sequences) https://datascience.stackexchange.com/a/32425/43077\n",
            "\n",
            "-------------------\n",
            "I'm trying to use sklearn pipelines and custom transformers to do outlier removal. What I want to do is identify outliers using an IQR-filter, set the outlier values to 'OUTLIER' (not NaN), and then remove all 'OUTLIER' values in the numerical columns. The reason for setting the otulier values to 'OUTLIER' instead of NaN is because I want to impute existing NaN values while removing outlier values. The pipeline would then be IQR-filter, remove outliers, impute missing values, standard scaler.\n",
            "What I've tried are the following custom transformers:\n",
            "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
            "\n",
            "    def __init__(self, feature_names):\n",
            "        self.feature_names = feature_names\n",
            "        \n",
            "    def fit(self, X, y=None):\n",
            "        return self\n",
            "        \n",
            "    def transform(self, X):\n",
            "        return X[self.feature_names]\n",
            "\n",
            "class IQRFilter(BaseEstimator,TransformerMixin):\n",
            "    def __init__(self,factor=2):\n",
            "        self.factor = factor\n",
            "        \n",
            "    def outlier_detector(self,X,y=None):\n",
            "        X = pd.Series(X).copy()\n",
            "        q1 = X.quantile(0.25)\n",
            "        q3 = X.quantile(0.75)\n",
            "        iqr = q3 - q1\n",
            "        self.lower_bound.append(q1 - (self.factor * iqr))\n",
            "        self.upper_bound.append(q3 + (self.factor * iqr))\n",
            "\n",
            "    def fit(self,X,y=None):\n",
            "        self.lower_bound = []\n",
            "        self.upper_bound = []\n",
            "        X.apply(self.outlier_detector)\n",
            "        return self\n",
            "    \n",
            "    def transform(self,X,y=None):\n",
            "        X = pd.DataFrame(X).copy()\n",
            "        for i in range(X.shape[1]):\n",
            "            x = X.iloc[:, i].copy()\n",
            "            x[(x &lt; self.lower_bound[i]) | (x &gt; self.upper_bound[i])] = 'OUTLIER'\n",
            "            X.iloc[:, i] = x\n",
            "        return X\n",
            "\n",
            "class RemoveIQROutliers(BaseEstimator, TransformerMixin):\n",
            "    def __init__(self):\n",
            "        pass\n",
            "    \n",
            "    def fit(self, X, y=None):\n",
            "        return self\n",
            "    \n",
            "    def transform(self, X):\n",
            "        return X.loc[X != 'OUTLIER']\n",
            "\n",
            "And the following pipeline:\n",
            "numerical_pipeline = Pipeline([\n",
            "    ('FeatureSelector', FeatureSelector(num_cols)),\n",
            "    ('iqr_filter', IQRFilter()),\n",
            "    ('remove_outliers', RemoveIQROutliers()),\n",
            "    ('imputer', SimpleImputer(strategy='median')),\n",
            "    ('std_scaler', StandardScaler())\n",
            "])\n",
            "\n",
            "However, runnning numerical_pipeline.fit_transform(X_train) results in this error output:\n",
            "KeyError                                  Traceback (most recent call last)\n",
            "C:\\Users\\usert~1\\AppData\\Local\\Temp/ipykernel_21276/3029174893.py in &lt;module&gt;\n",
            "----&gt; 1 numerical_pipeline.fit_transform(X_train)\n",
            "\n",
            "~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py in fit_transform(self, X, y, **fit_params)\n",
            "    412         &quot;&quot;&quot;\n",
            "    413         fit_params_steps = self._check_fit_params(**fit_params)\n",
            "--&gt; 414         Xt = self._fit(X, y, **fit_params_steps)\n",
            "    415 \n",
            "    416         last_step = self._final_estimator\n",
            "\n",
            "~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py in _fit(self, X, y, **fit_params_steps)\n",
            "    334                 cloned_transformer = clone(transformer)\n",
            "    335             # Fit or load from cache the current transformer\n",
            "--&gt; 336             X, fitted_transformer = fit_transform_one_cached(\n",
            "    337                 cloned_transformer,\n",
            "    338                 X,\n",
            "\n",
            "c:\\anaconda3\\envs\\datascience\\lib\\site-packages\\joblib\\memory.py in __call__(self, *args, **kwargs)\n",
            "    347 \n",
            "    348     def __call__(self, *args, **kwargs):\n",
            "--&gt; 349         return self.func(*args, **kwargs)\n",
            "    350 \n",
            "    351     def call_and_shelve(self, *args, **kwargs):\n",
            "...\n",
            "-&gt; 5842                 raise KeyError(f&quot;None of [{key}] are in the [{axis_name}]&quot;)\n",
            "   5843 \n",
            "   5844             not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())\n",
            "\n",
            "KeyError: &quot;None of [Index([                                                            ('l', 'i', 'n', 'e', '_', 'n', 'o'),\\n                                                    ('p', 'a', 'r', 't', '_', 'p', 'r', 'i', 'c', 'e'),\\n                                               ('b', 'u', 'y', '_', 'q', 't', 'y', '_', 'd', 'u', 'e'),\\n                 ('l', 'i', 'n', 'e', '_', 't', 'o', 't', 'a', 'l', '_', 'w', 'e', 'i', 'g', 'h', 't'),\\n                                                                                  ('c', 'o', 's', 't'),\\n                                                              ('d', 'i', 's', 'c', 'o', 'u', 'n', 't'),\\n                                ('p', 'o', 'r', 't', '_', 'l', 'e', 'a', 'd', '_', 't', 'i', 'm', 'e'),\\n                                                    ('d', 'e', 'a', 'd', 'w', 'e', 'i', 'g', 'h', 't'),\\n       ('v', 'a', 'l', 'u', 'e', '_', 'u', 's', 'd', '_', 's', 'p', 'o', 't', '_', 'l', 'i', 'n', 'e')],\\n      dtype='object')] are in the [index]&quot;\n",
            "\n",
            "Edit:\n",
            "X_train.info():\n",
            "&lt;class 'pandas.core.frame.DataFrame'&gt;\n",
            "Int64Index: 13400 entries, 1993441 to 1970783\n",
            "Data columns (total 20 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   X1                   13400 non-null  float64\n",
            " 1   X2                   13400 non-null  float64\n",
            " 2   X3                   13400 non-null  float64\n",
            " 3   X4                   13181 non-null  float64\n",
            " 4   X5                   13400 non-null  float64\n",
            " 5   X6                   13400 non-null  float64\n",
            " 6   X7                   13209 non-null  float64\n",
            " 7   X8                   11956 non-null  float64\n",
            " 8   X9                   13400 non-null  float64\n",
            " 9   X10                  13400 non-null  object \n",
            " 10  X11                  13400 non-null  object \n",
            " 11  X12                  13191 non-null  object \n",
            " 12  X13                  11999 non-null  object \n",
            " 13  X14                  13380 non-null  object \n",
            " 14  X15                  13400 non-null  object \n",
            " 15  X16                  13400 non-null  object \n",
            " 16  X17                  11976 non-null  object \n",
            " 17  X18                  11976 non-null  object \n",
            " 18  X19                  13367 non-null  object \n",
            " 19  X20                  13400 non-null  object \n",
            "dtypes: float64(9), object(11)\n",
            "memory usage: 2.1+ MB\n",
            "\n",
            "\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "for _, row in results[0:5].iterrows():\n",
        "    print(row['Clean Body'])\n",
        "    print('-------------------')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x9m_LFo_yog2"
      },
      "source": [
        "Compare the naive method with your improvements and the boolean and probabilistic search. (report)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "arfKMWtLyyxY"
      },
      "source": [
        "# Evaluate the Search\n",
        "\n",
        "Now you implement multiple search methods and you're able to improve it. You have to define metric to compare it objectively.\n",
        "\n",
        "\n",
        "\n",
        "We ask you to implement NDCG (Normalized Discounted Cumulative Gain) from few queries we implement on a dozen of post. We already defined the values of relevance judgement in the xlsx file : . The final score will be the mean quadratic error of the queries.\n",
        "\n",
        "\n",
        "Explication for the xlsx file :\n",
        "\n",
        "We propose you a Excel file with some posts and a mesure of relevancy for the queries\n",
        "\n",
        "- First column is the post Id,\n",
        "- Columns starting by query are the queries you have to test.\n",
        "- The values in this columns are the rank of relevancy of the post in regard with the query.\n",
        "- The missing values indicates you should not take into account the post\n",
        "\n",
        "\n",
        "You will have to criticize this metric and your result in the report. Then you will have to propose some improvements. \n",
        "\n",
        "Thereafter in this week, you will have to compare your different search engines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxCcftBPagMQ"
      },
      "outputs": [],
      "source": [
        "# Read Relevancy CSV\n",
        "df_relevancy = pd.read_excel(\"/content/drive/MyDrive/TP Centrale/evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVxld-Ujy0nN"
      },
      "outputs": [],
      "source": [
        "def calculate_ndgc(query_col=\"query\", output_col=\"query_output\"):\n",
        "  # TODO\n",
        "\n",
        "  return\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
