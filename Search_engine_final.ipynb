{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Group 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/himmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "FILEPATH = \"datascience.stackexchange.com\" # path to unzipped data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log10,sqrt\n",
    "import re\n",
    "import tkinter as tk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def extract_data(filepath):\n",
    "    return pd.read_xml(filepath, parser=\"etree\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id  PostTypeId             CreationDate  Score  ViewCount  \\\n",
      "0  115535           1  2022-10-24T12:58:24.757      1       29.0   \n",
      "1  115536           1  2022-10-24T13:45:55.820      0       30.0   \n",
      "2  115537           1  2022-10-24T13:56:40.603      0       12.0   \n",
      "3  115538           2  2022-10-24T14:36:39.480      1        NaN   \n",
      "4  115539           1  2022-10-24T15:22:37.823      0       14.0   \n",
      "\n",
      "                                                Body  OwnerUserId  \\\n",
      "0  [pa, in, advance, if, this, question, is, so, ...      30838.0   \n",
      "1  [pi, just, need, to, check, my, understanding,...      87037.0   \n",
      "2  [pi, am, trying, to, tune, gradient, boost, ca...      64199.0   \n",
      "3  [p, it, is, correct, if, you, compare, neural,...     119140.0   \n",
      "4  [pi, starting, to, study, how, to, rank, words...     141937.0   \n",
      "\n",
      "          LastActivityDate                                              Title  \\\n",
      "0  2022-10-24T12:58:24.757  [information, retrieval, vs, recommendation, s...   \n",
      "1  2022-10-24T14:36:39.480                          [the, idea, behind, drop]   \n",
      "2  2022-10-24T13:56:40.603  [maximize, accuracy, with, differential, evolu...   \n",
      "3  2022-10-24T14:36:39.480                                                 []   \n",
      "4  2022-10-24T15:22:37.823              [help, using, b, to, rank, sentences]   \n",
      "\n",
      "                                                Tags  ...  ClosedDate  \\\n",
      "0    <recommender-system><information-retrieval><ai>  ...        None   \n",
      "1                                          <dropout>  ...        None   \n",
      "2  <machine-learning><r><accuracy><ensemble-learn...  ...        None   \n",
      "3                                               None  ...        None   \n",
      "4  <machine-learning-model><tfidf><information-re...  ...        None   \n",
      "\n",
      "   ContentLicense AcceptedAnswerId LastEditorUserId  LastEditDate  ParentId  \\\n",
      "0    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "1    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "2    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "3    CC BY-SA 4.0              NaN              NaN          None  115536.0   \n",
      "4    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "\n",
      "  OwnerDisplayName  CommunityOwnedDate LastEditorDisplayName FavoriteCount  \n",
      "0             None                None                  None           NaN  \n",
      "1             None                None                  None           NaN  \n",
      "2             None                None                  None           NaN  \n",
      "3             None                None                  None           NaN  \n",
      "4             None                None                  None           NaN  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_posts = extract_data(filepath=FILEPATH+\"/Posts.xml\")\n",
    "users = extract_data(filepath=FILEPATH+\"/Users.xml\") # usefull for user reputation\n",
    "comments = extract_data(filepath=FILEPATH+\"/Comments.xml\") # usefull for user reputation and accepted answer\n",
    "votes = extract_data(filepath=FILEPATH+\"/Votes.xml\") # usefull for post score\n",
    "badges = extract_data(filepath=FILEPATH+\"/Badges.xml\") # usefull for user reputation\n",
    "file = \"posts.pkl\"  # precomputed tokenization in file posts.pkl\n",
    "with open(file, 'rb') as f:\n",
    "    posts = pkl.load(f)\n",
    "\n",
    "print(posts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(posts,column): # returns inverted index\n",
    "    inverted_index = {}\n",
    "    for i in range(len(posts)):\n",
    "        post_id = str(posts['Id'][i])\n",
    "        for word in posts[column][i]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {}\n",
    "            if post_id not in inverted_index[word]:\n",
    "                inverted_index[word][post_id] = 0\n",
    "            inverted_index[word][post_id] = inverted_index[word][post_id] + 1\n",
    "    return inverted_index\n",
    "  \n",
    "inverted_index_body = get_inverted_index(posts,\"Body\")\n",
    "inverted_index_title = get_inverted_index(posts,\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'these', 't', 'd', 'being', 'hers', 'some', 'as', 'p', 'that', 'again', 'nor', 'll', 'for', 're', 'won', 'but', 'just', 'from', 'yourselves', 'of', 'shouldn', 'between', 'once', 'wasn', 'not', \"haven't\", 'now', 'very', 'through', 'those', 'there', 'own', 'hasn', 'more', 'no', 'haven', 'couldn', 'having', \"wouldn't\", 'any', 'off', 'them', 'each', \"that'll\", \"weren't\", 'wouldn', 'ain', 'a', \"you're\", 'herself', 'ours', 'yourself', 'all', 'their', 'our', 'm', 'o', 'here', 'then', 'how', 'during', 'his', 'do', 'should', 'same', 'have', 'yours', 'with', \"hadn't\", 'above', 'or', 'it', 'further', 'whom', 'we', \"wasn't\", 'at', 'she', \"needn't\", 'too', \"shan't\", 'isn', 'your', 'shan', 'under', 'her', 'hadn', \"isn't\", 'up', \"should've\", 'does', 'because', \"don't\", \"couldn't\", 'myself', 'before', 'were', 'him', 'am', 'both', 'didn', 'he', 'other', 'and', 'needn', 'so', \"didn't\", 'when', 'are', 'where', \"aren't\", 'be', 'down', 'you', 'against', 'on', 'had', \"shouldn't\", 'ourselves', 'doing', 'until', 'weren', 'itself', 'i', 'been', 'such', 'which', \"doesn't\", 'aren', 'himself', 'only', 'to', \"it's\", 'the', 'they', 'below', 'over', 'is', \"mustn't\", \"you've\", \"hasn't\", 'after', 's', 'by', 'did', 'my', 'about', 've', 'out', 'mightn', 'what', 'y', 'me', 'was', 'an', 'few', 'don', 'if', 'while', \"mightn't\", \"she's\", 'than', 'this', 'doesn', 'ma', \"won't\", 'who', 'why', 'theirs', 'into', 'themselves', 'most', 'has', 'will', \"you'll\", 'can', \"you'd\", 'in', 'its', 'mustn'}\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "stop_words_nltk = stopwords.words('english') # list of stopwords\n",
    "stop_words = set(stop_words_nltk + [\"p\"])\n",
    "print(stop_words)\n",
    "\n",
    "def remove_stopwords(inverted_index):\n",
    "    for word in stop_words:\n",
    "        if word in inverted_index:\n",
    "            del inverted_index[word]\n",
    "    return inverted_index\n",
    "\n",
    "inverted_index_body = remove_stopwords(inverted_index_body)\n",
    "inverted_index_title = remove_stopwords(inverted_index_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(inverted_index, posts): # returns tf_idf\n",
    "    tf_idf = {}\n",
    "    for word in inverted_index:\n",
    "        tf_idf[word] = {}\n",
    "        for post_id in inverted_index[word]:\n",
    "            tf_idf[word][post_id] = inverted_index[word][post_id] * log10(len(posts)/len(inverted_index[word]))\n",
    "    return tf_idf\n",
    "\n",
    "tf_idf_body = get_tf_idf(inverted_index_body, posts)\n",
    "tf_idf_title = get_tf_idf(inverted_index_title, posts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('116141', 21.661042056227643), ('339', 21.661042056227643), ('109823', 19.380932366098417), ('76841', 15.96076783090458), ('95007', 15.96076783090458)]\n",
      "[('116141', 19), ('339', 19), ('109823', 17), ('76841', 14), ('95007', 14)]\n",
      "[('29542', 3.6589177006247477), ('65063', 3.6589177006247477), ('11404', 3.6589177006247477), ('64403', 3.6589177006247477), ('29057', 3.6589177006247477)]\n",
      "[('29542', 2), ('65063', 2), ('11404', 2), ('64403', 2), ('29057', 2)]\n"
     ]
    }
   ],
   "source": [
    "# posts that contain the word python the most in the body and title\n",
    "print(sorted(tf_idf_body[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(inverted_index_body[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(tf_idf_title[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(inverted_index_title[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_inverted_index(inverted_index):\n",
    "    stemmed_inverted_index = {}\n",
    "    for word in inverted_index:\n",
    "        stemmed = stemmer.stem(word) # stem word\n",
    "        if stemmed not in stemmed_inverted_index:\n",
    "            stemmed_inverted_index[stemmed] = {}\n",
    "        for post_id in inverted_index[word]:\n",
    "            if post_id not in stemmed_inverted_index[stemmed]:\n",
    "                stemmed_inverted_index[stemmed][post_id] = 0\n",
    "            stemmed_inverted_index[stemmed][post_id] = stemmed_inverted_index[stemmed][post_id] + inverted_index[word][post_id]\n",
    "    return stemmed_inverted_index\n",
    "\n",
    "stemmed_inverted_index_body = stem_inverted_index(inverted_index_body)\n",
    "stemmed_inverted_index_title = stem_inverted_index(inverted_index_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('115768', 13), ('60039', 11), ('119916', 11), ('20380', 9), ('117097', 8)]\n",
      "[('76321', 54), ('74666', 25), ('115768', 13), ('60039', 11), ('119916', 11)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(inverted_index_body[\"open\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(stemmed_inverted_index_body[\"open\"].items(), key=lambda x: x[1], reverse=True)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 09:47:02.861305: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 09:47:03.051012: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-02 09:47:03.051032: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-02 09:47:04.471637: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 09:47:04.471784: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 09:47:04.471803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inform', 'retriev']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "def text_process(query):\n",
    "    # tokenize query (or text) with bert-case-uncased tokenizer\n",
    "    # keep only alpha words\n",
    "    # remove stopwords\n",
    "    # stem query\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "    query = tokenizer.tokenize(query)\n",
    "    query = [word for word in query if word.isalpha()]\n",
    "    query = [word for word in query if word not in stop_words]\n",
    "    query = [stemmer.stem(word) for word in query]\n",
    "    return query\n",
    "\n",
    "query = \"information retrieval\"\n",
    "query = text_process(query)\n",
    "print(query) # ['error', 'open', 'file', 'python']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting relevant metadata from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(post_id):\n",
    "    taglist = posts[posts[\"Id\"] == post_id][\"Tags\"]\n",
    "    if taglist.values[0] is None: return []\n",
    "    taglist = taglist.values[0].replace(\"<\", \"\").replace(\">\", \" \").split(\" \")\n",
    "    return [tag.replace(\"-\", \" \") for tag in taglist]\n",
    "\n",
    "def get_all_tags():\n",
    "    all_tags = []\n",
    "    for i in range(len(posts)):\n",
    "        all_tags = all_tags + get_tags(posts[\"Id\"][i])\n",
    "    return list(set(all_tags))\n",
    "\n",
    "# all_tags = get_all_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60888.0\n"
     ]
    }
   ],
   "source": [
    "def get_body(posts, post_id):\n",
    "    # tokenized body of the post\n",
    "    body = posts[posts[\"Id\"] == post_id][\"Body\"].values[0]\n",
    "    return body\n",
    "\n",
    "def get_title(posts, post_id):\n",
    "    # tokenized title of the post\n",
    "    title = posts[posts[\"Id\"] == post_id][\"Title\"].values[0]\n",
    "    return title\n",
    "\n",
    "def get_tags_from_postid(post_id):\n",
    "    taglist = posts[posts[\"Id\"] == post_id][\"Tags\"]\n",
    "    if taglist.values[0] is None: return []\n",
    "    taglist = taglist.values[0].replace(\"<\", \"\").replace(\">\", \" \").split(\" \")\n",
    "    # replace \"-\"\" with \" \"\n",
    "    for i in range(len(taglist)):\n",
    "        taglist[i] = text_process(taglist[i].replace(\"-\", \" \"))\n",
    "    return taglist[:-1]\n",
    "\n",
    "def get_raw_tags_from_postid(post_id):\n",
    "    taglist = posts[posts[\"Id\"] == post_id][\"Tags\"]\n",
    "    if taglist.values[0] is None: return []\n",
    "    taglist = taglist.values[0].replace(\"<\", \"\").replace(\">\", \" \").split(\" \")\n",
    "    return taglist[:-1]\n",
    "\n",
    "# print(get_tags_from_postid(115768))\n",
    "\n",
    "def get_reputation(post_id):\n",
    "    # reputation of the user who posted the post\n",
    "    owner_user_id = posts[posts[\"Id\"] == post_id][\"OwnerUserId\"].values[0]\n",
    "    if pd.isna(owner_user_id):\n",
    "        return 0\n",
    "    reputation = users[users[\"Id\"] == int(owner_user_id)][\"Reputation\"].values[0]\n",
    "    return reputation\n",
    "\n",
    "\n",
    "# print(get_reputation(posts, 115768))\n",
    "def get_inverted_index_tags(posts,tag_to_processed_tag_dict):\n",
    "    # Inverted index of tags\n",
    "    inverted_index_tags = {}\n",
    "    for i in range(len(posts)):\n",
    "        post_id = posts[\"Id\"][i]\n",
    "        taglist = [tag_to_processed_tag_dict[tag] for tag in get_raw_tags_from_postid(post_id)]\n",
    "        taglist = [t for tag in taglist for t in tag]\n",
    "        for tag in taglist:\n",
    "            if tag not in inverted_index_tags:\n",
    "                inverted_index_tags[tag] = {}\n",
    "            if post_id not in inverted_index_tags[tag]:\n",
    "                inverted_index_tags[tag][post_id] = 0\n",
    "            inverted_index_tags[tag][post_id] = inverted_index_tags[tag][post_id] + 1\n",
    "    return inverted_index_tags\n",
    "\n",
    "\n",
    "def get_votes(post_id):\n",
    "    # number of votes of the post\n",
    "    nb_votes = posts[posts[\"Id\"] == post_id][\"Score\"].values[0]\n",
    "    return nb_votes\n",
    "\n",
    "def get_number_answers(post_id):\n",
    "    # number of answers of the post\n",
    "    nb_answers = posts[posts[\"ParentId\"] == post_id].shape[0]\n",
    "    return nb_answers\n",
    "\n",
    "def get_badges_user(post_id):\n",
    "    # number of badges of the user who posted the post\n",
    "    # use the variable badges and users\n",
    "    owner_user_id = posts[posts[\"Id\"] == post_id][\"OwnerUserId\"].values[0]\n",
    "    if pd.isna(owner_user_id):\n",
    "        return 0\n",
    "    nb_badges = badges[badges[\"UserId\"] == int(posts[posts[\"Id\"] == post_id][\"OwnerUserId\"])][\"Class\"].shape[0]\n",
    "    return nb_badges\n",
    "\n",
    "def get_answered(post_id):\n",
    "    # 1 if the post is answered, 0 otherwise\n",
    "    # use the variable comments\n",
    "    if comments[comments[\"PostId\"] == int(post_id)].shape[0] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_views(post_id):\n",
    "    # number of views of the post\n",
    "    # use the variable posts\n",
    "    nb_views = posts[posts[\"Id\"] == post_id][\"ViewCount\"].values[0]\n",
    "    return nb_views\n",
    "\n",
    "# print(posts.head())\n",
    "\n",
    "print(get_views(12761))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests\n",
    "#print(get_body(posts,123)) OK\n",
    "#print(get_title(posts,5)) OK\n",
    "#print(get_tags_from_postid(5)) OK\n",
    "#print(get_raw_tags_from_postid(5)) OK\n",
    "#print(get_reputation(5)) OK\n",
    "#print(get_votes(5)) OK\n",
    "# get_reputation(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# tag_to_processed_tag_dict ={}\n",
    "# for tag in tqdm(all_tags):\n",
    "#     tag_to_processed_tag_dict[tag] = text_process(tag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save tag_to_processed_tag_dict\n",
    "# import pickle\n",
    "# with open('tag_to_processed_tag_dict.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tag_to_processed_tag_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)$\n",
    "# load tag_to_processed_tag_dict\n",
    "import pickle\n",
    "with open('tag_to_processed_tag_dict.pickle', 'rb') as handle:\n",
    "    tag_to_processed_tag_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'regex', 'nosql', 'validation', 'odds', 'rocr-package', 'mlops', 'bayesian', 'rasa-nlu', 'perplexity', 'data-source', 'real-ml-usecase', 'randomized-algorithms', 'sequential-pattern-mining', 'google-prediction-api', 'feature-scaling', 'noise', 'colab', 'marketing', 'survival-analysis', 'python-polars', 'markov', 'bootstraping', 'uncertainty', 'jaccard-coefficient', 'fasttext', 'ibm-watson', 'cross-entropy', 'json', 'state-of-the-art', 'training', 'ai', 'ngrams', 'machine-translation', 'multitask-learning', 'probability-calibration', 'api', 'embeddings', 'web-scraping', 'octave', 'apache-kafka', 'exploratory-factor-analysis', 'normalization', 'mse', 'csv', 'catboost', 'stacked-lstm', 'bigdata', 'vgg16', 'anonymization', 'hyperparameter', 'caret', 'openai-gpt', 'flask', 'methodology', 'visualization', 'gradient', 'elastic-search', 'scoring', 'finite-precision', 'kernel', 'consumerweb', 'knowledge-base', 'nlp', 'bioinformatics', 'spatial-transformer', 'stacking', 'pandas', 'apache-pig', 'definitions', 'cost-function', 'inceptionresnetv2', 'metaheuristics', 'tools', 'collinearity', 'libsvm', 'keras', 'conda', 'tfidf', 'binary-classification', 'rbf', 'missing-data', 'grid-search', 'sagemaker', 'knowledge-distillation', 'numerical', 'finetuning', 'labels', 'cs231n', 'hypothesis-testing', 'chatbot', 'audio-recognition', 'svr', 'mxnet', 'mlflow', 'bert', 'pearsons-correlation-coefficient', 'transfer-learning', 'self-study', 'reward', 'rstudio', 'association-rules', 'pvalue', 'learning-rate', 'gini-index', 'sentiment-analysis', 'data-augmentation', 'pooling', 'search-engine', 'gaussian-process', 'algorithms', 'dplyr', 'manhattan', 'counts', 'social-network-analysis', 'dynamic-programming', 'epochs', 'analysis', 'least-squares-svm', 'information-extraction', 'text-classification', 'hpc', 'multi-instance-learning', 'hbase', 'feature-engineering', 'hierarchical-data-format', 'ensemble-learning', 'ngboost', 'goodness-of-fit', 'bart', 'multi-output', 'data-leakage', 'education', 'markov-process', 'list', 'mlp', 'ocr', 'unsupervised-learning', 'activity-recognition', 'vector-space-models', 'linear-regression', 'momentum', 'gmm', 'document-term-matrix', 'spyder', 'feature-selection', 'manifold', 'community', 'time-series', 'hinge-loss', 'convolutional-neural-network', 'sampling', 'image-preprocessing', 'taxonomy', 'crawling', 'outlier', 'dropout', 'beginner', 'learning', 'one-shot-learning', 'retraining', 'feature-interaction', 'conformalprediction', 'cross-validation', 'agglomerative', 'neo4j', 'infographics', 'nlg', 'labeling', 'policy-gradients', 'map-reduce', 'mean', 'heatmap', 'hashingvectorizer', 'functional-api', 'text-generation', 'probability', 'serialisation', 'early-stopping', 'softmax', 'reference-request', 'javascript', 'question-answering', 'deployment', 'ethical-ai', 'domain-adaptation', 'hardware', 'naive-bayes-classifier', 'cloud-computing', 'python', 'generative-models', 'auc', 't', 'k-means', 'shap', 'density-estimation', 'tsne', 'data-stream-mining', 'git', 'error-handling', 'alex-net', 'simulation', 'learning-to-rank', 'prophet', 'distributed', 'faster-rcnn', 'hyperparameter-tuning', 'sigmoid', 'replication', 'research', 'deepmind', 'kalman-filter', 'derivation', 'boosting', 'coherence', 'automatic-summarization', 'actor-critic', 'feature-extraction', 'feature-importances', 'federated-learning', 'predictor-importance', 'rfe', 'time', 'monte-carlo', 'windows', 'anomaly-detection', 'reshape', 'google-cloud-platform', 'confidence', 'entropy', 'privacy', 'c++', 'reinforcement-learning', 'redshift', 'scipy', 'rnn', 'structural-equation-modelling', 'programming', 'rbm', 'logistic', 'torch', 'categorical-data', 'finance', 'stemming', 'text-to-columns', 'semantic-segmentation', 'anaconda', 'parameter', 'natural-gradient-boosting', 'speech-to-text', 'mongodb', 'normal', 'interpretation', 'cost-sensitive-learning', 'online-learning', 'keras-rl', 'loss-function', 'concept-drift', 'reproducibility', 'dummy-variables', 'version-control', 'mean-shift', 'arima', 'mutual-information', 'feature-construction', 'encoder', 'combinatorics', 'nvidia', 'imbalanced-learn', 'tpu', 'fastai', 'library', 'competitions', 'named-entity-recognition', 'r-squared', 'esl', 'one-class-classification', 'pytorch-geometric', 'adversarial-ml', 'dqn', 'coursera', 'information-theory', 'pgm', 'intuition', 'naive-bayes-algorithim', 'target-encoding', 'pytorch', 'memory', 'knime', 'regularization', 'weka', 'xgboost', 'pruning', 'image-classification', 'facebook', 'sequence-to-sequence', 'mini-batch-gradient-descent', 'ridge-regression', 'mcmc', 'image', 'efficiency', 'causalimpact', 'google-cloud', 'kendalls-tau-coefficient', 'bayes-error', 'gan', 'noisification', 'pip', 'semantic-similarity', 'linear-models', 'imbalanced-data', 'overfitting', 'evolutionary-algorithms', 'binary', 'inception', 'discriminant-analysis', 'networkx', 'gradient-descent', 'descriptive-statistics', 'apache-spark', 'stata', 'processing', 'self-driving', 'computer-vision', 'text', 'test', 'sparse', 'evaluation', 'universal-approximation-theorem', 'ensemble-modeling', 'apache-mahout', 'gpu', 'data-table', 'mnist', 'code', 'semi-supervised-learning', 'share-point', 'dbscan', 'pcamixdata', 'project-planning', 'score', 'cuda', 'ggplot2', 'mathematics', 'text-processing', 'vae', 'automl', 'meta-learning', 'data-science-model', 'matplotlib', 'software-recommendation', 'activation-function', 'entity-matching', 'partial-least-squares', 'kaggle', 'regression', 'neural-network', 'classificationmulti', 'cause-and-effect', 'normal-equation', 'gradient-boosting-decision-trees', 'ann', 'usecase', 'symbolic-learning', 'kedro', 'custom-layer', 'precision-recall-curve', 'interpolation', 'genetic-algorithms', 'forecasting', 'bag-of-words', 'career', 'few-shot-learning', 'object-recognition', 'non-parametric', 'pasting', 'lightgbm', 'cloud', 'pymc3', 'text-mining', 'software-development', 'data-indexing-techniques', 'train', 'siamese-networks', 'variance', 'word-embeddings', 'predict', 'features', 'data', 'data-cleaning', 'clustering', 'classification', 'lsi', 'structured-data', 'lstm', 'apache-hadoop', 'pattern-recognition', 'search', 'dictionary', 'encoding', 'smote', 'similar-documents', 'openai-gym', 'estimation', 'bagging', 'rattle', 'best-practice', 'hashing-trick', 'methods', 'predictive-modeling', 'smotenc', 'one-hot-encoding', 'data-formats', 'google-bigquery', 'doc2vec', 'sensors', 'boruta', 'open-source', 'pacf', 'prediction', 'distance', 'data-wrangling', 'lime', 'random-forest', 'elastic-net', 'implementation', 'parallel', 'google', 'fuzzy-classification', 'homework', 'statistics', 'model-selection', 'probabilistic-programming', 'accuracy', 'market-basket-analysis', 'pycaret', 'mysql', 'bayesian-nonparametric', 'parameter-estimation', 'azure-ml', 'java', 'svm', 'oversampling', 'language-model', 'recommender-system', 'matrix', 'hive', 'difference', 'poisson', 'indexing', 'transformation', 'cart', 'numpy', 'h2o', 'management', 'discounted-reward', 'excel', 'masking', 'multivariate-distribution', 'data-mining', 'sequence', 'allennlp', 'featurization', 'chi-square-test', 'relational-dbms', 'logarithmic', 'cosine-distance', 'theano', 'object-detection', 'google-data-studio', 'nltk', 'linux', 'corpus', 'pybrain', 'gridsearchcv', 'scikit-learn', 'grammar-inference', 'jupyter', 'functions', 'dataset', 'partial-dependence-plot', 'glorot-initialization', 'histogram', 'sports', 'topic-model', 'class-imbalance', 'feature-reduction', 'multiclass-classification', 'neural-style-transfer', 'c', 'gnn', 'markov-hidden-model', 'anomaly', 'linear-programming', 'scalability', 'pyspark', 'seaborn', 'notation', 'lasso', 'plotly', 'metric', 'experiments', 'churn', 'data-engineering', 'image-segmentation', 'objective-function', 'wasserstein', 'tableau', 'cyclegan', 'supervised-learning', 'history', 'categorical-encoding', 'data-analysis', 'geospatial', 'gbm', 'r', 'forecast', 'k-nn', 'convolution', '3d-reconstruction', 'machine-learning-model', 'spacy', 'bias', 'f1score', 'performance', 'distribution', 'fourier', 'weight-initialization', 'ab-test', 'perceptron', 'knowledge-graph', 'hdbscan', 'loss', 'context-vector', 'word2vec', 'lda', 'shadow-deployment', 'sgd', 'batch-normalization', 'data-quality', 'feature-map', 'bar-chart', 'huggingface', 'text-filter', 'bokeh', 'image-size', 'metadata', 'amazon-ml', 'wolfram-language', '3d-object-detection', 'game', 'caffe', 'explainable-ai', 'sas', 'matlab', 'dirichlet', 'spearmans-rank-correlation', 'backtest', 'python-3.x', 'yolo', 'decision-trees', 'graph-neural-network', 'entity-linking', 'active-learning', 'recurrent-neural-network', 'databases', 'annotation', 'rmse', 'ndcg', 'powerbi', 'similarity', 'spectral-clustering', 'multilabel-classification', 'convergence', 'aws', 'pipelines', 'groupby', 'image-recognition', 'tokenization', 'correlation', 'parsing', 'scala', 'document-understanding', 'pickle', 'pretraining', 'pca', 'confusion-matrix', 'aggregation', 'tensorflow', 'glm', 'time-complexity', 'field-aware-factorization-machines', 'backpropagation', 'ipython', 'weighted-data', 'etl', 'wikipedia', 'tensorboard', 'matrix-factorisation', 'tflearn', 'stanford-nlp', 'movielens', 'twitter', 'ranking', 'statsmodels', 'dataframe', 'preprocessing', 'fuzzy-logic', 'opencv', 'genetic', 'scraping', 'gaussian', 'duplicate', 'linearly-separable', 'gensim', 'imbalance', 'graphs', 'classifier', 'representation', 'hog', 'lda-classifier', 'sql', 'isolation-forest', 'orange', 'rapidminer', 'cnn', 'attention-mechanism', '.net', 'knowledge-canonicalization', 'julia', 'tranformation', 'labelling', 'graphical-model', 'terminology', 'word', 'gru', 'spss', 'automation', 'umap', 'linear-algebra', 'generalization', 'estimators', 'theory', 'roc', 'optimization', 'information-retrieval', 'logistic-regression', 'tesseract', 'q-learning', 'data-imputation', 'inference', 'graphviz', 'deep-learning', 'plotting', 'lemmatization', 'dimensionality-reduction', 'data-drift', 'aws-lambda', 'freebase', 'adaboost', 'ensemble', 'anova', 'hashing', 'feedback-loop', 'deepar', 'books', 'expectation-maximization', 'model-evaluations', 'vc-theory', 'permutation-test', 'dynamic-time-warping', 'bayesian-networks', 'autoencoder', 'machine-learning', 'pac-learning', 'sparsity', 'transformer', 'data-product', 'torchvision', 'genetic-programming']\n"
     ]
    }
   ],
   "source": [
    "print(list(tag_to_processed_tag_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted_index_tags = get_inverted_index_tags(posts,tag_to_processed_tag_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # save inverted index tags as pkl\n",
    "# with open('inverted_index_tags.pkl', 'wb') as f:\n",
    "#     pickle.dump(inverted_index_tags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inverted index tags from pkl\n",
    "with open('inverted_index_tags.pkl', 'rb') as f:\n",
    "    inverted_index_tags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recommend', 'inform', 'ai', 'drop', 'machin', 'r', 'accuraci', 'ensembl', 'meta', 'rank', 'learn', 'classif', 'nl', 'multi', 'reinforc', 'polici', 'tabl', 'rs', 'binari', 'big', 'apach', 'map', 'deep', 'neural', 'con', 'ml', 'python', 'time', 'chu', 'random', 'supervis', 'comput', 'languag', 'open', 'imag', 'transform', 'hug', 'pre', 'data', 'web', 'predict', 'forecast', 'panda', 'prep', 'featur', 'l', 'loss', 'optim', 'statist', 'count', 'normal', 'sql', 'databas', 'object', 'grid', 'isol', 'excel', 'semant', 'fourier', 'nu', 'mat', 'sampl', 'ke', 'cnn', 'spa', 'sv', 'regress', 'algorithm', 'text', 'graph', 'sci', 'faster', 'commun', 'pc', 'deploy', 'dimension', 'model', 'train', 'distribut', 'tensor', 'gp', 'visual', 'x', 'hyper', 'hardwar', 'stanford', 'cluster', 'anomali', 'correl', 'un', 'encod', 'dummi', 'code', 'mathemat', 'class', 'evalu', 'kernel', 'ju', 'bert', 'semi', 'c', 'java', 'wolf', 'confus', 'soft', 'fine', 'pipelin', 'token', 'hierarch', 'autom', 'km', 'linear', 'implement', 'ann', 'one', 'sg', 'log', 'decis', 'softwar', 'rm', 'transfer', 'descript', 'cross', 'valid', 'self', 'bia', 'batch', 'estim', 'label', 'g', 'rn', 'bag', 'ari', 'naiv', 'miss', 'sha', 'topic', 'ld', 'em', 'word', 'gradient', 'cat', 'caus', 'confid', 'fed', 'name', 'lass', 'q', 'perform', 'kn', 'varianc', 'ga', 'financ', 'no', 'sequenc', 'distanc', 'genera', 'momentum', 'im', 'market', 'chi', 'structur', 'sentiment', 'auto', 'gan', 'process', 'probabl', 'elast', 'ms', 'siam', 'refer', 'search', 'genet', 'least', 'mn', 'pool', 'sequenti', 'pattern', 'gen', 'orang', 'dq', 'partial', 'ja', 'cart', 'ka', 'score', 'bio', 'googl', 'back', 'associ', 'marko', 'ng', 'epoch', 'weight', 'similar', 'vector', 'metric', 'regular', 'error', 're', 'begin', 'theori', 'domain', 'cs', 'terminolog', 'definit', 'activ', 'chat', 'reg', 'game', 'per', 'book', 'plot', 'sea', 'ts', 'rb', 'twitter', 'cost', 'scala', 'experi', 'ab', 'gb', 'parallel', 'disc', 'ip', 'fuzzi', 'social', 'light', 'nois', 'incept', 'cola', 'homework', 'audio', 'aggreg', 'scrape', 'sa', 'paramet', 'educ', 'career', 'sm', 'surviv', 'linux', 'infer', 'interpret', 'lime', 'cycl', 'mutual', 'au', 'roc', 'geo', 'octav', 'matrix', 'mont', 'par', 'attent', 'effici', 'causal', 'power', 'cu', 'speech', 'mini', 'hypothesi', 'h', 'densiti', 'target', 'numer', 'fast', 'index', 'knowledg', 'explain', 'tool', 'free', 'hash', 'bay', 'co', 'manag', 'onlin', 'gener', 'aw', 'relat', 'stat', 'boost', 'non', 'sensor', 'cloud', 'azur', 'v', 'methodolog', 'yo', 'n', 'state', 'db', 'et', 'hp', 'graphic', 'boot', 'research', 'inter', 'tran', 'stack', 'good', 'api', 'converg', 'memori', 'serial', 'mx', 'automat', 'mean', 'ada', 'corpu', 'evolutionari', 'pac', 'ana', 'dynam', 'vc', 'julia', 'ridg', 'method', 'program', 'window', 'doc', 'mask', 'group', 'concept', 'feat', 'mc', 'earli', 'version', 'collin', 'natur', 'deriv', 'gr', 'hi', 'actor', 'pg', 'hive', 'neo', 'caf', 'info', 'est', 'manifold', 'uncertainti', 'es', 'torch', 'amazon', 'spars', 'expect', 'mon', 'net', 'metadata', 'use', 'consum', 'alex', 'simul', 'pip', 'en', 'bart', 'rf', 'pick', 'pro', 'competit', 'j', 'spectral', 'rapid', 'si', 'over', 'sport', 'manhattan', 'red', 'finit', 'entiti', 'heat', 'spi', 'metal', 'univers', 'wikipedia', 'va', 'conform', 'test', 'rec', 'replic', 'gi', 'histori', 'li', 'ad', 'differ', 'sp', 'document', 'spatial', 'best', 'pr', 'librari', 'represent', 'privaci', 'tess', 'rep', 'spear', 'function', 'pearson', 'question', 'pv', 'ra', 'network', 'gm', 'le', 'rattl', 'crawl', 'project', 'movi', 'cours', 'po', 'sage', 'feedback', 'ked', 'ethic', 'ex', 'dir', 'hd', 'field', 'kendal', 'realm', 'symbol', 'reward', 'notat', 'hog', 'custom', 'allen', 'gin', 'comb', 'grammar', 'ibm', 'discount', 'odd', 'ag', 'stem', 'past', 'context', 'prophet', 'precis', 'bo', 'share', 'um', 'entropi', 'facebook', 'intuit', 'taxonomi', 'care', 'shadow', 'fl', 'bar', 'dictionari', 'duplic', 'list', 'analysi']\n"
     ]
    }
   ],
   "source": [
    "print(list(inverted_index_tags.keys()))\n",
    "# print(inverted_index_tags[\"optimization\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine (We name it : CobraSearch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to use a probabilistic model because it is well suited to an inverted index. The most used probabilistic model is the BM25 model, However, it supposes that all documents have the same prior relevance. In our case, we want to take into account the non textual metadata of the documents. \\\n",
    "An extensive bibliographic overview has led us to use the following model :\n",
    "Our chosen model is based on the following paper: https://dl.acm.org/doi/10.1561/1500000019 Sections 3.6 and 3.7 \\\n",
    "It is derived from the BM25 model with 3 Streams : title, body and tags, but also taking into account non textual features like the number votes, comments, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "log10_values = [0] # precompute log10 values\n",
    "for i in range(1,100000):\n",
    "    log10_values.append(np.log10(i))\n",
    "\n",
    "def post_to_post_feature(post_id):\n",
    "    post_feature = {}\n",
    "    post_feature[\"post_id\"] = post_id\n",
    "    # post_feature[\"tf_body\"] = term_frequency(post_id, inverted_index_body)\n",
    "    # post_feature[\"tf_title\"] = term_frequency(post_id, inverted_index_title)\n",
    "    # post_feature[\"tf_tags\"] = term_frequency(post_id, inverted_index_tags)\n",
    "    post_feature[\"reputation\"] = get_reputation(post_id)\n",
    "    post_feature[\"votes\"] = get_votes(post_id)\n",
    "    post_feature[\"number_answers\"] = get_number_answers(post_id)\n",
    "    post_feature[\"badges\"] = get_badges_user(post_id)\n",
    "    post_feature[\"answered\"] = get_answered(post_id)\n",
    "    post_feature[\"views\"] = get_views(post_id)\n",
    "    return post_feature\n",
    "\n",
    "def precompute_features_score(post_id):\n",
    "    lambda_reputation = 1\n",
    "    w_reputation = 5\n",
    "    lambda_votes = 0\n",
    "    w_votes = 7\n",
    "    lambda_nb_answers = 1\n",
    "    w_nb_answers = 2\n",
    "    lambda_badges = 1\n",
    "    w_badges = 7\n",
    "    w_answered = 1\n",
    "    lambda_views = 1\n",
    "    w_views = 4\n",
    "    Vi = {}\n",
    "    def even_log(x,lambda_votes):\n",
    "        if x >=0:\n",
    "            return log10_values[x+lambda_votes]\n",
    "        elif x <0:\n",
    "            return -log10_values[-x+lambda_votes]\n",
    "    post_features = post_to_post_feature(post_id)\n",
    "    Vi[post_id] = {\n",
    "        \"reputation\": w_reputation*log10_values[post_features[\"reputation\"] + lambda_reputation],\n",
    "        \"votes\": w_votes*even_log(post_features[\"votes\"], lambda_votes),\n",
    "        \"number_answers\": w_nb_answers*log10_values[post_features[\"number_answers\"]+lambda_nb_answers],\n",
    "        \"badges\":w_badges*log10_values[post_features[\"badges\"]+lambda_badges],\n",
    "        \"answered\":w_answered*post_features[\"answered\"],\n",
    "        \"views\":w_views*log10_values[post_features[\"badges\"]+lambda_views],\n",
    "    }\n",
    "    return sum([Vi[post_id][feature] for feature in Vi[post_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi_sums = {}\n",
    "# for post_id in tqdm(posts[\"Id\"]):\n",
    "#     Vi_sums[post_id] = precompute_features_score(post_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save vi_sums to file\n",
    "# with open('vi_sums.pkl', 'wb') as f:\n",
    "#     pickle.dump(Vi_sums, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vi_sums.pkl', 'rb') as f:\n",
    "    Vi_sums = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(post_id,inverted_index):\n",
    "    # term frequency of the post\n",
    "    # use the variable posts\n",
    "    tf = {}\n",
    "    for word in inverted_index:\n",
    "        if str(post_id) in inverted_index[word]:\n",
    "            tf[word] = inverted_index[word][str(post_id)]/len(posts[posts[\"Id\"] == post_id][\"Body\"].values[0])\n",
    "        else:\n",
    "            tf[word] = 0\n",
    "    return tf\n",
    "\n",
    "def prefilter_posts(query_processed, posts):\n",
    "    # keep only posts that contain all words of the query in the body, title or tags\n",
    "    # use the variable posts\n",
    "    print(\"prefiltering posts...\")\n",
    "    count_words = {}\n",
    "    for query_word in query_processed:\n",
    "        for post_id in tqdm(posts[\"Id\"]):\n",
    "            if (query_word in inverted_index_body and post_id in inverted_index_body[query_word]) or (query_word in inverted_index_title and post_id in inverted_index_title[query_word]) or (query_word in inverted_index_tags and post_id in inverted_index_tags[query_word]):\n",
    "                if post_id in count_words:\n",
    "                    count_words[post_id] += 1\n",
    "                else:\n",
    "                    count_words[post_id] = 1\n",
    "    # get max value in count_words\n",
    "    max_value = max(count_words.values())\n",
    "    # get all post_ids that have max_value\n",
    "    return [int(post_id) for post_id in count_words if count_words[post_id] == max_value]\n",
    "\n",
    "def CobraSearch(query,posts=posts,top=10):\n",
    "    query_precessed = text_process(query)\n",
    "    print(\"query processed: \",query_precessed)\n",
    "    post_ids_filtered = prefilter_posts(query_precessed,posts)\n",
    "    print(\"number of posts\",len(post_ids_filtered))\n",
    "    # hyperparameters\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    k3 = 1000\n",
    "\n",
    "    # Vi, score function of each feature\n",
    "\n",
    "    print(\"Vi computed\")\n",
    "    W_scores = {}\n",
    "    RSV_scores={}\n",
    "    w_tf_body = 1\n",
    "    w_tf_title = 5\n",
    "    w_tf_tags = 10\n",
    "    w_Ld_body = 1\n",
    "    w_Ld_title = 3\n",
    "    w_Ld_tags = 10\n",
    "    def get_Ld(post_id):\n",
    "        Ld_body = len(posts[posts[\"Id\"] == post_id][\"Body\"].values[0])\n",
    "        Ld_title = len(posts[posts[\"Id\"] == post_id][\"Title\"].values[0])\n",
    "        Ld_tags = len(get_raw_tags_from_postid(post_id))\n",
    "        return Ld_body*w_Ld_body + Ld_title*w_Ld_title + Ld_tags*w_Ld_tags\n",
    "    # avearge Ld over all post_id\n",
    "    print()\n",
    "    m = np.mean([get_Ld(post_id) for post_id in post_ids_filtered])\n",
    "\n",
    "    for post_id in tqdm(post_ids_filtered):\n",
    "\n",
    "        term_freq_body = term_frequency(post_id, inverted_index_body)\n",
    "        term_freq_title = term_frequency(post_id, inverted_index_title)\n",
    "        term_freq_tags = term_frequency(post_id, inverted_index_tags)\n",
    "        tf = lambda word: w_tf_body*term_freq_body[word] + w_tf_title*term_freq_title[word] + w_tf_tags*term_freq_tags[word]\n",
    "        Ld = get_Ld(post_id)\n",
    "        W_scores[str(post_id)] = sum([(k1+1)*tf(word)/(k1*((1-b)+b*Ld/m)+tf(word))*(k3+1)*tf(word)/(k3+tf(word))\n",
    "                               for word in query_precessed if word in inverted_index_body and word in inverted_index_title and word in inverted_index_tags]\n",
    "                               )\n",
    "        RSV_scores[str(post_id)] = W_scores[str(post_id)] + Vi_sums[post_id]\n",
    "    print(\"Scores computed\")\n",
    "    sorted_keys = sorted(RSV_scores, key=RSV_scores.get, reverse=True)\n",
    "    # print(RSV_scores)\n",
    "    return sorted_keys[:top]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query processed:  ['perform', 'multi', 'model']\n",
      "prefiltering posts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67171a4dad5e440b98a74f5ead468997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba650fa75ec4f0f861e6b072f919d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413b7d2c71d44f4b8e3aac911107620e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of posts 17\n",
      "Vi computed\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a273a276eb41dabcb36054922e904f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n"
     ]
    }
   ],
   "source": [
    "result1 = CobraSearch(\"mesure performance for multiclassification model\",posts=posts,top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6778', '8734', '15553', '85435', '63842', '99589', '97503', '84488', '97034', '107796']\n"
     ]
    }
   ],
   "source": [
    "print(result1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interface Tkinter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import webbrowser\n",
    "\n",
    "# create a global variable for the photo object\n",
    "photo = None\n",
    "\n",
    "def search(query, top=10):\n",
    "    # search the query in the database and return the top 10 results\n",
    "    # use the variable posts\n",
    "    # use the function CobraSearch\n",
    "    result = CobraSearch(query)\n",
    "    return result\n",
    "    \n",
    "\n",
    "# Tkinter interface for the search engine\n",
    "# create a window\n",
    "window = tk.Tk()\n",
    "window.title(\"Search Engine\")\n",
    "window.geometry(\"1500x1200\")  # set window size to 800x1500\n",
    "window.resizable(False, False)  # disable window resizing\n",
    "window.configure(bg=\"#333333\")  # set background color to a darker shade of gray\n",
    "\n",
    "# create a label with a custom font, size, and color\n",
    "lbl = tk.Label(window, text=\"Search Engine\", font=(\"Arial Bold\", 50), fg=\"#14DEA5\", bg=\"#333333\")\n",
    "lbl.pack(pady=20)\n",
    "\n",
    "# create a frame for the input section\n",
    "input_frame = tk.Frame(window, bg=\"#333333\")\n",
    "input_frame.pack(pady=20)\n",
    "\n",
    "# create a label for the input section with custom font, size, and color\n",
    "lbl_query = tk.Label(input_frame, text=\"Enter your query\", font=(\"Arial Bold\", 20), fg=\"white\", bg=\"#333333\")\n",
    "lbl_query.grid(row=0, column=0, padx=10, pady=5, sticky=\"e\")\n",
    "\n",
    "# create a text entry box with custom font, size, and color\n",
    "txt = tk.Entry(input_frame, width=50, font=(\"Arial\", 14), bg=\"#555555\", fg=\"white\")\n",
    "txt.grid(row=0, column=1, padx=5, pady=5, sticky=\"w\")\n",
    "\n",
    "# create a frame for the result section\n",
    "result_frame = tk.Frame(window, bg=\"#333333\")\n",
    "result_frame.pack()\n",
    "\n",
    "# function to handle link clicks\n",
    "def open_link(link):\n",
    "    webbrowser.open(link)\n",
    "\n",
    "# call the function search when the button is clicked\n",
    "def clicked():\n",
    "    query = txt.get()\n",
    "    result = search(query, top=10)\n",
    "    result_list.delete(0, tk.END)  # clear previous results\n",
    "    for post_id in result:\n",
    "        result_list.insert(tk.END, f\"https://datascience.stackexchange.com/questions/{post_id}\")\n",
    "   \n",
    "def clicked_result(event):\n",
    "    # get the index of the clicked item\n",
    "    index = result_list.curselection()[0]\n",
    "    # get the text of the clicked item\n",
    "    post_id = result_list.get(index)\n",
    "    # open the link in the browser\n",
    "    open_link(post_id)\n",
    "\n",
    "# bind the click event to the listbox\n",
    "\n",
    "\n",
    "# create a button with a custom font, size, and color\n",
    "btn = tk.Button(window, text=\"Search\", command=clicked, font=(\"Arial\", 14), bg=\"#14DEA5\", fg=\"#333333\")\n",
    "btn.pack(pady=10)\n",
    "\n",
    "# load the image and create a photo object\n",
    "img = Image.open(\"cobralogo.jpeg\")\n",
    "photo = ImageTk.PhotoImage(img)\n",
    "\n",
    "# create a label with the photo object\n",
    "panel = tk.Label(window, image=photo, bg=\"#333333\")\n",
    "panel.pack(pady=20)\n",
    "\n",
    "# create a listbox for displaying the search results with custom font, size, and color\n",
    "result_list = tk.Listbox(result_frame, font=(\"Arial\", 14), fg=\"#14DEA5\", bg=\"#333333\", selectbackground=\"#14DEA5\", selectforeground=\"#333333\")\n",
    "result_list.pack(pady=10, fill=tk.BOTH, expand=True)\n",
    "result_list.config(width=50,height=10) \n",
    "\n",
    "result_list.bind(\"<<ListboxSelect>>\", clicked_result)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of our search method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our search engine we are using the NDCG score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "def get_ndcg_scores(df_relevancy,method=CobraSearch,top=len(posts)):\n",
    "    \"\"\"\n",
    "    Calculates the NDCG (Normalized Discounted Cumulative Gain) score for each query in the given relevance dataframe\n",
    "    using the specified search method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_relevancy : pandas.DataFrame\n",
    "        A dataframe containing the relevance scores for each post and query.\n",
    "        The first column should contain the post IDs, and the remaining columns from the 4th should be named 'query X',\n",
    "        where X is the query number starting from 1.\n",
    "        The values in the columns should be the relevance scores for each post with respect to the corresponding query.\n",
    "    method : function\n",
    "        The search method to use for retrieving the top documents for each query.\n",
    "        The function should take a query string as input and return a pandas.DataFrame containing the top documents.\n",
    "    top : int, optional\n",
    "        The number of top documents to retrieve for each query.\n",
    "        The default value is the total number of posts in the dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing the NDCG score for each query.\n",
    "        The keys are the query numbers starting from 1, and the values are the corresponding NDCG scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # for each PostId in the relevancy dataframe, get the rank of the post according to the method\n",
    "    querys = {}\n",
    "    for i in range(3, 8):\n",
    "        querys[i] = df_relevancy.columns[i][10:]\n",
    "    method_results = {}\n",
    "    for i in range(3, 8):\n",
    "        method_results[i] = method(query=querys[i],top=top) # list of post_id sorted by the method\n",
    "    posts_results = {}\n",
    "    # sort the posts according to the method\n",
    "    for i in range(3, 8):\n",
    "        posts_results[i] = posts[posts[\"Id\"].isin(method_results[i][\"Id\"].astype(int))].copy()\n",
    "        #print(method_results[i].head())\n",
    "    # get the score of each post according to the method\n",
    "    rel_preds = {}\n",
    "    for i in range(3, 8):\n",
    "        # the rankings of the posts in PostId\n",
    "        rel_preds[i] = [list(posts_results[i][\"Id\"]).index(x) if x in posts_results[i][\"Id\"] else top for x in df_relevancy['PostId']]\n",
    "        #print(rel_preds[i])\n",
    "    rel_trues = {}\n",
    "    for i in range(3, 8):\n",
    "        # the rankings of the posts\n",
    "        rel_trues[i] = df_relevancy[df_relevancy.columns[i]].tolist()\n",
    "        #print(rel_trues[i])\n",
    "        rel_trues[i] = [top if np.isnan(x) else x for x in rel_trues[i]]\n",
    "\n",
    "    # calculate the ndcg score for each query\n",
    "    ndcg_scores = {}\n",
    "    for i in range(3, 8):\n",
    "        ndcg_scores[i] = ndcg_score([rel_trues[i]], [rel_preds[i]])\n",
    "    return ndcg_scores\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query processed:  ['perform', 'multi', 'model']\n",
      "prefiltering posts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d39e4539274f67a901f17ddfa18dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d978b500bbb147d49acb69f8f0814e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1347f48f74cc40f8bcdd30e5c3dd556e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of posts 17\n",
      "Vi computed\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05097922a71a472fb13f5836c08d2dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n",
      "query processed:  ['draw', 'neural', 'network']\n",
      "prefiltering posts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f7fb4b753e42b2b55ef8812661f6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19e46a1276143a9a78c740ab4a0d739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c94b2670e44ea29ce8f63a8d5a34e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of posts 4295\n",
      "Vi computed\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f0154f38ec4aa182c82e524d22eac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4295 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read Relevancy CSV\n",
    "df_relevancy = pd.read_excel(\"evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\")\n",
    "df_relevancy.head()\n",
    "df_relevancy\n",
    "print(get_ndcg_scores(df_relevancy,method=CobraSearch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
