{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Group 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/himmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "FILEPATH = \"datascience.stackexchange.com\" # path to unzipped data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log10,sqrt\n",
    "import re\n",
    "import tkinter as tk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def extract_data(filepath):\n",
    "    return pd.read_xml(filepath, parser=\"etree\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id  PostTypeId             CreationDate  Score  ViewCount  \\\n",
      "0  115535           1  2022-10-24T12:58:24.757      1       29.0   \n",
      "1  115536           1  2022-10-24T13:45:55.820      0       30.0   \n",
      "2  115537           1  2022-10-24T13:56:40.603      0       12.0   \n",
      "3  115538           2  2022-10-24T14:36:39.480      1        NaN   \n",
      "4  115539           1  2022-10-24T15:22:37.823      0       14.0   \n",
      "\n",
      "                                                Body  OwnerUserId  \\\n",
      "0  [pa, in, advance, if, this, question, is, so, ...      30838.0   \n",
      "1  [pi, just, need, to, check, my, understanding,...      87037.0   \n",
      "2  [pi, am, trying, to, tune, gradient, boost, ca...      64199.0   \n",
      "3  [p, it, is, correct, if, you, compare, neural,...     119140.0   \n",
      "4  [pi, starting, to, study, how, to, rank, words...     141937.0   \n",
      "\n",
      "          LastActivityDate                                              Title  \\\n",
      "0  2022-10-24T12:58:24.757  [information, retrieval, vs, recommendation, s...   \n",
      "1  2022-10-24T14:36:39.480                          [the, idea, behind, drop]   \n",
      "2  2022-10-24T13:56:40.603  [maximize, accuracy, with, differential, evolu...   \n",
      "3  2022-10-24T14:36:39.480                                                 []   \n",
      "4  2022-10-24T15:22:37.823              [help, using, b, to, rank, sentences]   \n",
      "\n",
      "                                                Tags  ...  ClosedDate  \\\n",
      "0    <recommender-system><information-retrieval><ai>  ...        None   \n",
      "1                                          <dropout>  ...        None   \n",
      "2  <machine-learning><r><accuracy><ensemble-learn...  ...        None   \n",
      "3                                               None  ...        None   \n",
      "4  <machine-learning-model><tfidf><information-re...  ...        None   \n",
      "\n",
      "   ContentLicense AcceptedAnswerId LastEditorUserId  LastEditDate  ParentId  \\\n",
      "0    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "1    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "2    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "3    CC BY-SA 4.0              NaN              NaN          None  115536.0   \n",
      "4    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "\n",
      "  OwnerDisplayName  CommunityOwnedDate LastEditorDisplayName FavoriteCount  \n",
      "0             None                None                  None           NaN  \n",
      "1             None                None                  None           NaN  \n",
      "2             None                None                  None           NaN  \n",
      "3             None                None                  None           NaN  \n",
      "4             None                None                  None           NaN  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# posts = extract_data(filepath=FILEPATH+\"/Posts.xml\") # precomputed tokenization in file posts.pkl\n",
    "users = extract_data(filepath=FILEPATH+\"/Users.xml\") # usefull for user reputation\n",
    "comments = extract_data(filepath=FILEPATH+\"/Comments.xml\") # usefull for user reputation and accepted answer\n",
    "votes = extract_data(filepath=FILEPATH+\"/Votes.xml\") # usefull for post score\n",
    "badges = extract_data(filepath=FILEPATH+\"/Badges.xml\") # usefull for user reputation\n",
    "file = \"posts.pkl\"\n",
    "with open(file, 'rb') as f:\n",
    "    posts = pkl.load(f)\n",
    "\n",
    "print(posts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(posts,column): # returns inverted index\n",
    "    inverted_index = {}\n",
    "    for i in range(len(posts)):\n",
    "        post_id = str(posts['Id'][i])\n",
    "        for word in posts[column][i]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {}\n",
    "            if post_id not in inverted_index[word]:\n",
    "                inverted_index[word][post_id] = 0\n",
    "            inverted_index[word][post_id] = inverted_index[word][post_id] + 1\n",
    "    return inverted_index\n",
    "  \n",
    "inverted_index_body = get_inverted_index(posts,\"Body\")\n",
    "inverted_index_title = get_inverted_index(posts,\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doing', 'where', 'this', 'again', 'other', 'didn', 'each', 'yours', 'before', 'mustn', 'about', 'if', 'be', 'nor', 'not', \"isn't\", 'll', 'up', 'through', 'we', 'her', \"didn't\", \"hasn't\", 're', 'is', 'don', 'same', 'all', 'isn', 'these', 'until', 'him', 'its', 'd', 'above', 'some', 've', 'do', 'who', 'into', 'my', 'does', 'have', 'aren', 'myself', 'of', 'them', 'then', 'so', 'they', \"it's\", 'will', 'haven', 'most', \"you're\", \"you'd\", \"weren't\", 'against', 'between', 'because', 'he', 'shouldn', \"that'll\", 'it', 'below', \"haven't\", 'his', 'both', 'any', 'wasn', 'for', 'no', 'doesn', 'can', 'with', 'than', 'o', 'weren', \"wouldn't\", 'our', 'once', 'are', \"aren't\", 'while', 'an', 'own', 'when', 'whom', 'very', 'themselves', 'only', 'ma', 'theirs', 'by', 'had', 'or', 'at', 'after', 'that', 'further', 'there', 't', 'yourself', 'over', 'itself', \"you'll\", \"don't\", 'she', 'having', 'am', \"you've\", 'being', 'to', 'why', 'during', \"mightn't\", 'from', \"shouldn't\", 'won', 'the', 'your', 'hers', 'now', 'down', \"hadn't\", 'yourselves', 'wouldn', 'as', \"wasn't\", 'm', 'has', 'ain', 'which', 'was', 'did', 's', \"mustn't\", 'me', 'ours', 'i', 'hasn', 'himself', 'you', 'been', 'in', 'more', 'their', 'such', 'but', 'y', \"won't\", 'few', 'what', 'those', 'and', 'hadn', 'under', 'couldn', \"doesn't\", 'herself', \"needn't\", 'a', 'shan', \"she's\", \"should've\", 'mightn', \"couldn't\", 'p', 'out', 'on', 'just', \"shan't\", 'here', 'ourselves', 'needn', 'too', 'should', 'were', 'how', 'off'}\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "stop_words_nltk = stopwords.words('english') # list of stopwords\n",
    "stop_words = set(stop_words_nltk + [\"p\"])\n",
    "print(stop_words)\n",
    "\n",
    "def remove_stopwords(inverted_index):\n",
    "    for word in stop_words:\n",
    "        if word in inverted_index:\n",
    "            del inverted_index[word]\n",
    "    return inverted_index\n",
    "\n",
    "inverted_index_body = remove_stopwords(inverted_index_body)\n",
    "inverted_index_title = remove_stopwords(inverted_index_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(inverted_index, posts): # returns tf_idf\n",
    "    tf_idf = {}\n",
    "    for word in inverted_index:\n",
    "        tf_idf[word] = {}\n",
    "        for post_id in inverted_index[word]:\n",
    "            tf_idf[word][post_id] = inverted_index[word][post_id] * log10(len(posts)/len(inverted_index[word]))\n",
    "    return tf_idf\n",
    "\n",
    "tf_idf_body = get_tf_idf(inverted_index_body, posts)\n",
    "tf_idf_title = get_tf_idf(inverted_index_title, posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('116141', 21.661042056227643), ('339', 21.661042056227643), ('109823', 19.380932366098417), ('76841', 15.96076783090458), ('95007', 15.96076783090458)]\n",
      "[('116141', 19), ('339', 19), ('109823', 17), ('76841', 14), ('95007', 14)]\n",
      "[('29542', 3.6589177006247477), ('65063', 3.6589177006247477), ('11404', 3.6589177006247477), ('64403', 3.6589177006247477), ('29057', 3.6589177006247477)]\n",
      "[('29542', 2), ('65063', 2), ('11404', 2), ('64403', 2), ('29057', 2)]\n"
     ]
    }
   ],
   "source": [
    "# posts that contain the word python the most in the body and title\n",
    "print(sorted(tf_idf_body[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(inverted_index_body[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(tf_idf_title[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(inverted_index_title[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_inverted_index(inverted_index):\n",
    "    stemmed_inverted_index = {}\n",
    "    for word in inverted_index:\n",
    "        stemmed = stemmer.stem(word) # stem word\n",
    "        if stemmed not in stemmed_inverted_index:\n",
    "            stemmed_inverted_index[stemmed] = {}\n",
    "        for post_id in inverted_index[word]:\n",
    "            if post_id not in stemmed_inverted_index[stemmed]:\n",
    "                stemmed_inverted_index[stemmed][post_id] = 0\n",
    "            stemmed_inverted_index[stemmed][post_id] = stemmed_inverted_index[stemmed][post_id] + inverted_index[word][post_id]\n",
    "    return stemmed_inverted_index\n",
    "\n",
    "stemmed_inverted_index_body = stem_inverted_index(inverted_index_body)\n",
    "stemmed_inverted_index_title = stem_inverted_index(inverted_index_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('115768', 13), ('60039', 11), ('119916', 11), ('20380', 9), ('117097', 8)]\n",
      "[('76321', 54), ('74666', 25), ('115768', 13), ('60039', 11), ('119916', 11)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(inverted_index_body[\"open\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(stemmed_inverted_index_body[\"open\"].items(), key=lambda x: x[1], reverse=True)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inform', 'retriev']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "def text_process(query):\n",
    "    # tokenize query (or text) with bert-case-uncased tokenizer\n",
    "    # keep only alpha words\n",
    "    # remove stopwords\n",
    "    # stem query\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "    query = tokenizer.tokenize(query)\n",
    "    query = [word for word in query if word.isalpha()]\n",
    "    query = [word for word in query if word not in stop_words]\n",
    "    query = [stemmer.stem(word) for word in query]\n",
    "    return query\n",
    "\n",
    "query = \"information retrieval\"\n",
    "query = text_process(query)\n",
    "print(query) # ['error', 'open', 'file', 'python']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting relevant metadata from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ']' (2066743062.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    return [tag.replace(\"-\", \" \") for tag in taglist]]\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ']'\n"
     ]
    }
   ],
   "source": [
    "def get_tags(post_id):\n",
    "    taglist = posts[posts[\"Id\"] == post_id][\"Tags\"]\n",
    "    if taglist.values[0] is None: return []\n",
    "    taglist = taglist.values[0].replace(\"<\", \"\").replace(\">\", \" \").split(\" \")\n",
    "    return [tag.replace(\"-\", \" \") for tag in taglist]]\n",
    "\n",
    "def get_all_tags():\n",
    "    all_tags = []\n",
    "    for i in range(len(posts)):\n",
    "        all_tags = all_tags + get_tags(posts[\"Id\"][i])\n",
    "    return list(set(all_tags))\n",
    "\n",
    "# all_tags = get_all_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'regex', 'nosql', 'validation', 'odds', 'rocr-package', 'mlops', 'bayesian', 'rasa-nlu', 'perplexity', 'data-source', 'real-ml-usecase', 'randomized-algorithms', 'sequential-pattern-mining', 'google-prediction-api', 'feature-scaling', 'noise', 'colab', 'marketing', 'survival-analysis', 'python-polars', 'markov', 'bootstraping', 'uncertainty', 'jaccard-coefficient', 'fasttext', 'ibm-watson', 'cross-entropy', 'json', 'state-of-the-art', 'training', 'ai', 'ngrams', 'machine-translation', 'multitask-learning', 'probability-calibration', 'api', 'embeddings', 'web-scraping', 'octave', 'apache-kafka', 'exploratory-factor-analysis', 'normalization', 'mse', 'csv', 'catboost', 'stacked-lstm', 'bigdata', 'vgg16', 'anonymization', 'hyperparameter', 'caret', 'openai-gpt', 'flask', 'methodology', 'visualization', 'gradient', 'elastic-search', 'scoring', 'finite-precision', 'kernel', 'consumerweb', 'knowledge-base', 'nlp', 'bioinformatics', 'spatial-transformer', 'stacking', 'pandas', 'apache-pig', 'definitions', 'cost-function', 'inceptionresnetv2', 'metaheuristics', 'tools', 'collinearity', 'libsvm', 'keras', 'conda', 'tfidf', 'binary-classification', 'rbf', 'missing-data', 'grid-search', 'sagemaker', 'knowledge-distillation', 'numerical', 'finetuning', 'labels', 'cs231n', 'hypothesis-testing', 'chatbot', 'audio-recognition', 'svr', 'mxnet', 'mlflow', 'bert', 'pearsons-correlation-coefficient', 'transfer-learning', 'self-study', 'reward', 'rstudio', 'association-rules', 'pvalue', 'learning-rate', 'gini-index', 'sentiment-analysis', 'data-augmentation', 'pooling', 'search-engine', 'gaussian-process', 'algorithms', 'dplyr', 'manhattan', 'counts', 'social-network-analysis', 'dynamic-programming', 'epochs', 'analysis', 'least-squares-svm', 'information-extraction', 'text-classification', 'hpc', 'multi-instance-learning', 'hbase', 'feature-engineering', 'hierarchical-data-format', 'ensemble-learning', 'ngboost', 'goodness-of-fit', 'bart', 'multi-output', 'data-leakage', 'education', 'markov-process', 'list', 'mlp', 'ocr', 'unsupervised-learning', 'activity-recognition', 'vector-space-models', 'linear-regression', 'momentum', 'gmm', 'document-term-matrix', 'spyder', 'feature-selection', 'manifold', 'community', 'time-series', 'hinge-loss', 'convolutional-neural-network', 'sampling', 'image-preprocessing', 'taxonomy', 'crawling', 'outlier', 'dropout', 'beginner', 'learning', 'one-shot-learning', 'retraining', 'feature-interaction', 'conformalprediction', 'cross-validation', 'agglomerative', 'neo4j', 'infographics', 'nlg', 'labeling', 'policy-gradients', 'map-reduce', 'mean', 'heatmap', 'hashingvectorizer', 'functional-api', 'text-generation', 'probability', 'serialisation', 'early-stopping', 'softmax', 'reference-request', 'javascript', 'question-answering', 'deployment', 'ethical-ai', 'domain-adaptation', 'hardware', 'naive-bayes-classifier', 'cloud-computing', 'python', 'generative-models', 'auc', 't', 'k-means', 'shap', 'density-estimation', 'tsne', 'data-stream-mining', 'git', 'error-handling', 'alex-net', 'simulation', 'learning-to-rank', 'prophet', 'distributed', 'faster-rcnn', 'hyperparameter-tuning', 'sigmoid', 'replication', 'research', 'deepmind', 'kalman-filter', 'derivation', 'boosting', 'coherence', 'automatic-summarization', 'actor-critic', 'feature-extraction', 'feature-importances', 'federated-learning', 'predictor-importance', 'rfe', 'time', 'monte-carlo', 'windows', 'anomaly-detection', 'reshape', 'google-cloud-platform', 'confidence', 'entropy', 'privacy', 'c++', 'reinforcement-learning', 'redshift', 'scipy', 'rnn', 'structural-equation-modelling', 'programming', 'rbm', 'logistic', 'torch', 'categorical-data', 'finance', 'stemming', 'text-to-columns', 'semantic-segmentation', 'anaconda', 'parameter', 'natural-gradient-boosting', 'speech-to-text', 'mongodb', 'normal', 'interpretation', 'cost-sensitive-learning', 'online-learning', 'keras-rl', 'loss-function', 'concept-drift', 'reproducibility', 'dummy-variables', 'version-control', 'mean-shift', 'arima', 'mutual-information', 'feature-construction', 'encoder', 'combinatorics', 'nvidia', 'imbalanced-learn', 'tpu', 'fastai', 'library', 'competitions', 'named-entity-recognition', 'r-squared', 'esl', 'one-class-classification', 'pytorch-geometric', 'adversarial-ml', 'dqn', 'coursera', 'information-theory', 'pgm', 'intuition', 'naive-bayes-algorithim', 'target-encoding', 'pytorch', 'memory', 'knime', 'regularization', 'weka', 'xgboost', 'pruning', 'image-classification', 'facebook', 'sequence-to-sequence', 'mini-batch-gradient-descent', 'ridge-regression', 'mcmc', 'image', 'efficiency', 'causalimpact', 'google-cloud', 'kendalls-tau-coefficient', 'bayes-error', 'gan', 'noisification', 'pip', 'semantic-similarity', 'linear-models', 'imbalanced-data', 'overfitting', 'evolutionary-algorithms', 'binary', 'inception', 'discriminant-analysis', 'networkx', 'gradient-descent', 'descriptive-statistics', 'apache-spark', 'stata', 'processing', 'self-driving', 'computer-vision', 'text', 'test', 'sparse', 'evaluation', 'universal-approximation-theorem', 'ensemble-modeling', 'apache-mahout', 'gpu', 'data-table', 'mnist', 'code', 'semi-supervised-learning', 'share-point', 'dbscan', 'pcamixdata', 'project-planning', 'score', 'cuda', 'ggplot2', 'mathematics', 'text-processing', 'vae', 'automl', 'meta-learning', 'data-science-model', 'matplotlib', 'software-recommendation', 'activation-function', 'entity-matching', 'partial-least-squares', 'kaggle', 'regression', 'neural-network', 'classificationmulti', 'cause-and-effect', 'normal-equation', 'gradient-boosting-decision-trees', 'ann', 'usecase', 'symbolic-learning', 'kedro', 'custom-layer', 'precision-recall-curve', 'interpolation', 'genetic-algorithms', 'forecasting', 'bag-of-words', 'career', 'few-shot-learning', 'object-recognition', 'non-parametric', 'pasting', 'lightgbm', 'cloud', 'pymc3', 'text-mining', 'software-development', 'data-indexing-techniques', 'train', 'siamese-networks', 'variance', 'word-embeddings', 'predict', 'features', 'data', 'data-cleaning', 'clustering', 'classification', 'lsi', 'structured-data', 'lstm', 'apache-hadoop', 'pattern-recognition', 'search', 'dictionary', 'encoding', 'smote', 'similar-documents', 'openai-gym', 'estimation', 'bagging', 'rattle', 'best-practice', 'hashing-trick', 'methods', 'predictive-modeling', 'smotenc', 'one-hot-encoding', 'data-formats', 'google-bigquery', 'doc2vec', 'sensors', 'boruta', 'open-source', 'pacf', 'prediction', 'distance', 'data-wrangling', 'lime', 'random-forest', 'elastic-net', 'implementation', 'parallel', 'google', 'fuzzy-classification', 'homework', 'statistics', 'model-selection', 'probabilistic-programming', 'accuracy', 'market-basket-analysis', 'pycaret', 'mysql', 'bayesian-nonparametric', 'parameter-estimation', 'azure-ml', 'java', 'svm', 'oversampling', 'language-model', 'recommender-system', 'matrix', 'hive', 'difference', 'poisson', 'indexing', 'transformation', 'cart', 'numpy', 'h2o', 'management', 'discounted-reward', 'excel', 'masking', 'multivariate-distribution', 'data-mining', 'sequence', 'allennlp', 'featurization', 'chi-square-test', 'relational-dbms', 'logarithmic', 'cosine-distance', 'theano', 'object-detection', 'google-data-studio', 'nltk', 'linux', 'corpus', 'pybrain', 'gridsearchcv', 'scikit-learn', 'grammar-inference', 'jupyter', 'functions', 'dataset', 'partial-dependence-plot', 'glorot-initialization', 'histogram', 'sports', 'topic-model', 'class-imbalance', 'feature-reduction', 'multiclass-classification', 'neural-style-transfer', 'c', 'gnn', 'markov-hidden-model', 'anomaly', 'linear-programming', 'scalability', 'pyspark', 'seaborn', 'notation', 'lasso', 'plotly', 'metric', 'experiments', 'churn', 'data-engineering', 'image-segmentation', 'objective-function', 'wasserstein', 'tableau', 'cyclegan', 'supervised-learning', 'history', 'categorical-encoding', 'data-analysis', 'geospatial', 'gbm', 'r', 'forecast', 'k-nn', 'convolution', '3d-reconstruction', 'machine-learning-model', 'spacy', 'bias', 'f1score', 'performance', 'distribution', 'fourier', 'weight-initialization', 'ab-test', 'perceptron', 'knowledge-graph', 'hdbscan', 'loss', 'context-vector', 'word2vec', 'lda', 'shadow-deployment', 'sgd', 'batch-normalization', 'data-quality', 'feature-map', 'bar-chart', 'huggingface', 'text-filter', 'bokeh', 'image-size', 'metadata', 'amazon-ml', 'wolfram-language', '3d-object-detection', 'game', 'caffe', 'explainable-ai', 'sas', 'matlab', 'dirichlet', 'spearmans-rank-correlation', 'backtest', 'python-3.x', 'yolo', 'decision-trees', 'graph-neural-network', 'entity-linking', 'active-learning', 'recurrent-neural-network', 'databases', 'annotation', 'rmse', 'ndcg', 'powerbi', 'similarity', 'spectral-clustering', 'multilabel-classification', 'convergence', 'aws', 'pipelines', 'groupby', 'image-recognition', 'tokenization', 'correlation', 'parsing', 'scala', 'document-understanding', 'pickle', 'pretraining', 'pca', 'confusion-matrix', 'aggregation', 'tensorflow', 'glm', 'time-complexity', 'field-aware-factorization-machines', 'backpropagation', 'ipython', 'weighted-data', 'etl', 'wikipedia', 'tensorboard', 'matrix-factorisation', 'tflearn', 'stanford-nlp', 'movielens', 'twitter', 'ranking', 'statsmodels', 'dataframe', 'preprocessing', 'fuzzy-logic', 'opencv', 'genetic', 'scraping', 'gaussian', 'duplicate', 'linearly-separable', 'gensim', 'imbalance', 'graphs', 'classifier', 'representation', 'hog', 'lda-classifier', 'sql', 'isolation-forest', 'orange', 'rapidminer', 'cnn', 'attention-mechanism', '.net', 'knowledge-canonicalization', 'julia', 'tranformation', 'labelling', 'graphical-model', 'terminology', 'word', 'gru', 'spss', 'automation', 'umap', 'linear-algebra', 'generalization', 'estimators', 'theory', 'roc', 'optimization', 'information-retrieval', 'logistic-regression', 'tesseract', 'q-learning', 'data-imputation', 'inference', 'graphviz', 'deep-learning', 'plotting', 'lemmatization', 'dimensionality-reduction', 'data-drift', 'aws-lambda', 'freebase', 'adaboost', 'ensemble', 'anova', 'hashing', 'feedback-loop', 'deepar', 'books', 'expectation-maximization', 'model-evaluations', 'vc-theory', 'permutation-test', 'dynamic-time-warping', 'bayesian-networks', 'autoencoder', 'machine-learning', 'pac-learning', 'sparsity', 'transformer', 'data-product', 'torchvision', 'genetic-programming']\n"
     ]
    }
   ],
   "source": [
    "# print(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60888.0\n"
     ]
    }
   ],
   "source": [
    "def get_body(posts, post_id):\n",
    "    # tokenized body of the post\n",
    "    body = posts[posts[\"Id\"] == post_id][\"Body\"].values[0]\n",
    "    return body\n",
    "\n",
    "def get_title(posts, post_id):\n",
    "    # tokenized title of the post\n",
    "    title = posts[posts[\"Id\"] == post_id][\"Title\"].values[0]\n",
    "    return title\n",
    "\n",
    "def get_tags_from_postid(post_id):\n",
    "    taglist = posts[posts[\"Id\"] == post_id][\"Tags\"]\n",
    "    if taglist.values[0] is None: return []\n",
    "    taglist = taglist.values[0].replace(\"<\", \"\").replace(\">\", \" \").split(\" \")\n",
    "    # replace \"-\"\" with \" \"\n",
    "    for i in range(len(taglist)):\n",
    "        taglist[i] = text_process(taglist[i].replace(\"-\", \" \"))\n",
    "    return taglist[:-1]\n",
    "\n",
    "\n",
    "# print(get_tags_from_postid(115768))\n",
    "\n",
    "def get_reputation(post_id):\n",
    "    # reputation of the user who posted the post\n",
    "    reputation = users[users[\"Id\"] == int(posts[posts[\"Id\"] == post_id][\"OwnerUserId\"])][\"Reputation\"].values[0]\n",
    "    return reputation\n",
    "\n",
    "# print(get_reputation(posts, 115768))\n",
    "def get_inverted_index_tags(posts,tag_to_processed_tag_dict):\n",
    "    # Inverted index of tags\n",
    "    inverted_index_tags = {}\n",
    "    for i in range(len(posts)):\n",
    "        post_id = posts[\"Id\"][i]\n",
    "        taglist = [tag_to_processed_tag_dict[tag] for tag in get_tags_from_postid(post_id)]\n",
    "        taglist = [tag for tag in taglist if tag is not None]\n",
    "        for tag in taglist:\n",
    "            if tag not in inverted_index_tags:\n",
    "                inverted_index_tags[tag] = {}\n",
    "            if post_id not in inverted_index_tags[tag]:\n",
    "                inverted_index_tags[tag][post_id] = 0\n",
    "            inverted_index_tags[tag][post_id] = inverted_index_tags[tag][post_id] + 1\n",
    "\n",
    "\n",
    "def get_votes(post_id):\n",
    "    # number of votes of the post\n",
    "    nb_votes = posts[posts[\"Id\"] == post_id][\"Score\"].values[0]\n",
    "    return nb_votes\n",
    "\n",
    "def get_number_answers(post_id):\n",
    "    # number of answers of the post\n",
    "    nb_answers = posts[posts[\"ParentId\"] == post_id].shape[0]\n",
    "    return nb_answers\n",
    "\n",
    "def get_badges_user(post_id):\n",
    "    # number of badges of the user who posted the post\n",
    "    # use the variable badges and users\n",
    "    nb_badges = badges[badges[\"UserId\"] == int(posts[posts[\"Id\"] == post_id][\"OwnerUserId\"])][\"Class\"].shape[0]\n",
    "    return nb_badges\n",
    "\n",
    "def get_answered(post_id):\n",
    "    # 1 if the post is answered, 0 otherwise\n",
    "    # use the variable comments\n",
    "    if comments[comments[\"PostId\"] == int(post_id)].shape[0] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_views(post_id):\n",
    "    # number of views of the post\n",
    "    # use the variable posts\n",
    "    nb_views = posts[posts[\"Id\"] == post_id][\"ViewCount\"].values[0]\n",
    "    return nb_views\n",
    "\n",
    "# print(posts.head())\n",
    "\n",
    "print(get_views(12761))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5201cefc23eb4b9192d2a2c1e5d51cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# tag_to_processed_tag_dict ={}\n",
    "# for tag in tqdm(all_tags):\n",
    "#     tag_to_processed_tag_dict[tag] = text_process(tag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save tag_to_processed_tag_dict\n",
    "# import pickle\n",
    "# with open('tag_to_processed_tag_dict.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tag_to_processed_tag_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)$\n",
    "# load tag_to_processed_tag_dict\n",
    "import pickle\n",
    "with open('tag_to_processed_tag_dict.pickle', 'rb') as handle:\n",
    "    tag_to_processed_tag_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inverted_index_tags \u001b[39m=\u001b[39m get_inverted_index_tags(posts,tag_to_processed_tag_dict) \n",
      "Cell \u001b[0;32mIn[26], line 34\u001b[0m, in \u001b[0;36mget_inverted_index_tags\u001b[0;34m(posts, tag_to_processed_tag_dict)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(posts)):\n\u001b[1;32m     33\u001b[0m     post_id \u001b[39m=\u001b[39m posts[\u001b[39m\"\u001b[39m\u001b[39mId\u001b[39m\u001b[39m\"\u001b[39m][i]\n\u001b[0;32m---> 34\u001b[0m     taglist \u001b[39m=\u001b[39m [tag_to_processed_tag_dict[tag] \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m get_tags_from_postid(post_id)]\n\u001b[1;32m     35\u001b[0m     taglist \u001b[39m=\u001b[39m [tag \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m taglist \u001b[39mif\u001b[39;00m tag \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m taglist:\n",
      "Cell \u001b[0;32mIn[26], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(posts)):\n\u001b[1;32m     33\u001b[0m     post_id \u001b[39m=\u001b[39m posts[\u001b[39m\"\u001b[39m\u001b[39mId\u001b[39m\u001b[39m\"\u001b[39m][i]\n\u001b[0;32m---> 34\u001b[0m     taglist \u001b[39m=\u001b[39m [tag_to_processed_tag_dict[tag] \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m get_tags_from_postid(post_id)]\n\u001b[1;32m     35\u001b[0m     taglist \u001b[39m=\u001b[39m [tag \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m taglist \u001b[39mif\u001b[39;00m tag \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     36\u001b[0m     \u001b[39mfor\u001b[39;00m tag \u001b[39min\u001b[39;00m taglist:\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "inverted_index_tags = get_inverted_index_tags(posts,tag_to_processed_tag_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save inverted index tags as pkl\n",
    "with open('inverted_index_tags.pkl', 'wb') as f:\n",
    "    pickle.dump(inverted_index_tags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inverted index tags from pkl\n",
    "with open('inverted_index_tags.pkl', 'rb') as f:\n",
    "    inverted_index_tags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recommender-system', 'information-retrieval', 'ai', 'dropout', 'machine-learning', 'r', 'accuracy', 'ensemble-learning', 'metaheuristics', 'machine-learning-model', 'tfidf', 'ranking', 'learning-to-rank', 'classification', 'nlp', 'multiclass-classification', 'reinforcement-learning', 'policy-gradients', 'tableau', 'rstudio', 'binary-classification', 'bigdata', 'apache-hadoop', 'map-reduce', 'deep-learning', 'neural-network', 'convolutional-neural-network', 'mlp', 'python', 'time-series', 'churn', 'random-forest', 'supervised-learning', 'computer-vision', 'language-model', 'openai-gpt', 'deepmind', 'image-classification', 'transformer', 'huggingface', 'pretraining', 'data-mining', 'web-scraping', 'prediction', 'forecasting', 'pandas', 'data-cleaning', 'data', 'preprocessing', 'data-analysis', 'feature-engineering', 'feature-extraction', 'lstm', 'feature-scaling', 'loss-function', 'optimization', 'statistics', 'counts', 'pytorch', 'normalization', 'sql', 'databases', 'mysql', 'dataset', 'object-detection', 'outlier', 'grid-search', 'isolation-forest', 'excel', 'image-preprocessing', 'image-segmentation', 'semantic-segmentation', 'fourier', 'numpy', 'dataframe', 'matplotlib', 'sampling', 'keras', 'cnn', 'multilabel-classification', 'spacy', 'svm', 'regression', 'algorithms', 'text-classification', 'graphs', 'scikit-learn', 'ocr', 'faster-rcnn', 'community', 'pca', 'predictive-modeling', 'deployment', 'dimensionality-reduction', 'pyspark', 'data-science-model', 'model-selection', 'feature-importances', 'multivariate-distribution', 'training', 'loss', 'distribution', 'tensorflow', 'gpu', 'visualization', 'xgboost', 'hyperparameter', 'hyperparameter-tuning', 'hardware', 'stanford-nlp', 'clustering', 'anomaly-detection', 'feature-selection', 'correlation', 'data-leakage', 'unsupervised-learning', 'encoding', 'dummy-variables', 'code', 'mathematics', 'class-imbalance', 'evaluation', 'kernel', 'jupyter', 'bert', 'semi-supervised-learning', 'matlab', 'c', 'javascript', 'wolfram-language', 'overfitting', 'confusion-matrix', 'apache-spark', 'softmax', 'finetuning', 'learning', 'pipelines', 'data-engineering', 'machine-translation', 'tokenization', 'hierarchical-data-format', 'automation', 'k-means', 'linear-regression', 'implementation', 'annotation', 'one-hot-encoding', 'sgd', 'logistic-regression', 'decision-trees', 'ensemble-modeling', 'ensemble', 'software-development', 'rmse', 'r-squared', 'transfer-learning', 'text-mining', 'descriptive-statistics', 'cross-validation', 'validation', 'self-study', 'nltk', 'bias', 'batch-normalization', 'estimation', 'labelling', 'image-recognition', 'ggplot2', 'rnn', 'python-3.x', 'bag-of-words', 'arima', 'naive-bayes-classifier', 'missing-data', 'shap', 'text-generation', 'bagging', 'topic-model', 'lda', 'embeddings', 'graph-neural-network', 'word-embeddings', 'word2vec', 'data-imputation', 'gradient-boosting-decision-trees', 'categorical-data', 'cause-and-effect', 'one-class-classification', 'confidence', 'federated-learning', 'named-entity-recognition', 'lasso', 'q-learning', 'performance', 'k-nn', 'variance', 'gaussian', 'finance', 'nosql', 'sequence-to-sequence', 'distance', 'generative-models', 'gradient-descent', 'momentum', 'gradient', 'feature-construction', 'pytorch-geometric', 'imbalanced-learn', 'marketing', 'data-wrangling', 'linear-algebra', 'chi-square-test', 'structural-equation-modelling', 'sentiment-analysis', 'autoencoder', 'gan', 'processing', 'labels', 'probability-calibration', 'elastic-net', 'mse', 'siamese-networks', 'market-basket-analysis', 'data-augmentation', 'reference-request', 'search', 'genetic-algorithms', 'openai-gym', 'probability', 'sequence', 'least-squares-svm', 'mnist', 'pooling', 'sequential-pattern-mining', 'pattern-recognition', 'gensim', 'orange', 'dqn', 'partial-least-squares', 'dplyr', 'jaccard-coefficient', 'f1score', 'cart', 'kaggle', 'score', 'bioinformatics', 'google', 'backpropagation', 'association-rules', 'markov-process', 'ngrams', 'epochs', 'convolution', 'weighted-data', 'similar-documents', 'vector-space-models', 'metric', 'regularization', 'error-handling', 'reshape', 'beginner', 'theory', 'domain-adaptation', 'histogram', 'csv', 'terminology', 'definitions', 'activation-function', 'chatbot', 'regex', 'keras-rl', 'game', 'perceptron', 'books', 'learning-rate', 'similarity', 'weka', 'plotting', 'seaborn', 'tsne', 'text', 'rbm', 'forecast', 'software-recommendation', 'twitter', 'cost-function', 'scalability', 'experiments', 'ab-test', 'gbm', 'parallel', 'discriminant-analysis', 'ipython', 'fuzzy-logic', 'social-network-analysis', 'google-prediction-api', 'scipy', 'markov-hidden-model', 'lightgbm', 'noise', 'inception', 'colab', 'classifier', 'active-learning', 'homework', 'neural-style-transfer', 'audio-recognition', 'binary', 'aggregation', 'predictor-importance', 'scraping', 'sas', 'parameter-estimation', 'distributed', 'education', 'career', 'smote', 'survival-analysis', 'theano', 'linux', 'inference', 'google-bigquery', 'model-evaluations', 'mlops', 'interpretation', 'lime', 'gridsearchcv', 'cyclegan', 'mutual-information', 'opencv', 'time', 'auc', 'roc', 'image', '3d-object-detection', 'geospatial', 'categorical-encoding', 'rbf', 'octave', 'matrix', 'search-engine', 'object-recognition', 'monte-carlo', 'matrix-factorisation', 'parsing', 'semantic-similarity', 'attention-mechanism', 'imbalance', 'svr', 'features', 'normal-equation', 'multi-output', 'logarithmic', 'efficiency', 'causalimpact', 'powerbi', 'cuda', 'gnn', 'google-cloud-platform', 'speech-to-text', 'mini-batch-gradient-descent', 'hypothesis-testing', 'h2o', 'catboost', 'density-estimation', 'target-encoding', 'data-indexing-techniques', 'numerical', 'fastai', 'indexing', 'knowledge-graph', 'structured-data', 'transformation', 'explainable-ai', 'weight-initialization', 'mlflow', 'genetic', 'tools', 'apache-pig', 'open-source', 'freebase', 'hashingvectorizer', 'bayesian', 'cosine-distance', 'management', 'online-learning', 'generalization', 'knime', 'aws', 'gaussian-process', 'relational-dbms', 'data-formats', 'bayesian-networks', 'statsmodels', 'boosting', 'non-parametric', 'anova', 'sensors', 'cloud-computing', 'azure-ml', 'vgg16', 'word', 'methodology', 'yolo', 'nvidia', 'activity-recognition', 'nlg', 'anonymization', 'text-filter', 'java', 'state-of-the-art', 'ann', 'randomized-algorithms', 'dbscan', 'etl', 'hpc', 'graphical-model', 'bootstraping', 'glm', 'inceptionresnetv2', 'information-theory', 'research', 'interpolation', 'tranformation', 'scoring', 'stacked-lstm', 'data-quality', 'c++', 'goodness-of-fit', 'text-processing', 'api', 'retraining', 'convergence', 'memory', 'serialisation', 'mxnet', 'automatic-summarization', 'fasttext', 'mean', 'adaboost', 'naive-bayes-algorithim', 'automl', 'corpus', 'evolutionary-algorithms', 'pac-learning', 'anaconda', 'data-product', 'dynamic-programming', 'data-table', 'vc-theory', 'julia', 'ridge-regression', 'bayes-error', 'apache-mahout', 'hbase', 'methods', 'lda-classifier', 'programming', 'windows', 'doc2vec', 'aws-lambda', 'masking', 'groupby', 'concept-drift', 'cloud', 'featurization', 'mcmc', 'early-stopping', 'ngboost', 'version-control', 'linear-models', 'collinearity', 'natural-gradient-boosting', 'derivation', 'gru', 'hinge-loss', 'actor-critic', 'one-shot-learning', 'pgm', 'apache-kafka', 'hive', 'stata', 'neo4j', 'caffe', 'parameter', 'infographics', 'estimators', 'manifold', 'data-stream-mining', 'uncertainty', 'dynamic-time-warping', 'esl', 'scala', 'torch', 'amazon-ml', '3d-reconstruction', 'sparse', 'expectation-maximization', 'mongodb', '.net', 'metadata', 'usecase', 'consumerweb', 'alex-net', 'simulation', 'pip', 'encoder', 'feature-reduction', 'normal', 'bart', 'rfe', 'sparsity', 'pickle', 'probabilistic-programming', 'pybrain', 'competitions', 'json', 'spectral-clustering', 'knowledge-base', 'rapidminer', 'sigmoid', 'oversampling', 'anomaly', 'sports', 'manhattan', 'ndcg', 'redshift', 'self-driving', 'finite-precision', 'entity-linking', 'heatmap', 'fuzzy-classification', 'information-extraction', 'spyder', 'objective-function', 'meta-learning', 'universal-approximation-theorem', 'plotly', 'wikipedia', 'vae', 'conformalprediction', 'data-drift', 'test', 'recurrent-neural-network', 'tpu', 'replication', 'feature-interaction', 'git', 'history', 'libsvm', 'smotenc', 'adversarial-ml', 'wasserstein', 'cross-entropy', 'difference', 'spss', 'document-understanding', 'spatial-transformer', 'best-practice', 'pruning', 'library', 'representation', 'privacy', 'tesseract', 'multitask-learning', 'python-polars', 'reproducibility', 'spearmans-rank-correlation', 'bayesian-nonparametric', 'functional-api', 'kalman-filter', 'pearsons-correlation-coefficient', 'torchvision', 'lsi', 'imbalanced-data', 'linearly-separable', 'hashing-trick', 'question-answering', 'pvalue', 'rasa-nlu', 'google-cloud', 'predict', 'glorot-initialization', 'networkx', 'perplexity', 'gmm', 'lemmatization', 'rattle', 'crawling', 'project-planning', 'movielens', 'coursera', 'poisson', 'sagemaker', 'feedback-loop', 'knowledge-distillation', 'kedro', 'stacking', 'pycaret', 'ethical-ai', 'exploratory-factor-analysis', 'tensorboard', 'dirichlet', 'markov', 'hdbscan', 'linear-programming', 'genetic-programming', 'field-aware-factorization-machines', 'time-complexity', 'kendalls-tau-coefficient', 'real-ml-usecase', 'symbolic-learning', 'labeling', 'reward', 'mean-shift', 'image-size', 'notation', 'hog', 'tflearn', 'pacf', 'hashing', 'custom-layer', 'document-term-matrix', 'allennlp', 'logistic', 'rocr-package', 'coherence', 'functions', 'gini-index', 'combinatorics', 'grammar-inference', 'feature-map', 'pcamixdata', 'ibm-watson', 'discounted-reward', 'google-data-studio', 'cs231n', 'odds', 'multi-instance-learning', 'pymc3', 'agglomerative', 'stemming', 'pasting', 'context-vector', 'partial-dependence-plot', 'noisification', 'graphviz', 'prophet', 'few-shot-learning', 'precision-recall-curve', 'boruta', 'classificationmulti', 'data-source', 'share-point', 'conda', 'umap', 'entropy', 'elastic-search', 'facebook', 'intuition', 'taxonomy', 'train', 'caret', 'entity-matching', 'knowledge-canonicalization', 'cost-sensitive-learning', 'permutation-test', 'backtest', 'shadow-deployment', 'flask', 'bar-chart', 'bokeh', 'deepar', 't', 'text-to-columns', 'dictionary', 'duplicate', 'list', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "print(list(inverted_index_tags.keys()))\n",
    "# print(inverted_index_tags[\"optimization\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine (We name it : CobraSearch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to use a probabilistic model because it is well suited to an inverted index. The most used probabilistic model is the BM25 model, However, it supposes that all documents have the same prior relevance. In our case, we want to take into account the non textual metadata of the documents. \\\n",
    "An extensive bibliographic overview has led us to use the following model :\n",
    "Our chosen model is based on the following paper: https://dl.acm.org/doi/10.1561/1500000019 Sections 3.6 and 3.7 \\\n",
    "It is derived from the BM25 model with 3 Streams : title, body and tags, but also taking into account non textual features like the number votes, comments, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(post_id,inverted_index):\n",
    "    # term frequency of the post\n",
    "    # use the variable posts\n",
    "    tf = {}\n",
    "    for word in inverted_index:\n",
    "        if str(post_id) in inverted_index[word]:\n",
    "            tf[word] = inverted_index[word][str(post_id)]/len(posts[posts[\"Id\"] == post_id][\"Body\"].values[0])\n",
    "        else:\n",
    "            tf[word] = 0\n",
    "    return tf\n",
    "\n",
    "def post_to_post_feature(post_id):\n",
    "    post_feature = {}\n",
    "    post_feature[\"post_id\"] = post_id\n",
    "    post_feature[\"tf_body\"] = term_frequency(post_id, inverted_index_body)\n",
    "    post_feature[\"tf_title\"] = term_frequency(post_id, inverted_index_title)\n",
    "    post_feature[\"tf_tags\"] = term_frequency(post_id, inverted_index_tags)\n",
    "    post_feature[\"reputation\"] = get_reputation(post_id)\n",
    "    post_feature[\"votes\"] = get_votes(post_id)\n",
    "    post_feature[\"number_answers\"] = get_number_answers(post_id)\n",
    "    post_feature[\"badges\"] = get_badges_user(post_id)\n",
    "    post_feature[\"answered\"] = get_answered(post_id)\n",
    "    post_feature[\"views\"] = get_views(post_id)\n",
    "    return post_feature\n",
    "\n",
    "\n",
    "\n",
    "def CobraSearch(query):\n",
    "    query_precessed = text_process(query)\n",
    "    # hyperparameters\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    k3 = 1000\n",
    "\n",
    "    # Vi, score function of each feature\n",
    "    lambda_reputation = 1\n",
    "    w_reputation = 1\n",
    "    lambda_votes = 1\n",
    "    w_votes = 1\n",
    "    lambda_nb_answers = 1\n",
    "    w_nb_answers = 1\n",
    "    lambda_badges = 1\n",
    "    w_badges = 1\n",
    "    w_answered = 1\n",
    "    lambda_views = 1\n",
    "    w_views = 1\n",
    "    Vi = {}\n",
    "    for post_id in posts[\"Id\"]:\n",
    "        post_features = post_to_post_feature(post_id)\n",
    "        Vi[post_id] = {\n",
    "            \"reputation\": w_reputation*log10(post_features[\"reputation\"] + lambda_reputation),\n",
    "            \"votes\": w_votes*log10(post_features[\"votes\"] + lambda_votes),\n",
    "            \"number_answers\": w_nb_answers*log10(post_features[\"number_answers\"]+lambda_nb_answers),\n",
    "            \"badges\":w_badges*log10(post_features[\"badges\"]+lambda_badges),\n",
    "            \"answered\":w_answered*post_features[\"answered\"],\n",
    "            \"views\":w_views*log10(post_features[\"badges\"]+lambda_views),\n",
    "        }\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
