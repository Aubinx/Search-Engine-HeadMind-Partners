{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Group 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/himmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "FILEPATH = \"datascience.stackexchange.com\" # path to unzipped data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from math import log10,sqrt\n",
    "import re\n",
    "import tkinter as tk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def extract_data(filepath):\n",
    "    return pd.read_xml(filepath, parser=\"etree\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id  PostTypeId             CreationDate  Score  ViewCount  \\\n",
      "0  115535           1  2022-10-24T12:58:24.757      1       29.0   \n",
      "1  115536           1  2022-10-24T13:45:55.820      0       30.0   \n",
      "2  115537           1  2022-10-24T13:56:40.603      0       12.0   \n",
      "3  115538           2  2022-10-24T14:36:39.480      1        NaN   \n",
      "4  115539           1  2022-10-24T15:22:37.823      0       14.0   \n",
      "\n",
      "                                                Body  OwnerUserId  \\\n",
      "0  [pa, in, advance, if, this, question, is, so, ...      30838.0   \n",
      "1  [pi, just, need, to, check, my, understanding,...      87037.0   \n",
      "2  [pi, am, trying, to, tune, gradient, boost, ca...      64199.0   \n",
      "3  [p, it, is, correct, if, you, compare, neural,...     119140.0   \n",
      "4  [pi, starting, to, study, how, to, rank, words...     141937.0   \n",
      "\n",
      "          LastActivityDate                                              Title  \\\n",
      "0  2022-10-24T12:58:24.757  [information, retrieval, vs, recommendation, s...   \n",
      "1  2022-10-24T14:36:39.480                          [the, idea, behind, drop]   \n",
      "2  2022-10-24T13:56:40.603  [maximize, accuracy, with, differential, evolu...   \n",
      "3  2022-10-24T14:36:39.480                                                 []   \n",
      "4  2022-10-24T15:22:37.823              [help, using, b, to, rank, sentences]   \n",
      "\n",
      "                                                Tags  ...  ClosedDate  \\\n",
      "0    <recommender-system><information-retrieval><ai>  ...        None   \n",
      "1                                          <dropout>  ...        None   \n",
      "2  <machine-learning><r><accuracy><ensemble-learn...  ...        None   \n",
      "3                                               None  ...        None   \n",
      "4  <machine-learning-model><tfidf><information-re...  ...        None   \n",
      "\n",
      "   ContentLicense AcceptedAnswerId LastEditorUserId  LastEditDate  ParentId  \\\n",
      "0    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "1    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "2    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "3    CC BY-SA 4.0              NaN              NaN          None  115536.0   \n",
      "4    CC BY-SA 4.0              NaN              NaN          None       NaN   \n",
      "\n",
      "  OwnerDisplayName  CommunityOwnedDate LastEditorDisplayName FavoriteCount  \n",
      "0             None                None                  None           NaN  \n",
      "1             None                None                  None           NaN  \n",
      "2             None                None                  None           NaN  \n",
      "3             None                None                  None           NaN  \n",
      "4             None                None                  None           NaN  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_posts = extract_data(filepath=FILEPATH+\"/Posts.xml\")\n",
    "users = extract_data(filepath=FILEPATH+\"/Users.xml\") # usefull for user reputation\n",
    "comments = extract_data(filepath=FILEPATH+\"/Comments.xml\") # usefull for user reputation and accepted answer\n",
    "votes = extract_data(filepath=FILEPATH+\"/Votes.xml\") # usefull for post score\n",
    "badges = extract_data(filepath=FILEPATH+\"/Badges.xml\") # usefull for user reputation\n",
    "file = \"posts.pkl\"  # precomputed tokenization in file posts.pkl\n",
    "with open(file, 'rb') as f:\n",
    "    posts = pkl.load(f)\n",
    "\n",
    "print(posts.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(posts,column): # returns inverted index\n",
    "    inverted_index = {}\n",
    "    for i in range(len(posts)):\n",
    "        post_id = str(posts['Id'][i])\n",
    "        for word in posts[column][i]:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = {}\n",
    "            if post_id not in inverted_index[word]:\n",
    "                inverted_index[word][post_id] = 0\n",
    "            inverted_index[word][post_id] = inverted_index[word][post_id] + 1\n",
    "    return inverted_index\n",
    "  \n",
    "inverted_index_body = get_inverted_index(posts,\"Body\")\n",
    "inverted_index_title = get_inverted_index(posts,\"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ain', 'which', 'on', 'there', 'can', 'here', \"it's\", 'yourselves', \"you've\", 'into', 'just', \"haven't\", 'has', 'had', 'isn', 'do', 'ourselves', 'you', 'her', 'the', 'have', 'only', \"isn't\", 'a', 'does', 'if', \"needn't\", 'all', \"aren't\", 'their', 'mightn', \"weren't\", 'too', 'myself', 'of', 'were', 'did', 'hadn', 'as', 'mustn', 'himself', 'be', 'they', 'should', \"mustn't\", 'needn', 'where', 'during', 'this', 'other', 'against', 'them', 'over', 'who', 'with', 'is', 'themselves', 'was', 'again', 'p', 'more', 'under', 't', \"shan't\", 'further', \"mightn't\", 'wasn', 'for', 'through', 'most', 'not', 'from', 'doing', \"hasn't\", 'ma', 'him', 'his', 'then', 'yours', 'its', 'once', 'and', 'will', 've', 'down', 'how', 'at', 'we', 'such', 'no', 'couldn', 'own', \"that'll\", 'me', 'having', 'few', 'those', 'because', 's', 'before', 'shouldn', \"wasn't\", 'been', 'hasn', 'both', 'some', \"you'd\", 'aren', 'these', 'hers', 'in', \"didn't\", 'haven', 'after', 'weren', 'am', 'won', 'are', 'or', 'about', 'an', \"hadn't\", \"couldn't\", 'it', 'below', \"you're\", 'why', 'ours', 'nor', 'so', 'now', 'by', 'he', 'doesn', 'very', 'above', 'y', 'until', 'when', 'd', 'don', 'she', \"she's\", 'whom', \"should've\", 'o', \"doesn't\", \"shouldn't\", 'what', 'same', \"won't\", 'my', 'each', 'shan', 'up', 'to', 'while', 'out', 're', 'yourself', \"don't\", 'didn', 'off', \"you'll\", 'i', 'herself', 'being', 'any', 'm', 'but', 'our', 'theirs', 'itself', 'll', \"wouldn't\", 'that', 'between', 'your', 'wouldn', 'than'}\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "stop_words_nltk = stopwords.words('english') # list of stopwords\n",
    "stop_words = set(stop_words_nltk + [\"p\"])\n",
    "print(stop_words)\n",
    "\n",
    "def remove_stopwords(inverted_index):\n",
    "    for word in stop_words:\n",
    "        if word in inverted_index:\n",
    "            del inverted_index[word]\n",
    "    return inverted_index\n",
    "\n",
    "inverted_index_body = remove_stopwords(inverted_index_body)\n",
    "inverted_index_title = remove_stopwords(inverted_index_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(inverted_index, posts): # returns tf_idf\n",
    "    tf_idf = {}\n",
    "    for word in inverted_index:\n",
    "        tf_idf[word] = {}\n",
    "        for post_id in inverted_index[word]:\n",
    "            tf_idf[word][post_id] = inverted_index[word][post_id] * log10(len(posts)/len(inverted_index[word]))\n",
    "    return tf_idf\n",
    "\n",
    "tf_idf_body = get_tf_idf(inverted_index_body, posts)\n",
    "tf_idf_title = get_tf_idf(inverted_index_title, posts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('116141', 21.661042056227643), ('339', 21.661042056227643), ('109823', 19.380932366098417), ('76841', 15.96076783090458), ('95007', 15.96076783090458)]\n",
      "[('116141', 19), ('339', 19), ('109823', 17), ('76841', 14), ('95007', 14)]\n",
      "[('29542', 3.6589177006247477), ('65063', 3.6589177006247477), ('11404', 3.6589177006247477), ('64403', 3.6589177006247477), ('29057', 3.6589177006247477)]\n",
      "[('29542', 2), ('65063', 2), ('11404', 2), ('64403', 2), ('29057', 2)]\n"
     ]
    }
   ],
   "source": [
    "# posts that contain the word python the most in the body and title\n",
    "print(sorted(tf_idf_body[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(inverted_index_body[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(tf_idf_title[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(inverted_index_title[\"python\"].items(), key=lambda x: x[1], reverse=True)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_inverted_index(inverted_index):\n",
    "    stemmed_inverted_index = {}\n",
    "    for word in inverted_index:\n",
    "        stemmed = stemmer.stem(word) # stem word\n",
    "        if stemmed not in stemmed_inverted_index:\n",
    "            stemmed_inverted_index[stemmed] = {}\n",
    "        for post_id in inverted_index[word]:\n",
    "            if post_id not in stemmed_inverted_index[stemmed]:\n",
    "                stemmed_inverted_index[stemmed][post_id] = 0\n",
    "            stemmed_inverted_index[stemmed][post_id] = stemmed_inverted_index[stemmed][post_id] + inverted_index[word][post_id]\n",
    "    return stemmed_inverted_index\n",
    "\n",
    "stemmed_inverted_index_body = stem_inverted_index(inverted_index_body)\n",
    "stemmed_inverted_index_title = stem_inverted_index(inverted_index_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('115768', 13), ('60039', 11), ('119916', 11), ('20380', 9), ('117097', 8)]\n",
      "[('76321', 54), ('74666', 25), ('115768', 13), ('60039', 11), ('119916', 11)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(inverted_index_body[\"open\"].items(), key=lambda x: x[1], reverse=True)[0:5])\n",
    "print(sorted(stemmed_inverted_index_body[\"open\"].items(), key=lambda x: x[1], reverse=True)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 19:42:33.939909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 19:42:34.299818: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-02 19:42:34.299839: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-02 19:42:35.654754: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 19:42:35.655044: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 19:42:35.655066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inform', 'retriev']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "def text_process(query):\n",
    "    # tokenize query (or text) with bert-case-uncased tokenizer\n",
    "    # keep only alpha words\n",
    "    # remove stopwords\n",
    "    # stem query\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "    query = tokenizer.tokenize(query)\n",
    "    query = [word for word in query if word.isalpha()]\n",
    "    query = [word for word in query if word not in stop_words]\n",
    "    query = [stemmer.stem(word) for word in query]\n",
    "    return query\n",
    "\n",
    "query = \"information retrieval\"\n",
    "query = text_process(query)\n",
    "print(query) # ['error', 'open', 'file', 'python']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting relevant metadata from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(post_id):\n",
    "    taglist = posts[posts[\"Id\"] == post_id][\"Tags\"]\n",
    "    if taglist.values[0] is None: return []\n",
    "    taglist = taglist.values[0].replace(\"<\", \"\").replace(\">\", \" \").split(\" \")\n",
    "    return [tag.replace(\"-\", \" \") for tag in taglist]\n",
    "\n",
    "def get_all_tags():\n",
    "    all_tags = []\n",
    "    for i in range(len(posts)):\n",
    "        all_tags = all_tags + get_tags(posts[\"Id\"][i])\n",
    "    return list(set(all_tags))\n",
    "\n",
    "# all_tags = get_all_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60888.0\n"
     ]
    }
   ],
   "source": [
    "def get_body(posts, post_id):\n",
    "    # tokenized body of the post\n",
    "    body = posts[posts[\"Id\"] == post_id][\"Body\"].values[0]\n",
    "    return body\n",
    "\n",
    "def get_title(posts, post_id):\n",
    "    # tokenized title of the post\n",
    "    title = posts[posts[\"Id\"] == post_id][\"Title\"].values[0]\n",
    "    return title\n",
    "\n",
    "def get_tags_from_postid(post_id):\n",
    "    taglist = posts[posts[\"Id\"] == post_id][\"Tags\"]\n",
    "    if taglist.values[0] is None: return []\n",
    "    taglist = taglist.values[0].replace(\"<\", \"\").replace(\">\", \" \").split(\" \")\n",
    "    # replace \"-\"\" with \" \"\n",
    "    for i in range(len(taglist)):\n",
    "        taglist[i] = text_process(taglist[i].replace(\"-\", \" \"))\n",
    "    return taglist[:-1]\n",
    "\n",
    "def get_raw_tags_from_postid(post_id):\n",
    "    taglist = posts[posts[\"Id\"] == post_id][\"Tags\"]\n",
    "    if taglist.values[0] is None: return []\n",
    "    taglist = taglist.values[0].replace(\"<\", \"\").replace(\">\", \" \").split(\" \")\n",
    "    return taglist[:-1]\n",
    "\n",
    "# print(get_tags_from_postid(115768))\n",
    "\n",
    "def get_reputation(post_id):\n",
    "    # reputation of the user who posted the post\n",
    "    owner_user_id = posts[posts[\"Id\"] == post_id][\"OwnerUserId\"].values[0]\n",
    "    if pd.isna(owner_user_id):\n",
    "        return 0\n",
    "    reputation = users[users[\"Id\"] == int(owner_user_id)][\"Reputation\"].values[0]\n",
    "    return reputation\n",
    "\n",
    "\n",
    "# print(get_reputation(posts, 115768))\n",
    "def get_inverted_index_tags(posts,tag_to_processed_tag_dict):\n",
    "    # Inverted index of tags\n",
    "    inverted_index_tags = {}\n",
    "    for i in range(len(posts)):\n",
    "        post_id = posts[\"Id\"][i]\n",
    "        taglist = [tag_to_processed_tag_dict[tag] for tag in get_raw_tags_from_postid(post_id)]\n",
    "        taglist = [t for tag in taglist for t in tag]\n",
    "        for tag in taglist:\n",
    "            if tag not in inverted_index_tags:\n",
    "                inverted_index_tags[tag] = {}\n",
    "            if post_id not in inverted_index_tags[tag]:\n",
    "                inverted_index_tags[tag][post_id] = 0\n",
    "            inverted_index_tags[tag][post_id] = inverted_index_tags[tag][post_id] + 1\n",
    "    return inverted_index_tags\n",
    "\n",
    "\n",
    "def get_votes(post_id):\n",
    "    # number of votes of the post\n",
    "    nb_votes = posts[posts[\"Id\"] == post_id][\"Score\"].values[0]\n",
    "    return nb_votes\n",
    "\n",
    "def get_number_answers(post_id):\n",
    "    # number of answers of the post\n",
    "    nb_answers = posts[posts[\"ParentId\"] == post_id].shape[0]\n",
    "    return nb_answers\n",
    "\n",
    "def get_badges_user(post_id):\n",
    "    # number of badges of the user who posted the post\n",
    "    # use the variable badges and users\n",
    "    owner_user_id = posts[posts[\"Id\"] == post_id][\"OwnerUserId\"].values[0]\n",
    "    if pd.isna(owner_user_id):\n",
    "        return 0\n",
    "    nb_badges = badges[badges[\"UserId\"] == int(posts[posts[\"Id\"] == post_id][\"OwnerUserId\"])][\"Class\"].shape[0]\n",
    "    return nb_badges\n",
    "\n",
    "def get_answered(post_id):\n",
    "    # 1 if the post is answered, 0 otherwise\n",
    "    # use the variable comments\n",
    "    if comments[comments[\"PostId\"] == int(post_id)].shape[0] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_views(post_id):\n",
    "    # number of views of the post\n",
    "    # use the variable posts\n",
    "    nb_views = posts[posts[\"Id\"] == post_id][\"ViewCount\"].values[0]\n",
    "    return nb_views\n",
    "\n",
    "# print(posts.head())\n",
    "\n",
    "print(get_views(12761))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests\n",
    "#print(get_body(posts,123)) OK\n",
    "#print(get_title(posts,5)) OK\n",
    "#print(get_tags_from_postid(5)) OK\n",
    "#print(get_raw_tags_from_postid(5)) OK\n",
    "#print(get_reputation(5)) OK\n",
    "#print(get_votes(5)) OK\n",
    "# get_reputation(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "# tag_to_processed_tag_dict ={}\n",
    "# for tag in tqdm(all_tags):\n",
    "#     tag_to_processed_tag_dict[tag] = text_process(tag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save tag_to_processed_tag_dict\n",
    "# import pickle\n",
    "# with open('tag_to_processed_tag_dict.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tag_to_processed_tag_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)$\n",
    "# load tag_to_processed_tag_dict\n",
    "import pickle\n",
    "with open('tag_to_processed_tag_dict.pickle', 'rb') as handle:\n",
    "    tag_to_processed_tag_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'regex', 'nosql', 'validation', 'odds', 'rocr-package', 'mlops', 'bayesian', 'rasa-nlu', 'perplexity', 'data-source', 'real-ml-usecase', 'randomized-algorithms', 'sequential-pattern-mining', 'google-prediction-api', 'feature-scaling', 'noise', 'colab', 'marketing', 'survival-analysis', 'python-polars', 'markov', 'bootstraping', 'uncertainty', 'jaccard-coefficient', 'fasttext', 'ibm-watson', 'cross-entropy', 'json', 'state-of-the-art', 'training', 'ai', 'ngrams', 'machine-translation', 'multitask-learning', 'probability-calibration', 'api', 'embeddings', 'web-scraping', 'octave', 'apache-kafka', 'exploratory-factor-analysis', 'normalization', 'mse', 'csv', 'catboost', 'stacked-lstm', 'bigdata', 'vgg16', 'anonymization', 'hyperparameter', 'caret', 'openai-gpt', 'flask', 'methodology', 'visualization', 'gradient', 'elastic-search', 'scoring', 'finite-precision', 'kernel', 'consumerweb', 'knowledge-base', 'nlp', 'bioinformatics', 'spatial-transformer', 'stacking', 'pandas', 'apache-pig', 'definitions', 'cost-function', 'inceptionresnetv2', 'metaheuristics', 'tools', 'collinearity', 'libsvm', 'keras', 'conda', 'tfidf', 'binary-classification', 'rbf', 'missing-data', 'grid-search', 'sagemaker', 'knowledge-distillation', 'numerical', 'finetuning', 'labels', 'cs231n', 'hypothesis-testing', 'chatbot', 'audio-recognition', 'svr', 'mxnet', 'mlflow', 'bert', 'pearsons-correlation-coefficient', 'transfer-learning', 'self-study', 'reward', 'rstudio', 'association-rules', 'pvalue', 'learning-rate', 'gini-index', 'sentiment-analysis', 'data-augmentation', 'pooling', 'search-engine', 'gaussian-process', 'algorithms', 'dplyr', 'manhattan', 'counts', 'social-network-analysis', 'dynamic-programming', 'epochs', 'analysis', 'least-squares-svm', 'information-extraction', 'text-classification', 'hpc', 'multi-instance-learning', 'hbase', 'feature-engineering', 'hierarchical-data-format', 'ensemble-learning', 'ngboost', 'goodness-of-fit', 'bart', 'multi-output', 'data-leakage', 'education', 'markov-process', 'list', 'mlp', 'ocr', 'unsupervised-learning', 'activity-recognition', 'vector-space-models', 'linear-regression', 'momentum', 'gmm', 'document-term-matrix', 'spyder', 'feature-selection', 'manifold', 'community', 'time-series', 'hinge-loss', 'convolutional-neural-network', 'sampling', 'image-preprocessing', 'taxonomy', 'crawling', 'outlier', 'dropout', 'beginner', 'learning', 'one-shot-learning', 'retraining', 'feature-interaction', 'conformalprediction', 'cross-validation', 'agglomerative', 'neo4j', 'infographics', 'nlg', 'labeling', 'policy-gradients', 'map-reduce', 'mean', 'heatmap', 'hashingvectorizer', 'functional-api', 'text-generation', 'probability', 'serialisation', 'early-stopping', 'softmax', 'reference-request', 'javascript', 'question-answering', 'deployment', 'ethical-ai', 'domain-adaptation', 'hardware', 'naive-bayes-classifier', 'cloud-computing', 'python', 'generative-models', 'auc', 't', 'k-means', 'shap', 'density-estimation', 'tsne', 'data-stream-mining', 'git', 'error-handling', 'alex-net', 'simulation', 'learning-to-rank', 'prophet', 'distributed', 'faster-rcnn', 'hyperparameter-tuning', 'sigmoid', 'replication', 'research', 'deepmind', 'kalman-filter', 'derivation', 'boosting', 'coherence', 'automatic-summarization', 'actor-critic', 'feature-extraction', 'feature-importances', 'federated-learning', 'predictor-importance', 'rfe', 'time', 'monte-carlo', 'windows', 'anomaly-detection', 'reshape', 'google-cloud-platform', 'confidence', 'entropy', 'privacy', 'c++', 'reinforcement-learning', 'redshift', 'scipy', 'rnn', 'structural-equation-modelling', 'programming', 'rbm', 'logistic', 'torch', 'categorical-data', 'finance', 'stemming', 'text-to-columns', 'semantic-segmentation', 'anaconda', 'parameter', 'natural-gradient-boosting', 'speech-to-text', 'mongodb', 'normal', 'interpretation', 'cost-sensitive-learning', 'online-learning', 'keras-rl', 'loss-function', 'concept-drift', 'reproducibility', 'dummy-variables', 'version-control', 'mean-shift', 'arima', 'mutual-information', 'feature-construction', 'encoder', 'combinatorics', 'nvidia', 'imbalanced-learn', 'tpu', 'fastai', 'library', 'competitions', 'named-entity-recognition', 'r-squared', 'esl', 'one-class-classification', 'pytorch-geometric', 'adversarial-ml', 'dqn', 'coursera', 'information-theory', 'pgm', 'intuition', 'naive-bayes-algorithim', 'target-encoding', 'pytorch', 'memory', 'knime', 'regularization', 'weka', 'xgboost', 'pruning', 'image-classification', 'facebook', 'sequence-to-sequence', 'mini-batch-gradient-descent', 'ridge-regression', 'mcmc', 'image', 'efficiency', 'causalimpact', 'google-cloud', 'kendalls-tau-coefficient', 'bayes-error', 'gan', 'noisification', 'pip', 'semantic-similarity', 'linear-models', 'imbalanced-data', 'overfitting', 'evolutionary-algorithms', 'binary', 'inception', 'discriminant-analysis', 'networkx', 'gradient-descent', 'descriptive-statistics', 'apache-spark', 'stata', 'processing', 'self-driving', 'computer-vision', 'text', 'test', 'sparse', 'evaluation', 'universal-approximation-theorem', 'ensemble-modeling', 'apache-mahout', 'gpu', 'data-table', 'mnist', 'code', 'semi-supervised-learning', 'share-point', 'dbscan', 'pcamixdata', 'project-planning', 'score', 'cuda', 'ggplot2', 'mathematics', 'text-processing', 'vae', 'automl', 'meta-learning', 'data-science-model', 'matplotlib', 'software-recommendation', 'activation-function', 'entity-matching', 'partial-least-squares', 'kaggle', 'regression', 'neural-network', 'classificationmulti', 'cause-and-effect', 'normal-equation', 'gradient-boosting-decision-trees', 'ann', 'usecase', 'symbolic-learning', 'kedro', 'custom-layer', 'precision-recall-curve', 'interpolation', 'genetic-algorithms', 'forecasting', 'bag-of-words', 'career', 'few-shot-learning', 'object-recognition', 'non-parametric', 'pasting', 'lightgbm', 'cloud', 'pymc3', 'text-mining', 'software-development', 'data-indexing-techniques', 'train', 'siamese-networks', 'variance', 'word-embeddings', 'predict', 'features', 'data', 'data-cleaning', 'clustering', 'classification', 'lsi', 'structured-data', 'lstm', 'apache-hadoop', 'pattern-recognition', 'search', 'dictionary', 'encoding', 'smote', 'similar-documents', 'openai-gym', 'estimation', 'bagging', 'rattle', 'best-practice', 'hashing-trick', 'methods', 'predictive-modeling', 'smotenc', 'one-hot-encoding', 'data-formats', 'google-bigquery', 'doc2vec', 'sensors', 'boruta', 'open-source', 'pacf', 'prediction', 'distance', 'data-wrangling', 'lime', 'random-forest', 'elastic-net', 'implementation', 'parallel', 'google', 'fuzzy-classification', 'homework', 'statistics', 'model-selection', 'probabilistic-programming', 'accuracy', 'market-basket-analysis', 'pycaret', 'mysql', 'bayesian-nonparametric', 'parameter-estimation', 'azure-ml', 'java', 'svm', 'oversampling', 'language-model', 'recommender-system', 'matrix', 'hive', 'difference', 'poisson', 'indexing', 'transformation', 'cart', 'numpy', 'h2o', 'management', 'discounted-reward', 'excel', 'masking', 'multivariate-distribution', 'data-mining', 'sequence', 'allennlp', 'featurization', 'chi-square-test', 'relational-dbms', 'logarithmic', 'cosine-distance', 'theano', 'object-detection', 'google-data-studio', 'nltk', 'linux', 'corpus', 'pybrain', 'gridsearchcv', 'scikit-learn', 'grammar-inference', 'jupyter', 'functions', 'dataset', 'partial-dependence-plot', 'glorot-initialization', 'histogram', 'sports', 'topic-model', 'class-imbalance', 'feature-reduction', 'multiclass-classification', 'neural-style-transfer', 'c', 'gnn', 'markov-hidden-model', 'anomaly', 'linear-programming', 'scalability', 'pyspark', 'seaborn', 'notation', 'lasso', 'plotly', 'metric', 'experiments', 'churn', 'data-engineering', 'image-segmentation', 'objective-function', 'wasserstein', 'tableau', 'cyclegan', 'supervised-learning', 'history', 'categorical-encoding', 'data-analysis', 'geospatial', 'gbm', 'r', 'forecast', 'k-nn', 'convolution', '3d-reconstruction', 'machine-learning-model', 'spacy', 'bias', 'f1score', 'performance', 'distribution', 'fourier', 'weight-initialization', 'ab-test', 'perceptron', 'knowledge-graph', 'hdbscan', 'loss', 'context-vector', 'word2vec', 'lda', 'shadow-deployment', 'sgd', 'batch-normalization', 'data-quality', 'feature-map', 'bar-chart', 'huggingface', 'text-filter', 'bokeh', 'image-size', 'metadata', 'amazon-ml', 'wolfram-language', '3d-object-detection', 'game', 'caffe', 'explainable-ai', 'sas', 'matlab', 'dirichlet', 'spearmans-rank-correlation', 'backtest', 'python-3.x', 'yolo', 'decision-trees', 'graph-neural-network', 'entity-linking', 'active-learning', 'recurrent-neural-network', 'databases', 'annotation', 'rmse', 'ndcg', 'powerbi', 'similarity', 'spectral-clustering', 'multilabel-classification', 'convergence', 'aws', 'pipelines', 'groupby', 'image-recognition', 'tokenization', 'correlation', 'parsing', 'scala', 'document-understanding', 'pickle', 'pretraining', 'pca', 'confusion-matrix', 'aggregation', 'tensorflow', 'glm', 'time-complexity', 'field-aware-factorization-machines', 'backpropagation', 'ipython', 'weighted-data', 'etl', 'wikipedia', 'tensorboard', 'matrix-factorisation', 'tflearn', 'stanford-nlp', 'movielens', 'twitter', 'ranking', 'statsmodels', 'dataframe', 'preprocessing', 'fuzzy-logic', 'opencv', 'genetic', 'scraping', 'gaussian', 'duplicate', 'linearly-separable', 'gensim', 'imbalance', 'graphs', 'classifier', 'representation', 'hog', 'lda-classifier', 'sql', 'isolation-forest', 'orange', 'rapidminer', 'cnn', 'attention-mechanism', '.net', 'knowledge-canonicalization', 'julia', 'tranformation', 'labelling', 'graphical-model', 'terminology', 'word', 'gru', 'spss', 'automation', 'umap', 'linear-algebra', 'generalization', 'estimators', 'theory', 'roc', 'optimization', 'information-retrieval', 'logistic-regression', 'tesseract', 'q-learning', 'data-imputation', 'inference', 'graphviz', 'deep-learning', 'plotting', 'lemmatization', 'dimensionality-reduction', 'data-drift', 'aws-lambda', 'freebase', 'adaboost', 'ensemble', 'anova', 'hashing', 'feedback-loop', 'deepar', 'books', 'expectation-maximization', 'model-evaluations', 'vc-theory', 'permutation-test', 'dynamic-time-warping', 'bayesian-networks', 'autoencoder', 'machine-learning', 'pac-learning', 'sparsity', 'transformer', 'data-product', 'torchvision', 'genetic-programming']\n"
     ]
    }
   ],
   "source": [
    "print(list(tag_to_processed_tag_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted_index_tags = get_inverted_index_tags(posts,tag_to_processed_tag_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # save inverted index tags as pkl\n",
    "# with open('inverted_index_tags.pkl', 'wb') as f:\n",
    "#     pickle.dump(inverted_index_tags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inverted index tags from pkl\n",
    "with open('inverted_index_tags.pkl', 'rb') as f:\n",
    "    inverted_index_tags = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recommend', 'inform', 'ai', 'drop', 'machin', 'r', 'accuraci', 'ensembl', 'meta', 'rank', 'learn', 'classif', 'nl', 'multi', 'reinforc', 'polici', 'tabl', 'rs', 'binari', 'big', 'apach', 'map', 'deep', 'neural', 'con', 'ml', 'python', 'time', 'chu', 'random', 'supervis', 'comput', 'languag', 'open', 'imag', 'transform', 'hug', 'pre', 'data', 'web', 'predict', 'forecast', 'panda', 'prep', 'featur', 'l', 'loss', 'optim', 'statist', 'count', 'normal', 'sql', 'databas', 'object', 'grid', 'isol', 'excel', 'semant', 'fourier', 'nu', 'mat', 'sampl', 'ke', 'cnn', 'spa', 'sv', 'regress', 'algorithm', 'text', 'graph', 'sci', 'faster', 'commun', 'pc', 'deploy', 'dimension', 'model', 'train', 'distribut', 'tensor', 'gp', 'visual', 'x', 'hyper', 'hardwar', 'stanford', 'cluster', 'anomali', 'correl', 'un', 'encod', 'dummi', 'code', 'mathemat', 'class', 'evalu', 'kernel', 'ju', 'bert', 'semi', 'c', 'java', 'wolf', 'confus', 'soft', 'fine', 'pipelin', 'token', 'hierarch', 'autom', 'km', 'linear', 'implement', 'ann', 'one', 'sg', 'log', 'decis', 'softwar', 'rm', 'transfer', 'descript', 'cross', 'valid', 'self', 'bia', 'batch', 'estim', 'label', 'g', 'rn', 'bag', 'ari', 'naiv', 'miss', 'sha', 'topic', 'ld', 'em', 'word', 'gradient', 'cat', 'caus', 'confid', 'fed', 'name', 'lass', 'q', 'perform', 'kn', 'varianc', 'ga', 'financ', 'no', 'sequenc', 'distanc', 'genera', 'momentum', 'im', 'market', 'chi', 'structur', 'sentiment', 'auto', 'gan', 'process', 'probabl', 'elast', 'ms', 'siam', 'refer', 'search', 'genet', 'least', 'mn', 'pool', 'sequenti', 'pattern', 'gen', 'orang', 'dq', 'partial', 'ja', 'cart', 'ka', 'score', 'bio', 'googl', 'back', 'associ', 'marko', 'ng', 'epoch', 'weight', 'similar', 'vector', 'metric', 'regular', 'error', 're', 'begin', 'theori', 'domain', 'cs', 'terminolog', 'definit', 'activ', 'chat', 'reg', 'game', 'per', 'book', 'plot', 'sea', 'ts', 'rb', 'twitter', 'cost', 'scala', 'experi', 'ab', 'gb', 'parallel', 'disc', 'ip', 'fuzzi', 'social', 'light', 'nois', 'incept', 'cola', 'homework', 'audio', 'aggreg', 'scrape', 'sa', 'paramet', 'educ', 'career', 'sm', 'surviv', 'linux', 'infer', 'interpret', 'lime', 'cycl', 'mutual', 'au', 'roc', 'geo', 'octav', 'matrix', 'mont', 'par', 'attent', 'effici', 'causal', 'power', 'cu', 'speech', 'mini', 'hypothesi', 'h', 'densiti', 'target', 'numer', 'fast', 'index', 'knowledg', 'explain', 'tool', 'free', 'hash', 'bay', 'co', 'manag', 'onlin', 'gener', 'aw', 'relat', 'stat', 'boost', 'non', 'sensor', 'cloud', 'azur', 'v', 'methodolog', 'yo', 'n', 'state', 'db', 'et', 'hp', 'graphic', 'boot', 'research', 'inter', 'tran', 'stack', 'good', 'api', 'converg', 'memori', 'serial', 'mx', 'automat', 'mean', 'ada', 'corpu', 'evolutionari', 'pac', 'ana', 'dynam', 'vc', 'julia', 'ridg', 'method', 'program', 'window', 'doc', 'mask', 'group', 'concept', 'feat', 'mc', 'earli', 'version', 'collin', 'natur', 'deriv', 'gr', 'hi', 'actor', 'pg', 'hive', 'neo', 'caf', 'info', 'est', 'manifold', 'uncertainti', 'es', 'torch', 'amazon', 'spars', 'expect', 'mon', 'net', 'metadata', 'use', 'consum', 'alex', 'simul', 'pip', 'en', 'bart', 'rf', 'pick', 'pro', 'competit', 'j', 'spectral', 'rapid', 'si', 'over', 'sport', 'manhattan', 'red', 'finit', 'entiti', 'heat', 'spi', 'metal', 'univers', 'wikipedia', 'va', 'conform', 'test', 'rec', 'replic', 'gi', 'histori', 'li', 'ad', 'differ', 'sp', 'document', 'spatial', 'best', 'pr', 'librari', 'represent', 'privaci', 'tess', 'rep', 'spear', 'function', 'pearson', 'question', 'pv', 'ra', 'network', 'gm', 'le', 'rattl', 'crawl', 'project', 'movi', 'cours', 'po', 'sage', 'feedback', 'ked', 'ethic', 'ex', 'dir', 'hd', 'field', 'kendal', 'realm', 'symbol', 'reward', 'notat', 'hog', 'custom', 'allen', 'gin', 'comb', 'grammar', 'ibm', 'discount', 'odd', 'ag', 'stem', 'past', 'context', 'prophet', 'precis', 'bo', 'share', 'um', 'entropi', 'facebook', 'intuit', 'taxonomi', 'care', 'shadow', 'fl', 'bar', 'dictionari', 'duplic', 'list', 'analysi']\n"
     ]
    }
   ],
   "source": [
    "print(list(inverted_index_tags.keys()))\n",
    "# print(inverted_index_tags[\"optimization\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engine (We name it : CobraSearch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to use a probabilistic model because it is well suited to an inverted index. The most used probabilistic model is the BM25 model, However, it supposes that all documents have the same prior relevance. In our case, we want to take into account the non textual metadata of the documents. \\\n",
    "An extensive bibliographic overview has led us to use the following model :\n",
    "Our chosen model is based on the following paper: https://dl.acm.org/doi/10.1561/1500000019 Sections 3.6 and 3.7 \\\n",
    "It is derived from the BM25 model with 3 Streams : title, body and tags, but also taking into account non textual features like the number votes, comments, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "log10_values = [0] # precompute log10 values\n",
    "for i in range(1,100000):\n",
    "    log10_values.append(np.log10(i))\n",
    "\n",
    "def post_to_post_feature(post_id):\n",
    "    post_feature = {}\n",
    "    post_feature[\"post_id\"] = post_id\n",
    "    # post_feature[\"tf_body\"] = term_frequency(post_id, inverted_index_body)\n",
    "    # post_feature[\"tf_title\"] = term_frequency(post_id, inverted_index_title)\n",
    "    # post_feature[\"tf_tags\"] = term_frequency(post_id, inverted_index_tags)\n",
    "    post_feature[\"reputation\"] = get_reputation(post_id)\n",
    "    post_feature[\"votes\"] = get_votes(post_id)\n",
    "    post_feature[\"number_answers\"] = get_number_answers(post_id)\n",
    "    post_feature[\"badges\"] = get_badges_user(post_id)\n",
    "    post_feature[\"answered\"] = get_answered(post_id)\n",
    "    post_feature[\"views\"] = get_views(post_id)\n",
    "    return post_feature\n",
    "\n",
    "def precompute_features_score(post_id):\n",
    "    lambda_reputation = 1\n",
    "    w_reputation = 5\n",
    "    lambda_votes = 0\n",
    "    w_votes = 7\n",
    "    lambda_nb_answers = 1\n",
    "    w_nb_answers = 2\n",
    "    lambda_badges = 1\n",
    "    w_badges = 7\n",
    "    w_answered = 1\n",
    "    lambda_views = 1\n",
    "    w_views = 4\n",
    "    Vi = {}\n",
    "    def even_log(x,lambda_votes):\n",
    "        if x >=0:\n",
    "            return log10_values[x+lambda_votes]\n",
    "        elif x <0:\n",
    "            return -log10_values[-x+lambda_votes]\n",
    "    post_features = post_to_post_feature(post_id)\n",
    "    Vi[post_id] = {\n",
    "        \"reputation\": w_reputation*log10_values[post_features[\"reputation\"] + lambda_reputation],\n",
    "        \"votes\": w_votes*even_log(post_features[\"votes\"], lambda_votes),\n",
    "        \"number_answers\": w_nb_answers*log10_values[post_features[\"number_answers\"]+lambda_nb_answers],\n",
    "        \"badges\":w_badges*log10_values[post_features[\"badges\"]+lambda_badges],\n",
    "        \"answered\":w_answered*post_features[\"answered\"],\n",
    "        \"views\":w_views*log10_values[post_features[\"badges\"]+lambda_views],\n",
    "    }\n",
    "    return sum([Vi[post_id][feature] for feature in Vi[post_id]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi_sums = {}\n",
    "# for post_id in tqdm(posts[\"Id\"]):\n",
    "#     Vi_sums[post_id] = precompute_features_score(post_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save vi_sums to file\n",
    "# with open('vi_sums.pkl', 'wb') as f:\n",
    "#     pickle.dump(Vi_sums, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vi_sums.pkl', 'rb') as f:\n",
    "    Vi_sums = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.000e+00, 4.000e+00, 8.500e+01, 7.500e+01, 1.381e+03, 4.610e+02,\n",
       "        6.780e+02, 2.800e+02, 2.800e+01, 5.200e+02, 5.900e+02, 2.130e+02,\n",
       "        3.980e+02, 2.700e+02, 1.136e+03, 6.620e+02, 9.910e+02, 1.058e+03,\n",
       "        6.100e+02, 1.303e+03, 9.250e+02, 1.326e+03, 1.418e+03, 1.159e+03,\n",
       "        1.511e+03, 1.818e+03, 1.533e+03, 1.903e+03, 1.925e+03, 1.922e+03,\n",
       "        1.937e+03, 1.849e+03, 1.831e+03, 1.670e+03, 1.755e+03, 1.652e+03,\n",
       "        1.520e+03, 1.654e+03, 1.615e+03, 1.498e+03, 1.435e+03, 1.489e+03,\n",
       "        1.469e+03, 1.514e+03, 1.402e+03, 1.433e+03, 1.269e+03, 1.344e+03,\n",
       "        1.126e+03, 1.156e+03, 1.325e+03, 8.510e+02, 1.030e+03, 1.271e+03,\n",
       "        1.132e+03, 8.070e+02, 9.050e+02, 1.035e+03, 7.610e+02, 9.300e+02,\n",
       "        1.339e+03, 1.085e+03, 6.000e+02, 6.040e+02, 6.390e+02, 1.431e+03,\n",
       "        6.970e+02, 3.280e+02, 3.370e+02, 3.310e+02, 2.750e+02, 1.810e+02,\n",
       "        1.690e+02, 1.820e+02, 7.500e+01, 1.110e+02, 8.100e+01, 6.800e+01,\n",
       "        5.600e+01, 4.400e+01, 3.100e+01, 3.000e+01, 2.100e+01, 9.000e+00,\n",
       "        1.200e+01, 9.000e+00, 3.000e+00, 7.000e+00, 7.000e+00, 1.000e+00,\n",
       "        4.000e+00, 2.000e+00, 1.000e+00, 2.000e+00, 0.000e+00, 0.000e+00,\n",
       "        1.000e+00, 0.000e+00, 1.000e+00, 1.000e+00]),\n",
       " array([-1.8346988 , -1.12920838, -0.42371796,  0.28177246,  0.98726288,\n",
       "         1.69275331,  2.39824373,  3.10373415,  3.80922457,  4.51471499,\n",
       "         5.22020542,  5.92569584,  6.63118626,  7.33667668,  8.0421671 ,\n",
       "         8.74765753,  9.45314795, 10.15863837, 10.86412879, 11.56961921,\n",
       "        12.27510964, 12.98060006, 13.68609048, 14.3915809 , 15.09707132,\n",
       "        15.80256175, 16.50805217, 17.21354259, 17.91903301, 18.62452343,\n",
       "        19.33001386, 20.03550428, 20.7409947 , 21.44648512, 22.15197554,\n",
       "        22.85746597, 23.56295639, 24.26844681, 24.97393723, 25.67942765,\n",
       "        26.38491808, 27.0904085 , 27.79589892, 28.50138934, 29.20687976,\n",
       "        29.91237019, 30.61786061, 31.32335103, 32.02884145, 32.73433187,\n",
       "        33.4398223 , 34.14531272, 34.85080314, 35.55629356, 36.26178398,\n",
       "        36.96727441, 37.67276483, 38.37825525, 39.08374567, 39.78923609,\n",
       "        40.49472652, 41.20021694, 41.90570736, 42.61119778, 43.3166882 ,\n",
       "        44.02217863, 44.72766905, 45.43315947, 46.13864989, 46.84414031,\n",
       "        47.54963074, 48.25512116, 48.96061158, 49.666102  , 50.37159242,\n",
       "        51.07708285, 51.78257327, 52.48806369, 53.19355411, 53.89904453,\n",
       "        54.60453496, 55.31002538, 56.0155158 , 56.72100622, 57.42649664,\n",
       "        58.13198707, 58.83747749, 59.54296791, 60.24845833, 60.95394875,\n",
       "        61.65943918, 62.3649296 , 63.07042002, 63.77591044, 64.48140086,\n",
       "        65.18689129, 65.89238171, 66.59787213, 67.30336255, 68.00885297,\n",
       "        68.7143434 ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histogram of Vi_sums\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(list(Vi_sums.values()), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(post_id,inverted_index):\n",
    "    # term frequency of the post\n",
    "    # use the variable posts\n",
    "    tf = {}\n",
    "    for word in inverted_index:\n",
    "        if str(post_id) in inverted_index[word]:\n",
    "            tf[word] = inverted_index[word][str(post_id)]/len(posts[posts[\"Id\"] == post_id][\"Body\"].values[0])\n",
    "        else:\n",
    "            tf[word] = 0\n",
    "    return tf\n",
    "\n",
    "def prefilter_posts(query_processed, posts):\n",
    "    # keep only posts that contain all words of the query in the body, title or tags\n",
    "    # use the variable posts\n",
    "    print(\"prefiltering posts...\")\n",
    "    count_words = {}\n",
    "    for query_word in query_processed:\n",
    "        for post_id in posts[\"Id\"]:\n",
    "            post_id = str(post_id)\n",
    "            if (query_word in inverted_index_body and post_id in inverted_index_body[query_word]) or (query_word in inverted_index_title and post_id in inverted_index_title[query_word]) or (query_word in inverted_index_tags and post_id in inverted_index_tags[query_word]):\n",
    "                if post_id in count_words:\n",
    "                    count_words[post_id] += 1\n",
    "                else:\n",
    "                    count_words[post_id] = 1\n",
    "    # get max value in count_words\n",
    "    max_value = max(count_words.values())\n",
    "    # get all post_ids that have max_value\n",
    "    return [int(post_id) for post_id in count_words if count_words[post_id] == max_value]\n",
    "def CobraSearch(query,posts=posts,top=10,verbose=False):\n",
    "    query_precessed = text_process(query)\n",
    "    if(verbose):\n",
    "        print(\"query processed: \",query_precessed)\n",
    "    post_ids_filtered = prefilter_posts(query_precessed,posts)\n",
    "    if(verbose):\n",
    "        print(\"number of posts\",len(post_ids_filtered))\n",
    "    # hyperparameters\n",
    "    k1 = 1.2\n",
    "    b = 0.75\n",
    "    k3 = 1000\n",
    "\n",
    "    # Vi, score function of each feature\n",
    "    if verbose:\n",
    "        print(\"Vi computed\")\n",
    "    W_scores = {}\n",
    "    RSV_scores={}\n",
    "    w_tf_body = 1\n",
    "    w_tf_title = 5\n",
    "    w_tf_tags = 10\n",
    "    w_Ld_body = 1\n",
    "    w_Ld_title = 3\n",
    "    w_Ld_tags = 10\n",
    "    def get_Ld(post_id):\n",
    "        Ld_body = len(posts[posts[\"Id\"] == post_id][\"Body\"].values[0])\n",
    "        Ld_title = len(posts[posts[\"Id\"] == post_id][\"Title\"].values[0])\n",
    "        Ld_tags = len(get_raw_tags_from_postid(post_id))\n",
    "        return Ld_body*w_Ld_body + Ld_title*w_Ld_title + Ld_tags*w_Ld_tags\n",
    "    # avearge Ld over all post_id\n",
    "    print()\n",
    "    m = np.mean([get_Ld(post_id) for post_id in post_ids_filtered])\n",
    "\n",
    "    for post_id in tqdm(post_ids_filtered):\n",
    "\n",
    "        term_freq_body = term_frequency(post_id, inverted_index_body)\n",
    "        term_freq_title = term_frequency(post_id, inverted_index_title)\n",
    "        term_freq_tags = term_frequency(post_id, inverted_index_tags)\n",
    "        tf = lambda word: w_tf_body*term_freq_body[word] + w_tf_title*term_freq_title[word] + w_tf_tags*term_freq_tags[word]\n",
    "        Ld = get_Ld(post_id)\n",
    "        W_scores[str(post_id)] = 4000*sum([(k1+1)*tf(word)/(k1*((1-b)+b*Ld/m)+tf(word))*(k3+1)*tf(word)/(k3+tf(word))\n",
    "                               for word in query_precessed if word in inverted_index_body and word in inverted_index_title and word in inverted_index_tags]\n",
    "                               )\n",
    "        RSV_scores[str(post_id)] = W_scores[str(post_id)] + Vi_sums[post_id]\n",
    "    print(\"Scores computed\")\n",
    "    sorted_keys = sorted(RSV_scores, key=RSV_scores.get, reverse=True)\n",
    "    # plt.hist(list(W_scores.values()), bins=100)\n",
    "    # print(RSV_scores)\n",
    "    return sorted_keys[:top]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefiltering posts...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c73c7af8e7b4d6c9f610d1629628b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n"
     ]
    }
   ],
   "source": [
    "results = CobraSearch(\"deep learning neural network pytorch deployment serialisation\",posts=posts,top=10,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'47604': 1, '101756': 1, '72569': 1, '72734': 1, '22696': 1, '10463': 1, '10476': 1, '51203': 1, '51224': 1, '51236': 1, '12929': 1, '110437': 1, '79871': 1, '107808': 1, '107834': 1, '107838': 1, '107867': 1, '111098': 1, '54018': 1, '54026': 1, '68893': 1, '74444': 1, '74465': 1, '28157': 1, '31657': 1, '31680': 1, '16751': 1, '58491': 1, '58510': 1, '34024': 1, '34061': 1, '84798': 1, '84804': 1, '84827': 1, '23764': 1, '23828': 1, '23843': 1, '45174': 1, '45208': 1, '75726': 1, '39186': 1, '17367': 1, '88879': 1, '87432': 1, '26325': 1, '26918': 1, '13868': 1, '65773': 1, '11214': 1, '21794': 1, '10943': 1, '39252': 1, '52871': 1, '39859': 1, '58181': 1, '39898': 1, '40018': 1, '9854': 1, '106568': 1, '106626': 1, '106627': 1, '68329': 1, '57243': 1, '65024': 1, '93396': 1, '677': 1, '81479': 1, '106879': 1, '106964': 1, '42602': 1, '68998': 1, '69017': 1, '69036': 1, '18804': 1, '104944': 1, '104949': 1, '108939': 1, '108947': 1, '86287': 1, '14690': 1, '115654': 1, '115684': 1, '115707': 1, '65527': 1, '118201': 1, '118264': 1, '68136': 1, '73050': 1, '73072': 1, '57353': 1, '92799': 1, '19615': 1, '13550': 1, '93937': 1, '97544': 1, '115330': 1, '115359': 1, '66280': 1, '9539': 1, '97516': 1, '73096': 1, '118647': 1, '118650': 1, '36842': 1, '77134': 1, '77135': 1, '43821': 1, '58383': 1, '97329': 1, '97343': 1, '29975': 1, '40940': 1, '114148': 1, '6123': 1, '58961': 1, '21677': 1, '69914': 2, '69932': 1, '22087': 1, '41366': 1, '114591': 1, '48308': 1, '114439': 1, '114444': 1, '54873': 1, '54885': 1, '54904': 1, '86405': 1, '24323': 1, '49198': 1, '94716': 1, '25235': 1, '25266': 1, '10780': 1, '80775': 1, '81917': 1, '102131': 1, '102239': 1, '99725': 1, '102357': 1, '31485': 1, '114880': 1, '32866': 1, '32881': 1, '32905': 1, '48111': 1, '48112': 1, '11890': 1, '45355': 1, '117367': 1, '11334': 1, '16384': 1, '54742': 1, '41398': 1, '41531': 1, '36291': 1, '36295': 1, '36359': 1, '23132': 1, '111344': 1, '37899': 1, '61173': 1, '24989': 1, '16510': 1, '31914': 2, '31989': 1, '42061': 1, '42064': 1, '20415': 2, '110096': 1, '110098': 1, '40166': 1, '65310': 1, '81702': 1, '103715': 1, '38167': 1, '38196': 1, '118132': 1, '47087': 1, '77566': 1, '64885': 1, '64929': 1, '32221': 1, '34073': 1, '31300': 1, '68599': 1, '68618': 1, '89051': 1, '15314': 1, '66034': 1, '66079': 1, '62820': 1, '31804': 1, '65152': 1, '60429': 2, '43358': 1, '24405': 1, '109851': 1, '109902': 1, '87206': 1, '87222': 1, '43281': 1, '81198': 1, '39024': 1, '39071': 1, '39073': 1, '15587': 1, '15655': 1, '33459': 1, '41686': 1, '23362': 1, '52763': 1, '11454': 1, '65896': 1, '65968': 1, '92649': 1, '49775': 1, '60107': 1, '46905': 1, '116255': 1, '84007': 1, '90769': 1, '14355': 1, '78489': 1, '78509': 1, '78511': 1, '18308': 1, '47248': 1, '47266': 1, '80752': 1, '96542': 1, '69167': 1, '96825': 1, '13726': 1, '80991': 1, '65394': 1, '65399': 1, '65467': 1, '77339': 1, '77340': 1, '79858': 1, '26517': 1, '26570': 1, '8113': 1, '8202': 1, '40331': 1, '108529': 1, '108608': 1, '116589': 1, '116602': 1, '68203': 1, '28754': 1, '108665': 1, '114201': 1, '114234': 1, '24213': 1, '51908': 1, '112315': 1, '5689': 2, '84140': 1, '84172': 1, '52584': 1, '60323': 1, '60404': 3, '71741': 1, '49565': 1, '67682': 1, '86530': 1, '86571': 1, '111941': 1, '86910': 1, '30108': 1, '111123': 1, '111221': 1, '116456': 1, '27576': 1, '27581': 1, '63185': 1, '26013': 1, '26087': 1, '48711': 1, '58092': 1, '80392': 1, '71468': 1, '117723': 1, '117804': 1, '29352': 1, '36955': 1, '116836': 1, '80095': 1, '107955': 1, '103543': 1, '51983': 1, '82012': 1, '82014': 1, '13126': 1, '13185': 1, '13195': 1, '61696': 1, '60850': 1, '103214': 1, '63350': 1, '93263': 1, '93324': 1, '57924': 1, '57945': 1, '47913': 1, '47955': 1, '61483': 1, '61569': 1, '114828': 1, '101823': 1, '101918': 1, '78028': 1, '76464': 1, '76465': 1, '76515': 1, '66705': 1, '109761': 1, '109817': 1, '56731': 1, '108382': 1, '113484': 1, '30573': 1, '30607': 1, '30620': 1, '51155': 1, '70244': 1, '112626': 1, '55793': 1, '55928': 1, '114055': 1, '96652': 1, '77071': 1, '76788': 1, '62182': 1, '111304': 1, '36652': 1, '36703': 1, '36765': 1, '27393': 1, '27444': 1, '102484': 1, '14575': 1, '14599': 1, '14624': 1, '84343': 1, '736': 1, '8440': 1, '55398': 1, '55451': 1, '48447': 1, '41959': 1, '12644': 1, '92244': 1, '31134': 1, '31198': 1, '31263': 1, '113638': 1, '55654': 1, '66086': 1, '28988': 1, '114651': 1, '114661': 1, '114662': 1, '114737': 1, '56209': 1, '66931': 1, '111594': 1, '61899': 1, '41850': 1, '26652': 1, '26654': 1, '26661': 1, '15398': 1, '15459': 1, '61440': 1, '73673': 1, '61323': 1, '15135': 1, '28340': 1, '56278': 1, '117628': 1, '17925': 1, '17970': 1, '46218': 1, '77819': 1, '73224': 1, '8038': 1, '70312': 1, '70337': 1, '46332': 1, '80608': 1, '103419': 1, '75401': 1, '75481': 1, '116385': 1, '17125': 1, '66640': 1, '14477': 1, '100503': 1, '108893': 1, '57166': 1, '51444': 1, '35853': 1, '92939': 1, '5373': 1, '84427': 1, '84437': 1, '108208': 1, '108273': 1, '108277': 1, '108311': 1, '108328': 1, '87717': 1, '39694': 1, '118578': 1, '85152': 1, '104377': 1, '103066': 1, '48161': 1, '76277': 1, '106780': 1, '118052': 1, '32428': 1, '32442': 1, '48998': 1, '49012': 1, '74307': 1, '74363': 1, '74364': 1, '74374': 1, '40900': 1, '20217': 1, '20237': 1, '112749': 1, '74188': 1, '74253': 1, '23586': 1, '92402': 1, '92474': 1, '12793': 1, '90256': 1, '90262': 1, '36006': 1, '94543': 1, '66345': 1, '66388': 1, '6768': 1, '6809': 1, '37644': 1, '110974': 1, '31071': 1, '57563': 1, '36430': 1, '23322': 1, '49610': 1, '55146': 1, '52130': 1, '18086': 1, '13990': 1, '14046': 1, '46796': 1, '46802': 1, '106996': 1, '107012': 1, '107091': 1, '115826': 1, '14198': 1, '14216': 1, '109158': 1, '106420': 1, '76635': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(inverted_index_title[\"sk\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefiltering posts...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb877a8dcd5a495193525e99fb2ce50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n"
     ]
    }
   ],
   "source": [
    "result1 = CobraSearch(\"mesure performance for multiclassification model\",posts=posts,top=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['66380', '65664', '53134', '66397', '9823', '72167', '22762', '80785', '41299', '17446']\n"
     ]
    }
   ],
   "source": [
    "print(result1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interface Tkinter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefiltering posts...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f160bf05a04dd1ba63c3cac3a65d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from PIL import ImageTk, Image\n",
    "import webbrowser\n",
    "\n",
    "# create a global variable for the photo object\n",
    "photo = None\n",
    "\n",
    "def search(query, top=10):\n",
    "    # search the query in the database and return the top 10 results\n",
    "    # use the variable posts\n",
    "    # use the function CobraSearch\n",
    "    result = CobraSearch(query)\n",
    "    return result\n",
    "    \n",
    "\n",
    "# Tkinter interface for the search engine\n",
    "# create a window\n",
    "window = tk.Tk()\n",
    "window.title(\"Search Engine\")\n",
    "window.geometry(\"1500x1200\")  # set window size to 800x1500\n",
    "window.resizable(False, False)  # disable window resizing\n",
    "window.configure(bg=\"#333333\")  # set background color to a darker shade of gray\n",
    "\n",
    "# create a label with a custom font, size, and color\n",
    "lbl = tk.Label(window, text=\"Cobra Search\", font=(\"Arial Bold\", 50), fg=\"#14DEA5\", bg=\"#333333\")\n",
    "lbl.pack(pady=20)\n",
    "\n",
    "# create a frame for the input section\n",
    "input_frame = tk.Frame(window, bg=\"#333333\")\n",
    "input_frame.pack(pady=20)\n",
    "\n",
    "# create a label for the input section with custom font, size, and color\n",
    "lbl_query = tk.Label(input_frame, text=\"Enter your query\", font=(\"Arial Bold\", 20), fg=\"white\", bg=\"#333333\")\n",
    "lbl_query.grid(row=0, column=0, padx=10, pady=5, sticky=\"e\")\n",
    "\n",
    "# create a text entry box with custom font, size, and color\n",
    "txt = tk.Entry(input_frame, width=50, font=(\"Arial\", 14), bg=\"#555555\", fg=\"white\")\n",
    "txt.grid(row=0, column=1, padx=5, pady=5, sticky=\"w\")\n",
    "\n",
    "# create a frame for the result section\n",
    "result_frame = tk.Frame(window, bg=\"#333333\")\n",
    "result_frame.pack()\n",
    "\n",
    "# function to handle link clicks\n",
    "def open_link(link):\n",
    "    webbrowser.open(link)\n",
    "\n",
    "# call the function search when the button is clicked\n",
    "def clicked():\n",
    "    query = txt.get()\n",
    "    result = search(query, top=10)\n",
    "    result_list.delete(0, tk.END)  # clear previous results\n",
    "    for post_id in result:\n",
    "        result_list.insert(tk.END, f\"https://datascience.stackexchange.com/questions/{post_id}\")\n",
    "   \n",
    "def clicked_result(event):\n",
    "    # get the index of the clicked item\n",
    "    index = result_list.curselection()[0]\n",
    "    # get the text of the clicked item\n",
    "    post_id = result_list.get(index)\n",
    "    # open the link in the browser\n",
    "    open_link(post_id)\n",
    "\n",
    "# bind the click event to the listbox\n",
    "\n",
    "\n",
    "# create a button with a custom font, size, and color\n",
    "btn = tk.Button(window, text=\"Search\", command=clicked, font=(\"Arial\", 14), bg=\"#14DEA5\", fg=\"#333333\")\n",
    "btn.pack(pady=10)\n",
    "\n",
    "# load the image and create a photo object\n",
    "img = Image.open(\"cobralogo.jpeg\")\n",
    "photo = ImageTk.PhotoImage(img)\n",
    "\n",
    "# create a label with the photo object\n",
    "panel = tk.Label(window, image=photo, bg=\"#333333\")\n",
    "panel.pack(pady=20)\n",
    "\n",
    "# create a listbox for displaying the search results with custom font, size, and color\n",
    "result_list = tk.Listbox(result_frame, font=(\"Arial\", 14), fg=\"#14DEA5\", bg=\"#333333\", selectbackground=\"#14DEA5\", selectforeground=\"#333333\")\n",
    "result_list.pack(pady=10, fill=tk.BOTH, expand=True)\n",
    "result_list.config(width=50,height=10) \n",
    "\n",
    "result_list.bind(\"<<ListboxSelect>>\", clicked_result)\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of our search method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our search engine we are using the NDCG score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "def get_ndcg_scores(df_relevancy,method=CobraSearch,top=len(posts)):\n",
    "    \"\"\"\n",
    "    Calculates the NDCG (Normalized Discounted Cumulative Gain) score for each query in the given relevance dataframe\n",
    "    using the specified search method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_relevancy : pandas.DataFrame\n",
    "        A dataframe containing the relevance scores for each post and query.\n",
    "        The first column should contain the post IDs, and the remaining columns from the 4th should be named 'query X',\n",
    "        where X is the query number starting from 1.\n",
    "        The values in the columns should be the relevance scores for each post with respect to the corresponding query.\n",
    "    method : function\n",
    "        The search method to use for retrieving the top documents for each query.\n",
    "        The function should take a query string as input and return a pandas.DataFrame containing the top documents.\n",
    "    top : int, optional\n",
    "        The number of top documents to retrieve for each query.\n",
    "        The default value is the total number of posts in the dataset.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        A dictionary containing the NDCG score for each query.\n",
    "        The keys are the query numbers starting from 1, and the values are the corresponding NDCG scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # for each PostId in the relevancy dataframe, get the rank of the post according to the method\n",
    "    min_relevancy_index=3\n",
    "    max_relevancy_index=8\n",
    "    querys = {}\n",
    "    for i in range(min_relevancy_index, max_relevancy_index):\n",
    "        querys[i] = df_relevancy.columns[i][10:]\n",
    "    method_results = {}\n",
    "    for i in range(min_relevancy_index, max_relevancy_index):\n",
    "        method_results[i] = method(query=querys[i],top=top) # list of post_id sorted by the method\n",
    "        method_results[i] = [int(x) for x in method_results[i]] # remove post_id not in posts\n",
    "    posts_results = {}\n",
    "    # sort the posts according to the method\n",
    "    for i in range(min_relevancy_index, max_relevancy_index):\n",
    "        posts_results[i] = posts[posts[\"Id\"].isin(method_results[i])].copy()\n",
    "        # sort according to the method\n",
    "        posts_results[i][\"rank\"] = posts_results[i][\"Id\"].apply(lambda x: method_results[i].index(x))\n",
    "        posts_results[i].sort_values(by=[\"rank\"], inplace=True)\n",
    "        #print(method_results[i].head())\n",
    "    # get the score of each post according to the method\n",
    "    rel_preds = {}\n",
    "    for i in range(min_relevancy_index, max_relevancy_index):\n",
    "        # the rankings of the posts in PostId in df_relevancy\n",
    "        rel_preds[i] = []\n",
    "        # print(method_results[i])\n",
    "        for post_id in df_relevancy[\"PostId\"]:\n",
    "            if int(post_id) in method_results[i]:\n",
    "                rel_preds[i].append(posts_results[i][posts_results[i][\"Id\"]==post_id][\"rank\"].values[0])\n",
    "            else:\n",
    "                rel_preds[i].append(top)\n",
    "        #print(rel_preds[i])\n",
    "    rel_trues = {}\n",
    "    for i in range(min_relevancy_index, max_relevancy_index):\n",
    "        # the rankings of the posts\n",
    "        rel_trues[i] = df_relevancy[df_relevancy.columns[i]].tolist()\n",
    "        #print(rel_trues[i])\n",
    "        rel_trues[i] = [top if np.isnan(x) else x for x in rel_trues[i]]\n",
    "    # calculate the ndcg score for each query\n",
    "    ndcg_scores = {}\n",
    "    for i in range(min_relevancy_index, max_relevancy_index):\n",
    "        ndcg_scores[i] = ndcg_score([rel_trues[i]], [rel_preds[i]])\n",
    "    return ndcg_scores\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefiltering posts...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42f66d152114b3e8f22aeaccfde3ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n",
      "prefiltering posts...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449028539b97439da3b3ea5e1b96d593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n",
      "prefiltering posts...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778473d1a4bb4ad799cd0963b69fc590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n",
      "prefiltering posts...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d703eb829747b6905efb4d2c0a1502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n",
      "prefiltering posts...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5deaecd5dd7644558e4495b3b9fe3282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores computed\n",
      "{3: 0.9340604629174767, 4: 0.8166029099442649, 5: 0.7705534652666164, 6: 0.7705541356330388, 7: 0.8587868868940064}\n"
     ]
    }
   ],
   "source": [
    "# Read Relevancy CSV\n",
    "df_relevancy = pd.read_excel(\"evaluation_search_engine_post_queries_ranking_EI_CS.xlsx\")\n",
    "# print(df_relevancy.head())\n",
    "df_relevancy\n",
    "print(get_ndcg_scores(df_relevancy,method=CobraSearch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefiltering posts...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788609f9f45e4083946955ca6ddd0623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m results \u001b[39m=\u001b[39m {}\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m querys:\n\u001b[0;32m---> 11\u001b[0m     result \u001b[39m=\u001b[39m CobraSearch(query\u001b[39m=\u001b[39;49mquery, top\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m     results[query] \u001b[39m=\u001b[39m result\n",
      "Cell \u001b[0;32mIn[25], line 63\u001b[0m, in \u001b[0;36mCobraSearch\u001b[0;34m(query, posts, top, verbose)\u001b[0m\n\u001b[1;32m     59\u001b[0m m \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean([get_Ld(post_id) \u001b[39mfor\u001b[39;00m post_id \u001b[39min\u001b[39;00m post_ids_filtered])\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m post_id \u001b[39min\u001b[39;00m tqdm(post_ids_filtered):\n\u001b[0;32m---> 63\u001b[0m     term_freq_body \u001b[39m=\u001b[39m term_frequency(post_id, inverted_index_body)\n\u001b[1;32m     64\u001b[0m     term_freq_title \u001b[39m=\u001b[39m term_frequency(post_id, inverted_index_title)\n\u001b[1;32m     65\u001b[0m     term_freq_tags \u001b[39m=\u001b[39m term_frequency(post_id, inverted_index_tags)\n",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m, in \u001b[0;36mterm_frequency\u001b[0;34m(post_id, inverted_index)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m inverted_index:\n\u001b[1;32m      6\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(post_id) \u001b[39min\u001b[39;00m inverted_index[word]:\n\u001b[0;32m----> 7\u001b[0m         tf[word] \u001b[39m=\u001b[39m inverted_index[word][\u001b[39mstr\u001b[39m(post_id)]\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(posts[posts[\u001b[39m\"\u001b[39;49m\u001b[39mId\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39;49m post_id][\u001b[39m\"\u001b[39m\u001b[39mBody\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m])\n\u001b[1;32m      8\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m         tf[word] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3798\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[1;32m   3797\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> 3798\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_bool_array(key)\n\u001b[1;32m   3800\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m is_single_key \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3851\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3846\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mItem wrong length \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(key)\u001b[39m}\u001b[39;00m\u001b[39m instead of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3847\u001b[0m     )\n\u001b[1;32m   3849\u001b[0m \u001b[39m# check_bool_indexer will throw exception if Series key cannot\u001b[39;00m\n\u001b[1;32m   3850\u001b[0m \u001b[39m# be reindexed to match DataFrame rows\u001b[39;00m\n\u001b[0;32m-> 3851\u001b[0m key \u001b[39m=\u001b[39m check_bool_indexer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex, key)\n\u001b[1;32m   3852\u001b[0m indexer \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]\n\u001b[1;32m   3853\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexing.py:2571\u001b[0m, in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2567\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_array_like(result):\n\u001b[1;32m   2568\u001b[0m     \u001b[39m# GH 33924\u001b[39;00m\n\u001b[1;32m   2569\u001b[0m     \u001b[39m# key may contain nan elements, check_array_indexer needs bool array\u001b[39;00m\n\u001b[1;32m   2570\u001b[0m     result \u001b[39m=\u001b[39m pd_array(result, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[0;32m-> 2571\u001b[0m \u001b[39mreturn\u001b[39;00m check_array_indexer(index, result)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexers/utils.py:430\u001b[0m, in \u001b[0;36mcheck_array_indexer\u001b[0;34m(array, indexer)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[39mreturn\u001b[39;00m item\n\u001b[1;32m    426\u001b[0m \u001b[39m# -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m# Public indexer validation\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_array_indexer\u001b[39m(array: AnyArrayLike, indexer: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    431\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m    Check if `indexer` is a valid array indexer for `array`.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[39m    IndexError: arrays used as indices must be of integer or boolean type\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstruction\u001b[39;00m \u001b[39mimport\u001b[39;00m array \u001b[39mas\u001b[39;00m pd_array\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test sur quelques requetes\n",
    "querys = [\"object detection machine learning\",\n",
    "          \"how to do linear regression in python\",\n",
    "          \"similarity scores by clustering\",\n",
    "          \"map reduce algorithm database\"]\n",
    "results = {}\n",
    "for query in querys:\n",
    "    result = CobraSearch(query=query, top=10)\n",
    "    results[query] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictive modeling and machine learning\n",
      "['51468', '6189', '20201', '96490', '58448', '31167', '102173', '58102', '27431', '17266']\n",
      "\n",
      "\n",
      "machine learning ensemble modeling hyperparameter meta learning\n",
      "['36748']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for query in querys:\n",
    "    print(query)\n",
    "    print(results[query])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
