{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xycip-y7MmUq"
      },
      "source": [
        "# DAY 3: Student version"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gnCKD6NtjZno"
      },
      "source": [
        "**Machine Learning NLP**\n",
        "\n",
        "The goal of this session is to improve the search engine using NLP features.\n",
        "\n",
        "This notebook guides you through different techniques to explore. It is expected of you to be inventive and improve the techniques introduced. \n",
        "\n",
        "First, let's import the useful packages and load the data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tS0Nv3gMZw7D"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU8SMVS4Zu0w",
        "outputId": "0d620766-c1ed-4c34-d70a-54f92de506b6"
      },
      "outputs": [],
      "source": [
        "#  !pip install sentence-transformers --quiet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TBamWPKlkZs0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50KuGjmPjquG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from scipy.sparse import find\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bVl7AEcBkgyz"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVb0xZIvkevD",
        "outputId": "6e8028c1-4137-45b7-a6da-beebe12a595a"
      },
      "outputs": [],
      "source": [
        "# Only if you use Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "# TODO:\n",
        "DATA_PATH = 'datascience.stackexchange.com/' \n",
        "\n",
        "# # CORR:\n",
        "# DATA_PATH = '/content/drive/MyDrive/TP Centrale/data'\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBtfx_3RYSs9"
      },
      "outputs": [],
      "source": [
        "posts = pd.read_xml(os.path.join(DATA_PATH, 'Posts.xml'), parser=\"etree\", encoding=\"utf8\")\n",
        "posts"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2q_ryLonxLYe"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cnOIzOY4e5Xh"
      },
      "source": [
        "Implement a function to clean the posts. \n",
        "\n",
        "You can reuse what you have used in the Day 2 notebook or improve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9e-XxHMAkmEu"
      },
      "outputs": [],
      "source": [
        "def clean_post(text:str)->str:\n",
        "    try:\n",
        "        clean_post = re.sub(r'<.*?>', '', text)\n",
        "        return clean_post\n",
        "    except: # empty text (nan, None, etc..)\n",
        "        return \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv_n2e5Y1a04"
      },
      "outputs": [],
      "source": [
        "posts['cleaned_body'] = posts.Body.apply(clean_post)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9vYfAFcDe_ER"
      },
      "source": [
        "You can also implement a function that cleans the user's query (the query). \n",
        "\n",
        "This step is optionnal (if you don't think that it is necessary, just return the query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8_gvmjXwBwt"
      },
      "outputs": [],
      "source": [
        "def clean_query(text:str)->str:\n",
        "    # keep only letters\n",
        "    cleaned_query = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "    return cleaned_query"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BnjdSpeLlZzJ"
      },
      "source": [
        "## Text specific metadata"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qEyQEpoaoGLY"
      },
      "source": [
        "What metadata can you get from the text at your disposal ? Which ones are relevant ? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b_O0YS-oMAh"
      },
      "outputs": [],
      "source": [
        "print(posts.columns)\n",
        "# the relevant metadata is:\n",
        "# the Title, the Score, the ViewCount, the Tags, the AnswerCount, the CommentCount and the FavoriteCount"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o0rsRxD4xeli"
      },
      "source": [
        "## Classic Preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NLmg2T3qxjc3"
      },
      "source": [
        "The goal for this part is to implement a classic vectorization (Count vectorizer, tfidf...).\n",
        "\n",
        "You can do it on your own or use scikit-learn.\n",
        "\n",
        "Hints : pay attention to stopwords, additionnal preprocessing steps and techniques of vectoriation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTaWk2nAAiTj"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(posts.cleaned_body.values)\n",
        "vectors = vectorizer.transform(posts.cleaned_body.values)\n",
        "print(vectors)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "drg1x_9v1RD6"
      },
      "source": [
        "Write a function that applies the same process to the query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpBr0mfF1QVr"
      },
      "outputs": [],
      "source": [
        "def vectorize_query(query : str, vectorizer=vectorizer):\n",
        "    \"\"\"vectorizes the query\n",
        "    Args:\n",
        "        query (str): query string\n",
        "        vectorizer (optional): Defaults to vectorizer.\n",
        "\n",
        "    Returns:\n",
        "        query vectorized\n",
        "    \"\"\"\n",
        "    query_vectorized = vectorizer.transform([query])\n",
        "\n",
        "    return query_vectorized"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO5uBYas15HS"
      },
      "source": [
        "Determine a way to use this vectorization to suggest the closest items to the entry in the database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X2JhLiCU1NPu"
      },
      "outputs": [],
      "source": [
        "def vectorizer_search(query : str,\n",
        "                      vectors=vectors,\n",
        "                      vectorizer=vectorizer) -> list:\n",
        "    \n",
        "    query_vectorized = vectorize_query(query, vectorizer)\n",
        "    # compute cosine similarity\n",
        "    \n",
        "    similarity = np.dot(query_vectorized, vectors.T).todense()\n",
        "    # get the indices of the 5 most similar posts\n",
        "    indices = np.argsort(similarity)\n",
        "    indices = np.array(indices).flatten()[-5:][::-1]\n",
        "    print(len(indices))\n",
        "\n",
        "    # get the posts\n",
        "    print(indices)\n",
        "    closest_items = posts.iloc[indices] # buffer has wrong number of dimensions (expected 1, got 2)\n",
        "    return closest_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FHVHzd-G3Sd0"
      },
      "outputs": [],
      "source": [
        "entry = 'what is stochastic gradient descent ?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir0QIX-XGE-N",
        "outputId": "2e456d24-c2d8-4a64-b1f6-5c418e081f6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "[45389 25089 74678 10551  8366]\n",
            "I have been training a WGAN for a while now, with my generator training once in every five epochs.\n",
            "\n",
            "I have tried several model architectures(no of filters) and also tried varying the relationship with each other. No matter what happens, my output is essentially noise. On further reading, it seems to be a classic case of convergence failure. \n",
            "\n",
            "Over time, my generator loss gets more and more negative while my discriminator loss remains around -0.4\n",
            "\n",
            "My guess is that since the discriminator isn't improving enough, the generator doesn't get improve enough.\n",
            "\n",
            "gen_loss = 0.0, disc_loss = -0.03792113810777664\n",
            "Time for epoch 567 is 3.381150007247925 sec - gen_loss = 0.0, disc_loss = -0.037839196622371674\n",
            "Time for epoch 568 is 3.3113789558410645 sec - gen_loss = 0.0, disc_loss = -0.040219761431217194\n",
            "Time for epoch 569 is 3.2963240146636963 sec - gen_loss = 0.0, disc_loss = -0.04105686396360397\n",
            "Time for epoch 570 is 9.3097665309906 sec - gen_loss = -0.3167822062969208, disc_loss = -0.04042121022939682\n",
            "Time for epoch 571 is 3.314333200454712 sec - gen_loss = 0.0, disc_loss = -0.03815283626317978\n",
            "Time for epoch 572 is 3.3485965728759766 sec - gen_loss = 0.0, disc_loss = -0.038569360971450806\n",
            "Time for epoch 573 is 3.3110241889953613 sec - gen_loss = 0.0, disc_loss = -0.03980369493365288\n",
            "Time for epoch 574 is 3.4034907817840576 sec - gen_loss = 0.0, disc_loss = -0.0400879867374897\n",
            "Time for epoch 575 is 9.47887134552002 sec - gen_loss = -0.29222938418388367, disc_loss = -0.04063987731933594\n",
            "Time for epoch 576 is 3.329411745071411 sec - gen_loss = 0.0, disc_loss = -0.03907758742570877\n",
            "Time for epoch 577 is 3.3210532665252686 sec - gen_loss = 0.0, disc_loss = -0.03997725248336792\n",
            "Time for epoch 578 is 3.303483247756958 sec - gen_loss = 0.0, disc_loss = -0.041025031358003616\n",
            "Time for epoch 579 is 3.3799941539764404 sec - gen_loss = 0.0, disc_loss = -0.04217495769262314\n",
            "Time for epoch 580 is 9.339993000030518 sec - gen_loss = -0.324483722448349, disc_loss = -0.04243335872888565\n",
            "Time for epoch 581 is 3.300795555114746 sec - gen_loss = 0.0, disc_loss = -0.04034840315580368\n",
            "Time for epoch 582 is 3.322876453399658 sec - gen_loss = 0.0, disc_loss = -0.0420600101351738\n",
            "Time for epoch 583 is 3.328361749649048 sec - gen_loss = 0.0, disc_loss = -0.04354345053434372\n",
            "Time for epoch 584 is 3.277684211730957 sec - gen_loss = 0.0, disc_loss = -0.04367030784487724\n",
            "Time for epoch 585 is 9.337851762771606 sec - gen_loss = -0.29389116168022156, disc_loss = -0.04380493611097336\n",
            "Time for epoch 586 is 3.282655954360962 sec - gen_loss = 0.0, disc_loss = -0.041637666523456573\n",
            "Time for epoch 587 is 3.2763051986694336 sec - gen_loss = 0.0, disc_loss = -0.04260318726301193\n",
            "Time for epoch 588 is 3.3087923526763916 sec - gen_loss = 0.0, disc_loss = -0.043825842440128326\n",
            "Time for epoch 589 is 3.321415662765503 sec - gen_loss = 0.0, disc_loss = -0.044720184057950974\n",
            "Time for epoch 590 is 9.419749975204468 sec - gen_loss = -0.3067258596420288, disc_loss = -0.044646285474300385\n",
            "Time for epoch 591 is 3.4093453884124756 sec - gen_loss = 0.0, disc_loss = -0.04444003105163574\n",
            "Time for epoch 592 is 3.3971118927001953 sec - gen_loss = 0.0, disc_loss = -0.04441792890429497\n",
            "Time for epoch 593 is 3.4011573791503906 sec - gen_loss = 0.0, disc_loss = -0.04336284473538399\n",
            "\n",
            "\n",
            "I am not entirely sure about the zeros and I guess it has something to do with my printing:\n",
            "\n",
            "for epoch in range(epochs):\n",
            "        start = time.time()\n",
            "        disc_loss = 0\n",
            "        gen_loss = 0\n",
            "        for images in train_dataset:\n",
            "            #images=np.expand_dims(images, axis=0)\n",
            "            #images=images/255.\n",
            "            #images=images.resize\n",
            "            disc_loss += train_discriminator(images)\n",
            "            if disc_optimizer.iterations.numpy() % n_critic == 0:\n",
            "                gen_loss += train_generator()\n",
            "\n",
            "        print('Time for epoch {} is {} sec - gen_loss = {}, disc_loss = {}'.format(epoch + 1, time.time() - start, gen_loss / batch_size, disc_loss / (batch_size*n_critic)))\n",
            "\n",
            "        if epoch % save_interval == 0:\n",
            "            save_imgs(epoch, generator, seed)\n",
            "            #model.save_weights(checkpoint_path.format(epoch=0))\n",
            "            #save_weights(checkpoint_path.format(+=0))\n",
            "\n",
            "\n",
            "The current epoch snippet is for a smaller period. The losses can go as low as:\n",
            "\n",
            "gen_loss = Time for epoch 420 is 4.274262428283691 sec - gen_loss = -9.779035568237305, disc_loss = -0.9567102193832397\n",
            "Time for epoch 421 is 3.0774970054626465 sec - gen_loss = 0.0, disc_loss = -0.9157863855361938\n",
            "Time for epoch 422 is 3.0417258739471436 sec - gen_loss = 0.0, disc_loss = -0.8936088681221008\n",
            "Time for epoch 423 is 3.0934689044952393 sec - gen_loss = 0.0, disc_loss = -0.896615207195282\n",
            "Time for epoch 424 is 3.0459794998168945 sec - gen_loss = 0.0, disc_loss = -0.9322511553764343\n",
            "tf.Tensor(-81.94704, shape=(), dtype=float32)\n",
            "Time for epoch 425 is 4.237549543380737 sec - gen_loss = -10.243379592895508, disc_loss = -0.9419326782226562\n",
            "Time for epoch 426 is 3.03023099899292 sec - gen_loss = 0.0, disc_loss = -0.9134870767593384\n",
            "Time for epoch 427 is 2.998375177383423 sec - gen_loss = 0.0, disc_loss = -0.9380167126655579\n",
            "Time for epoch 428 is 2.9811060428619385 sec - gen_loss = 0.0, disc_loss = -0.9134092330932617\n",
            "Time for epoch 429 is 3.087916374206543 sec - gen_loss = 0.0, disc_loss = -0.9051135778427124\n",
            "tf.Tensor(-83.00238, shape=(), dtype=float32)\n",
            "Time for epoch 430 is 4.272223949432373 sec - gen_loss = -10.375297546386719, disc_loss = -1.003989577293396\n",
            "Time for epoch 431 is 3.0454840660095215 sec - gen_loss = 0.0, disc_loss = -0.9496141672134399\n",
            "Time for epoch 432 is 3.090559959411621 sec - gen_loss = 0.0, disc_loss = -0.9521171450614929\n",
            "Time for epoch 433 is 3.1101419925689697 sec - gen_loss = 0.0, disc_loss = -0.9876922369003296\n",
            "Time for epoch 434 is 3.0372989177703857 sec - gen_loss = 0.0, disc_loss = -0.9995473623275757\n",
            "tf.Tensor(-79.649864, shape=(), dtype=float32)\n",
            "Time for epoch 435 is 4.32908034324646 sec - gen_loss = -9.956233024597168, disc_loss = -1.003030776977539\n",
            "Time for epoch 436 is 3.106421947479248 sec - gen_loss = 0.0, disc_loss = -0.9365862011909485\n",
            "Time for epoch 437 is 3.1067636013031006 sec - gen_loss = 0.0, disc_loss = -1.0536330938339233\n",
            "Time for epoch 438 is 3.063079833984375 sec - gen_loss = 0.0, disc_loss = -0.9735730886459351\n",
            "Time for epoch 439 is 3.1522281169891357 sec - gen_loss = 0.0, disc_loss = -0.9937177896499634\n",
            "tf.Tensor(-75.338615, shape=(), dtype=float32)\n",
            "Time for epoch 440 is 4.324256896972656 sec - gen_loss = -9.417326927185059, disc_loss = -1.058488130569458\n",
            "Time for epoch 441 is 3.1664624214172363 sec - gen_loss = 0.0, disc_loss = -0.9834483861923218\n",
            "Time for epoch 442 is 3.086495876312256 sec - gen_loss = 0.0, disc_loss = -0.9649847149848938\n",
            "Time for epoch 443 is 3.0835535526275635 sec - gen_loss = 0.0, disc_loss = -1.0420929193496704\n",
            "Time for epoch 444 is 3.0816898345947266 sec - gen_loss = 0.0, disc_loss = -1.0146191120147705\n",
            "tf.Tensor(-74.671135, shape=(), dtype=float32)\n",
            "Time for epoch 445 is 4.27916693687439 sec - gen_loss = -9.333891868591309, disc_loss = -0.9707431793212891\n",
            "Time for epoch 446 is 3.0552937984466553 sec - gen_loss = 0.0, disc_loss = -1.0040273666381836\n",
            "Time for epoch 447 is 3.032083034515381 sec - gen_loss = 0.0, disc_loss = -1.078584909439087\n",
            "Time for epoch 448 is 3.1026992797851562 sec - gen_loss = 0.0, disc_loss = -0.9929893612861633\n",
            "Time for epoch 449 is 3.111077070236206 sec - gen_loss = 0.0, disc_loss = -1.0746228694915771\n",
            "tf.Tensor(-76.32341, shape=(), dtype=float32)\n",
            "Time for epoch 450 is 4.429989337921143 sec - gen_loss = -9.540426254272461, disc_loss = -0.9597347378730774\n",
            "Time for epoch 451 is 3.0502800941467285 sec - gen_loss = 0.0, disc_loss = -0.9540794491767883\n",
            "Time for epoch 452 is 3.0913126468658447 sec - gen_loss = 0.0, disc_loss = -1.008721113204956\n",
            "Time for epoch 453 is 3.05441951751709 sec - gen_loss = 0.0, disc_loss = -1.0138479471206665\n",
            "Time for epoch 454 is 3.041020631790161 sec - gen_loss = 0.0, disc_loss = -0.9122379422187805\n",
            "tf.Tensor(-73.73149, shape=(), dtype=float32)\n",
            "Time for epoch 455 is 4.271183967590332 sec - gen_loss = -9.216436386108398, disc_loss = -0.9789434671401978\n",
            "Time for epoch 456 is 3.10066556930542 sec - gen_loss = 0.0, disc_loss = -0.9811899065971375\n",
            "Time for epoch 457 is 3.1411514282226562 sec - gen_loss = 0.0, disc_loss = -0.9947725534439087\n",
            "Time for epoch 458 is 3.15008807182312 sec - gen_loss = 0.0, disc_loss = -1.0282094478607178\n",
            "Time for epoch 459 is 3.1146531105041504 sec - gen_loss = 0.0, disc_loss = -1.0895274877548218\n",
            "tf.Tensor(-79.197815, shape=(), dtype=float32)\n",
            "Time for epoch 460 is 4.334632396697998 sec - gen_loss = -9.899726867675781, disc_loss = -1.0837827920913696\n",
            "Time for epoch 461 is 3.084667205810547 sec - gen_loss = 0.0, disc_loss = -1.0000011920928955\n",
            "Time for epoch 462 is 2.9984536170959473 sec - gen_loss = 0.0, disc_loss = -1.064965009689331\n",
            "Time for epoch 463 is 3.0733871459960938 sec - gen_loss = 0.0, disc_loss = -1.088195562362671\n",
            "Time for epoch 464 is 3.0906479358673096 sec - gen_loss = 0.0, disc_loss = -1.0385420322418213\n",
            "tf.Tensor(-72.012695, shape=(), dtype=float32)\n",
            "Time for epoch 465 is 4.305361747741699 sec - gen_loss = -9.0015869140625, disc_loss = -1.0921838283538818\n",
            "Time for epoch 466 is 3.0221426486968994 sec - gen_loss = 0.0, disc_loss = -1.087393045425415\n",
            "Time for epoch 467 is 3.08805775642395 sec - gen_loss = 0.0, disc_loss = -1.0309044122695923\n",
            "Time for epoch 468 is 3.0641579627990723 sec - gen_loss = 0.0, disc_loss = -1.021532416343689\n",
            "Time for epoch 469 is 3.0942575931549072 sec - gen_loss = 0.0, disc_loss = -1.016531229019165\n",
            "tf.Tensor(-72.69449, shape=(), dtype=float32)\n",
            "Time for epoch 470 is 4.357362985610962 sec - gen_loss = -9.086811065673828, disc_loss = -1.0745207071304321\n",
            "Time for epoch 471 is 3.0401017665863037 sec - gen_loss = 0.0, disc_loss = -1.0113626718521118\n",
            "Time for epoch 472 is 3.0733351707458496 sec - gen_loss = 0.0, disc_loss = -1.0494859218597412\n",
            "Time for epoch 473 is 3.0829317569732666 sec - gen_loss = 0.0, disc_loss = -1.0223619937896729\n",
            "Time for epoch 474 is 3.008283853530884 sec - gen_loss = 0.0, disc_loss = -1.0144643783569336\n",
            "tf.Tensor(-72.84456, shape=(), dtype=float32)\n",
            "Time for epoch 475 is 4.384555339813232 sec - gen_loss = -9.105569839477539, disc_loss = -1.0208587646484375\n",
            "Time for epoch 476 is 3.0660083293914795 sec - gen_loss = 0.0, disc_loss = -0.9962297677993774\n",
            "Time for epoch 477 is 3.0829591751098633 sec - gen_loss = 0.0, disc_loss = -1.1114609241485596\n",
            "Time for epoch 478 is 3.0963735580444336 sec - gen_loss = 0.0, disc_loss = -1.1005897521972656\n",
            "Time for epoch 479 is 3.0595879554748535 sec - gen_loss = 0.0, disc_loss = -1.0915520191192627\n",
            "tf.Tensor(-71.82799, shape=(), dtype=float32)\n",
            "Time for epoch 480 is 4.281854629516602 sec - gen_loss = -8.978498458862305, disc_loss = -1.1100273132324219\n",
            "Time for epoch 481 is 3.091787338256836 sec - gen_loss = 0.0, disc_loss = -1.0624439716339111\n",
            "Time for epoch 482 is 3.006270170211792 sec - gen_loss = 0.0, disc_loss = -1.0699169635772705\n",
            "Time for epoch 483 is 2.963466167449951 sec - gen_loss = 0.0, disc_loss = -1.0502550601959229\n",
            "Time for epoch 484 is 3.079402446746826 sec - gen_loss = 0.0, disc_loss = -1.0949811935424805\n",
            "tf.Tensor(-75.14867, shape=(), dtype=float32)\n",
            "Time for epoch 485 is 4.3246009349823 sec - gen_loss = -9.393583297729492, disc_loss = -1.0904223918914795\n",
            "Time for epoch 486 is 3.068676710128784 sec - gen_loss = 0.0, disc_loss = -1.0440151691436768\n",
            "Time for epoch 487 is 3.0410079956054688 sec - gen_loss = 0.0, disc_loss = -1.0777149200439453\n",
            "Time for epoch 488 is 3.0098347663879395 sec - gen_loss = 0.0, disc_loss = -1.0182307958602905\n",
            "Time for epoch 489 is 3.0173022747039795 sec - gen_loss = 0.0, disc_loss = -1.0987876653671265\n",
            "tf.Tensor(-72.26952, shape=(), dtype=float32)\n",
            "Time for epoch 490 is 4.272535085678101 sec - gen_loss = -9.033690452575684, disc_loss = -1.1007413864135742\n",
            "Time for epoch 491 is 2.986027479171753 sec - gen_loss = 0.0, disc_loss = -1.1220260858535767\n",
            "Time for epoch 492 is 3.033278226852417 sec - gen_loss = 0.0, disc_loss = -1.0299326181411743\n",
            "Time for epoch 493 is 3.047642946243286 sec - gen_loss = 0.0, disc_loss = -1.0932931900024414\n",
            "Time for epoch 494 is 2.9984898567199707 sec - gen_loss = 0.0, disc_loss = -1.1239049434661865\n",
            "tf.Tensor(-79.26546, shape=(), dtype=float32)\n",
            "Time for epoch 495 is 4.306024551391602 sec - gen_loss = -9.908182144165039, disc_loss = -1.0246700048446655\n",
            "Time for epoch 496 is 2.9821512699127197 sec - gen_loss = 0.0, disc_loss = -1.0074901580810547\n",
            "Time for epoch 497 is 2.9943675994873047 sec - gen_loss = 0.0, disc_loss = -1.0436185598373413\n",
            "Time for epoch 498 is 2.981405735015869 sec - gen_loss = 0.0, disc_loss = -1.0111920833587646\n",
            "Time for epoch 499 is 3.0199577808380127 sec - gen_loss = 0.0, disc_loss = -1.0668728351593018\n",
            "tf.Tensor(-73.99868, shape=(), dtype=float32)\n",
            "Time for epoch 500 is 4.266490459442139 sec - gen_loss = -9.249835014343262, disc_loss = -1.1307373046875\n",
            "Time for epoch 501 is 3.0170788764953613 sec - gen_loss = 0.0, disc_loss = -1.1350711584091187\n",
            "Time for epoch 502 is 3.077758312225342 sec - gen_loss = 0.0, disc_loss = -1.1081371307373047\n",
            "Time for epoch 503 is 3.056257724761963 sec - gen_loss = 0.0, disc_loss = -1.1385138034820557\n",
            "Time for epoch 504 is 3.099039077758789 sec - gen_loss = 0.0, disc_loss = -1.0430777072906494\n",
            "tf.Tensor(-80.41304, shape=(), dtype=float32)\n",
            "Time for epoch 505 is 4.288381099700928 sec - gen_loss = -10.051630020141602, disc_loss = -1.0938444137573242\n",
            "Time for epoch 506 is 2.9821603298187256 sec - gen_loss = 0.0, disc_loss = -1.0110225677490234\n",
            "Time for epoch 507 is 3.041010618209839 sec - gen_loss = 0.0, disc_loss = -1.036383032798767\n",
            "Time for epoch 508 is 2.995178699493408 sec - gen_loss = 0.0, disc_loss = -1.0879932641983032\n",
            "Time for epoch 509 is 3.0064549446105957 sec - gen_loss = 0.0, disc_loss = -1.065393090248108\n",
            "tf.Tensor(-77.57954, shape=(), dtype=float32)\n",
            "Time for epoch 510 is 4.323476076126099 sec - gen_loss = -9.697442054748535, disc_loss = -1.0769941806793213\n",
            "Time for epoch 511 is 2.997896194458008 sec - gen_loss = 0.0, disc_loss = -1.1603240966796875\n",
            "Time for epoch 512 is 3.009216785430908 sec - gen_loss = 0.0, disc_loss = -1.0865892171859741\n",
            "Time for epoch 513 is 3.0198848247528076 sec - gen_loss = 0.0, disc_loss = -1.0485632419586182\n",
            "Time for epoch 514 is 3.0211234092712402 sec - gen_loss = 0.0, disc_loss = -1.0616661310195923\n",
            "tf.Tensor(-78.452515, shape=(), dtype=float32)\n",
            "Time for epoch 515 is 4.19796895980835 sec - gen_loss = -9.806564331054688, disc_loss = -1.0954179763793945\n",
            "Time for epoch 516 is 3.022573709487915 sec - gen_loss = 0.0, disc_loss = -1.0778796672821045\n",
            "Time for epoch 517 is 3.0197014808654785 sec - gen_loss = 0.0, disc_loss = -1.0623410940170288\n",
            "Time for epoch 518 is 3.0477373600006104 sec - gen_loss = 0.0, disc_loss = -1.163627028465271\n",
            "Time for epoch 519 is 3.0799355506896973 sec - gen_loss = 0.0, disc_loss = -1.149423599243164\n",
            "tf.Tensor(-76.386566, shape=(), dtype=float32)\n",
            "Time for epoch 520 is 4.313087224960327 sec - gen_loss = -9.548320770263672, disc_loss = -1.211676836013794\n",
            "Time for epoch 521 is 3.0512874126434326 sec - gen_loss = 0.0, disc_loss = -1.0809478759765625\n",
            "Time for epoch 522 is 3.0870730876922607 sec - gen_loss = 0.0, disc_loss = -1.099151611328125\n",
            "Time for epoch 523 is 3.0168159008026123 sec - gen_loss = 0.0, disc_loss = -1.1511256694793701\n",
            "Time for epoch 524 is 3.005920171737671 sec - gen_loss = 0.0, disc_loss = -1.069382667541504\n",
            "tf.Tensor(-79.15112, shape=(), dtype=float32)\n",
            "Time for epoch 525 is 4.212509870529175 sec - gen_loss = -9.893890380859375, disc_loss = -1.2109851837158203\n",
            "Time for epoch 526 is 2.9921183586120605 sec - gen_loss = 0.0, disc_loss = -1.0628010034561157\n",
            "Time for epoch 527 is 2.9992029666900635 sec - gen_loss = 0.0, disc_loss = -1.0484745502471924\n",
            "Time for epoch 528 is 3.058972120285034 sec - gen_loss = 0.0, disc_loss = -1.0775582790374756\n",
            "Time for epoch 529 is 2.911708116531372 sec - gen_loss = 0.0, disc_loss = -1.1414921283721924\n",
            "tf.Tensor(-87.31295, shape=(), dtype=float32)\n",
            "Time for epoch 530 is 4.178253412246704 sec - gen_loss = -10.914118766784668, disc_loss = -0.9588018655776978\n",
            "Time for epoch 531 is 2.9323318004608154 sec - gen_loss = 0.0, disc_loss = -1.011317491531372\n",
            "Time for epoch 532 is 2.960914134979248 sec - gen_loss = 0.0, disc_loss = -1.131879448890686\n",
            "Time for epoch 533 is 2.939626455307007 sec - gen_loss = 0.0, disc_loss = -1.1429363489151\n",
            "Time for epoch 534 is 2.974912643432617 sec - gen_loss = 0.0, disc_loss = -1.1597099304199219\n",
            "tf.Tensor(-79.52698, shape=(), dtype=float32)\n",
            "Time for epoch 535 is 4.104842662811279 sec - gen_loss = -9.940872192382812, disc_loss = -1.099220633506775\n",
            "Time for epoch 536 is 2.9220681190490723 sec - gen_loss = 0.0, disc_loss = -1.0265023708343506\n",
            "Time for epoch 537 is 2.9370899200439453 sec - gen_loss = 0.0, disc_loss = -1.1378921270370483\n",
            "Time for epoch 538 is 3.0328328609466553 sec - gen_loss = 0.0, disc_loss = -1.0911519527435303\n",
            "Time for epoch 539 is 3.024807929992676 sec - gen_loss = 0.0, disc_loss = -1.1400941610336304\n",
            "tf.Tensor(-73.72937, shape=(), dtype=float32)\n",
            "Time for epoch 540 is 4.289034366607666 sec - gen_loss = -9.216171264648438, disc_loss = -1.2123310565948486\n",
            "Time for epoch 541 is 2.9510338306427 sec - gen_loss = 0.0, disc_loss = -1.1270792484283447\n",
            "Time for epoch 542 is 2.9778366088867188 sec - gen_loss = 0.0, disc_loss = -0.9578657150268555\n",
            "Time for epoch 543 is 3.0087802410125732 sec - gen_loss = 0.0, disc_loss = -1.1833038330078125\n",
            "Time for epoch 544 is 2.9507062435150146 sec - gen_loss = 0.0, disc_loss = -0.9873024225234985\n",
            "tf.Tensor(-81.716965, shape=(), dtype=float32)\n",
            "Time for epoch 545 is 4.178644180297852 sec - gen_loss = -10.214620590209961, disc_loss = -1.1175721883773804\n",
            "Time for epoch 546 is 3.0262935161590576 sec - gen_loss = 0.0, disc_loss = -0.9683782458305359\n",
            "Time for epoch 547 is 2.8921780586242676 sec - gen_loss = 0.0, disc_loss = -1.0920101404190063\n",
            "Time for epoch 548 is 2.9628119468688965 sec - gen_loss = 0.0, disc_loss = -1.1132243871688843\n",
            "Time for epoch 549 is 3.0580990314483643 sec - gen_loss = 0.0, disc_loss = -1.1871049404144287\n",
            "tf.Tensor(-80.54263, shape=(), dtype=float32)\n",
            "Time for epoch 550 is 4.344376564025879 sec - gen_loss = -10.067829132080078, disc_loss = -1.184781789779663\n",
            "Time for epoch 551 is 3.001291036605835 sec - gen_loss = 0.0, disc_loss = -1.0630102157592773\n",
            "Time for epoch 552 is 3.0104005336761475 sec - gen_loss = 0.0, disc_loss = -1.1383047103881836\n",
            "Time for epoch 553 is 2.959202527999878 sec - gen_loss = 0.0, d10.067829132080078, disc_loss = -1.18478178977966\n",
            "\n",
            "\n",
            "I really don't know if I should beef up the generator or discriminator as both give me the same issue.\n",
            "\n",
            "I tried to make my generator twice as powerful as the discriminator\n",
            "\n",
            "-------------------\n",
            "(My answer is based mostly on Adam: A Method for Stochastic Optimization (the original Adam paper) and on the implementation of rmsprop with momentum in Tensorflow (which is operator() of struct ApplyRMSProp), as rmsprop is unpublished - it was described in a lecture by Geoffrey Hinton .)\n",
            "\n",
            "\n",
            "\n",
            "Some Background\n",
            "\n",
            "Adam and rmsprop with momentum are both methods (used by a gradient descent algorithm) to determine the step.\n",
            "\n",
            "Let $\\Delta x^{(t)}_j$ be the $j^{\\text{th}}$ component of the $t^{\\text{th}}$ step. Then:\n",
            "\n",
            "\n",
            "In Adam: $$\\Delta x_{j}^{(t)}=-\\frac{\\text{learning_rate}}{\\sqrt{\\text{BCMA}\\left(g_{j}^{2}\\right)}}\\cdot\\text{BCMA}\\left(g_{j}\\right)$$\n",
            "while:\n",
            "\n",
            "\n",
            "$\\text{learning_rate}$ is a hyperparameter.\n",
            "$\\text{BCMA}$ is short for \"bias-corrected (exponential) moving average\" (I made up the acronym for brevity).\n",
            "\n",
            "\n",
            "All of the moving averages I am going to talk about are exponential moving averages, so I would just refer to them as \"moving averages\".\n",
            "\n",
            "$g_j$ is the $j^{\\text{th}}$ component of the gradient, and so $\\text{BCMA}\\left(g_{j}\\right)$ is a bias-corrected moving average of the $j^{\\text{th}}$ components of the gradients that were calculated. Similarly, $\\text{BCMA}\\left(g_{j}^{2}\\right)$ is a bias-corrected moving average of the squares of the $j^{\\text{th}}$ components of the gradients that were calculated.\n",
            "For each moving average, the decay factor (aka smoothing factor) is a hyperparameter.\n",
            "Both the Adam paper and TensorFlow use the following notation:\n",
            "\n",
            "\n",
            "$\\beta_1$ is the decay factor for $\\text{BCMA}\\left(g_{j}\\right)$\n",
            "$\\beta_2$ is the decay factor for $\\text{BCMA}\\left(g^2_{j}\\right)$\n",
            "\n",
            "The denominator is actually $\\sqrt{\\text{BCMA}\\left(g_{j}^{2}\\right)}+\\epsilon$, while $\\epsilon$ is a small hyperparameter, but I would ignore it for simplicity.\n",
            "\n",
            "In rmsprop with momentum: $$\\Delta x_{j}^{\\left(t\\right)}=\\text{momentum_decay_factor}\\cdot\\Delta x_{j}^{\\left(t-1\\right)}-\\frac{\\text{learning_rate}}{\\sqrt{\\text{MA}\\left(g_{j}^{2}\\right)}}\\cdot g_{j}^{\\left(t\\right)}$$\n",
            "while:\n",
            "\n",
            "\n",
            "$\\text{momentum_decay_factor}$ is a hyperparameter, and I would assume it is in $(0,1)$ (as it usually is).\n",
            "In TensorFlow, this is the momentum argument of RMSPropOptimizer.\n",
            "$g^{(t)}_j$ is the $j^{\\text{th}}$ component of the gradient in the $t^{\\text{th}}$ step.\n",
            "$\\text{MA}\\left(g_{j}^{2}\\right)$ is a moving average of the squares of the $j^{\\text{th}}$ components of the gradients that were calculated.\n",
            "The decay factor of this moving average is a hyperparameter, and in TensorFlow, this is the decay argument of RMSPropOptimizer.\n",
            "\n",
            "\n",
            "\n",
            "High-Level Comparison\n",
            "\n",
            "Now we are finally ready to talk about the differences between the two.\n",
            "\n",
            "The denominator is quite similar (except for the bias-correction, which I explain about later).\n",
            "However, the momentum-like behavior that both share (Adam due to $\\text{BCMA}\\left(g_{j}\\right)$, and rmsprop with momentum due to explicitly taking a fraction of the previous step) is somewhat different.\n",
            "E.g. this is how Sebastian Ruder describes this difference in his blog post An overview of gradient descent optimization algorithms:\n",
            "\n",
            "\n",
            "  Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface [...]\n",
            "\n",
            "\n",
            "This description is based on the paper GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium, so check it out if you want to dive deeper.\n",
            "\n",
            "Next, I would describe 2 simple scenarios to demonstrate the difference in the momentum-like behaviors of the methods.\n",
            "\n",
            "Lastly, I would describe the difference with regard to bias-correction.\n",
            "\n",
            "\n",
            "\n",
            "Accumulating Momentum\n",
            "\n",
            "Consider the following scenario: The gradient was constant in every step in the recent past, and $\\Delta x_{j}^{(t-1)}=0$. Also, to keep it simple, $g_{j}&gt;0$.\n",
            "I.e. we can imagine our algorithm as a stationary ball on a linear slope.\n",
            "What would happen when we use each of the methods?\n",
            "\n",
            "\n",
            "\n",
            "In Adam\n",
            "\n",
            "The gradient was constant in the recent past, so $\\text{BCMA}\\left(g_{j}^{2}\\right)\\approx g_{j}^{2}$ and $\\text{BCMA}\\left(g_{j}\\right)\\approx g_j$.\n",
            "Thus we get:\n",
            "$$\\begin{gathered}\\\\\n",
            "\\Delta x_{j}^{(t)}=-\\frac{\\text{learning_rate}}{\\sqrt{g_{j}^{2}}}\\cdot g_{j}=-\\frac{\\text{learning_rate}}{|g_{j}|}\\cdot g_{j}\\\\\n",
            "\\downarrow\\\\\n",
            "\\Delta x_{j}^{\\left(t\\right)}=-\\text{learning_rate}\n",
            "\\end{gathered}\n",
            "$$\n",
            "\n",
            "I.e. the \"ball\" immediately starts moving downhill in a constant speed.\n",
            "\n",
            "In rmsprop with momentum\n",
            "\n",
            "Similarly, we get:\n",
            "$$\\Delta x_{j}^{\\left(t\\right)}=\\text{momentum_decay_factor}\\cdot\\Delta x_{j}^{\\left(t-1\\right)}-\\text{learning_rate}$$\n",
            "This case is a little more complicated, but we can see that:\n",
            "$$\\begin{gathered}\\\\\n",
            "\\Delta x_{j}^{\\left(t\\right)}=-\\text{learning_rate}\\\\\n",
            "\\Delta x_{j}^{\\left(t+1\\right)}=-\\text{learning_rate}\\cdot(1+\\text{momentum_decay_factor})\n",
            "\\end{gathered}\n",
            "$$\n",
            "So the \"ball\" starts accelerating downhill.\n",
            "Given that the gradient stays constant, you can prove that if:\n",
            "$$-\\frac{\\text{learning_rate}}{1-\\text{momentum_decay_factor}}&lt;\\Delta x_{j}^{\\left(k\\right)}$$\n",
            "then: $$-\\frac{\\text{learning_rate}}{1-\\text{momentum_decay_factor}}&lt;\\Delta x_{j}^{\\left(k+1\\right)}&lt;\\Delta x_{j}^{\\left(k\\right)}$$\n",
            "Therefore, we conclude that the step converges, i.e. $\\Delta x_{j}^{\\left(k\\right)}\\approx \\Delta x_{j}^{\\left(k-1\\right)}$ for some $k&gt;t$, and then:\n",
            "$$\\begin{gathered}\\\\\n",
            "\\Delta x_{j}^{\\left(k\\right)}\\approx \\text{momentum_decay_factor}\\cdot\\Delta x_{j}^{\\left(k\\right)}-\\text{learning_rate}\\\\\n",
            "\\downarrow\\\\\n",
            "\\Delta x_{j}^{\\left(k\\right)}\\approx -\\frac{\\text{learning_rate}}{1-\\text{momentum_decay_factor}}\n",
            "\\end{gathered}\n",
            "$$\n",
            "Thus, the \"ball\" accelerates downhill and approaches a speed $\\frac{1}{1-\\text{momentum_decay_factor}}$ times as large as the constant speed of Adam's \"ball\". (E.g. for a typical $\\text{momentum_decay_factor}=0.9$, it can approach $10\\times$ speed!)\n",
            "\n",
            "Changing Direction\n",
            "\n",
            "Now, consider a scenario following the previous one:\n",
            "After going down the slope (in the previous scenario) for quite some time (i.e. enough time for rmsprop with momentum to reach a nearly constant step size), suddenly a slope with an opposite and smaller constant gradient is reached.\n",
            "What would happen when we use each of the methods?\n",
            "\n",
            "\n",
            "\n",
            "This time I would just describe the results of my simulation of the scenario (my Python code is at the end of the answer).\n",
            "Note that I have chosen for Adam's $\\text{BCMA}\\left(g_{j}\\right)$ a decay factor equal to $\\text{momentum_decay_factor}$. Choosing differently would have changed the following results:\n",
            "\n",
            "\n",
            "Adam is slower to change its direction, and then much slower to get back to the minimum.\n",
            "However, rmsprop with momentum reaches much further before it changes direction (when both use the same $\\text{learning_rate}$).\n",
            "Note that this further reach is because rmsprop with momentum first reaches the opposite slope with much higher speed than Adam. If both reached the opposite slope with the same speed (which would happen if Adam's $\\text{learning_rate}$ were $\\frac{1}{1-\\text{momentum_decay_factor}}$ times as large as that of rmsprop with momentum), then Adam would reach further before changing direction.\n",
            "\n",
            "\n",
            "Bias-Correction\n",
            "\n",
            "What do we mean by a biased/bias-corrected moving average? (Or at least, what does the Adam paper mean by that?)\n",
            "\n",
            "Generally speaking, a moving average is a weighted average of:\n",
            "\n",
            "\n",
            "The moving average of all of the previous terms\n",
            "The current term\n",
            "\n",
            "\n",
            "Then what is the moving average in the first step?\n",
            "\n",
            "\n",
            "A natural choice for a programmer would be to initialize the \"moving average of all of the previous terms\" to $0$. We say that in this case the moving average is biased towards $0$.\n",
            "When you only have one term, by definition the average should be equal to that term.\n",
            "Thus, we say that the moving average is bias-corrected in case the moving average in the first step is the first term (and the moving average works as usual for the rest of the steps).\n",
            "\n",
            "\n",
            "So here is another difference: The moving averages in Adam are bias-corrected, while the moving average in rmsprop with momentum is biased towards $0$.\n",
            "\n",
            "For more about the bias-correction in Adam, see section 3 in the paper and also this answer.\n",
            "\n",
            "Simulation Python Code\n",
            "\n",
            "\n",
            "\n",
            "import numpy as np\n",
            "import matplotlib.pyplot as plt\n",
            "from matplotlib.animation import FuncAnimation\n",
            "\n",
            "###########################################\n",
            "# script parameters\n",
            "def f(x):\n",
            "    if x &gt; 0:\n",
            "        return x\n",
            "    else:\n",
            "        return -0.1 * x\n",
            "\n",
            "def f_grad(x):\n",
            "    if x &gt; 0:\n",
            "        return 1\n",
            "    else:\n",
            "        return -0.1\n",
            "\n",
            "METHOD_TO_LEARNING_RATE = {\n",
            "    'Adam': 0.01,\n",
            "    'GD': 0.00008,\n",
            "    'rmsprop_with_Nesterov_momentum': 0.008,\n",
            "    'rmsprop_with_momentum': 0.001,\n",
            "    'rmsprop': 0.02,\n",
            "    'momentum': 0.00008,\n",
            "    'Nesterov': 0.008,\n",
            "    'Adadelta': None,\n",
            "    }\n",
            "X0 = 2\n",
            "METHOD = 'rmsprop'\n",
            "METHOD = 'momentum'\n",
            "METHOD = 'GD'\n",
            "METHOD = 'rmsprop_with_Nesterov_momentum'\n",
            "METHOD = 'Nesterov'\n",
            "METHOD = 'Adadelta'\n",
            "METHOD = 'rmsprop_with_momentum'\n",
            "METHOD = 'Adam'\n",
            "LEARNING_RATE = METHOD_TO_LEARNING_RATE[METHOD]\n",
            "\n",
            "MOMENTUM_DECAY_FACTOR = 0.9\n",
            "RMSPROP_SQUARED_GRADS_AVG_DECAY_FACTOR = 0.9\n",
            "ADADELTA_DECAY_FACTOR = 0.9\n",
            "RMSPROP_EPSILON = 1e-10\n",
            "ADADELTA_EPSILON = 1e-6\n",
            "ADAM_EPSILON = 1e-10\n",
            "ADAM_SQUARED_GRADS_AVG_DECAY_FACTOR = 0.999\n",
            "ADAM_GRADS_AVG_DECAY_FACTOR = 0.9\n",
            "\n",
            "INTERVAL = 9e2\n",
            "INTERVAL = 1\n",
            "INTERVAL = 3e2\n",
            "INTERVAL = 3e1\n",
            "###########################################\n",
            "\n",
            "def plot_func(axe, f):\n",
            "    xs = np.arange(-X0 * 0.5, X0 * 1.05, abs(X0) / 100)\n",
            "    vf = np.vectorize(f)\n",
            "    ys = vf(xs)\n",
            "    return axe.plot(xs, ys, color='grey')\n",
            "\n",
            "def next_color(color, f):\n",
            "    color[1] -= 0.01\n",
            "    if color[1] &lt; 0:\n",
            "        color[1] = 1\n",
            "    return color[:]\n",
            "\n",
            "def update(frame):\n",
            "    global k, x, prev_step, squared_grads_decaying_avg, \\\n",
            "           squared_prev_steps_decaying_avg, grads_decaying_avg\n",
            "\n",
            "    if METHOD in ('momentum', 'Nesterov', 'rmsprop_with_momentum',\n",
            "                  'rmsprop_with_Nesterov_momentum'):\n",
            "        step_momentum_portion = MOMENTUM_DECAY_FACTOR * prev_step\n",
            "    if METHOD in ('Nesterov', 'rmsprop_with_Nesterov_momentum'):\n",
            "        gradient = f_grad(x + step_momentum_portion)\n",
            "    else:\n",
            "        gradient = f_grad(x)\n",
            "\n",
            "    if METHOD == 'GD':\n",
            "        step = -LEARNING_RATE * gradient\n",
            "    elif METHOD in ('momentum', 'Nesterov'):\n",
            "        step = step_momentum_portion - LEARNING_RATE * gradient\n",
            "    elif METHOD in ('rmsprop', 'rmsprop_with_momentum',\n",
            "                    'rmsprop_with_Nesterov_momentum'):\n",
            "        squared_grads_decaying_avg = (\n",
            "            RMSPROP_SQUARED_GRADS_AVG_DECAY_FACTOR * squared_grads_decaying_avg +\n",
            "            (1 - RMSPROP_SQUARED_GRADS_AVG_DECAY_FACTOR) * gradient ** 2)\n",
            "        grads_rms = np.sqrt(squared_grads_decaying_avg + RMSPROP_EPSILON)\n",
            "        if METHOD == 'rmsprop':\n",
            "            step = -LEARNING_RATE / grads_rms * gradient\n",
            "        else:\n",
            "            assert(METHOD in ('rmsprop_with_momentum',\n",
            "                              'rmsprop_with_Nesterov_momentum'))\n",
            "            print(f'LEARNING_RATE / grads_rms * gradient: {LEARNING_RATE / grads_rms * gradient}')\n",
            "            step = step_momentum_portion - LEARNING_RATE / grads_rms * gradient\n",
            "    elif METHOD == 'Adadelta':\n",
            "        gradient = f_grad(x)\n",
            "        squared_grads_decaying_avg = (\n",
            "            ADADELTA_DECAY_FACTOR * squared_grads_decaying_avg +\n",
            "            (1 - ADADELTA_DECAY_FACTOR) * gradient ** 2)\n",
            "        grads_rms = np.sqrt(squared_grads_decaying_avg + ADADELTA_EPSILON)\n",
            "        squared_prev_steps_decaying_avg = (\n",
            "            ADADELTA_DECAY_FACTOR * squared_prev_steps_decaying_avg +\n",
            "            (1 - ADADELTA_DECAY_FACTOR) * prev_step ** 2)\n",
            "        prev_steps_rms = np.sqrt(squared_prev_steps_decaying_avg + ADADELTA_EPSILON)\n",
            "        step = - prev_steps_rms / grads_rms * gradient\n",
            "    elif METHOD == 'Adam':\n",
            "        squared_grads_decaying_avg = (\n",
            "            ADAM_SQUARED_GRADS_AVG_DECAY_FACTOR * squared_grads_decaying_avg +\n",
            "            (1 - ADAM_SQUARED_GRADS_AVG_DECAY_FACTOR) * gradient ** 2)\n",
            "        unbiased_squared_grads_decaying_avg = (\n",
            "            squared_grads_decaying_avg /\n",
            "            (1 - ADAM_SQUARED_GRADS_AVG_DECAY_FACTOR ** (k + 1)))\n",
            "        grads_decaying_avg = (\n",
            "            ADAM_GRADS_AVG_DECAY_FACTOR * grads_decaying_avg +\n",
            "            (1 - ADAM_GRADS_AVG_DECAY_FACTOR) * gradient)\n",
            "        unbiased_grads_decaying_avg = (\n",
            "            grads_decaying_avg /\n",
            "            (1 - ADAM_GRADS_AVG_DECAY_FACTOR ** (k + 1)))\n",
            "        step = - (LEARNING_RATE /\n",
            "                  (np.sqrt(unbiased_squared_grads_decaying_avg) + ADAM_EPSILON) *\n",
            "                  unbiased_grads_decaying_avg)\n",
            "\n",
            "    x += step\n",
            "    prev_step = step\n",
            "    k += 1\n",
            "\n",
            "    color = next_color(cur_color, f)\n",
            "\n",
            "    print(f'k: {k}\\n'\n",
            "          f'x: {x}\\n'\n",
            "          f'step: {step}\\n'\n",
            "          f'gradient: {gradient}\\n')\n",
            "\n",
            "    k_x_marker, = k_and_x.plot(k, x, '.', color=color)\n",
            "    x_y_marker, = x_and_y.plot(x, f(x), '.', color=color)\n",
            "\n",
            "    return k_x_marker, x_y_marker\n",
            "\n",
            "k = 0\n",
            "x = X0\n",
            "cur_color = [0, 1, 1]\n",
            "prev_step = 0\n",
            "squared_grads_decaying_avg = 0\n",
            "squared_prev_steps_decaying_avg = 0\n",
            "grads_decaying_avg = 0\n",
            "\n",
            "fig, (k_and_x, x_and_y) = plt.subplots(1, 2, figsize=(9,5))\n",
            "k_and_x.set_xlabel('k')\n",
            "k_and_x.set_ylabel('x', rotation=0)\n",
            "x_and_y.set_xlabel('x')\n",
            "x_and_y.set_ylabel('y', rotation=0)\n",
            "plot_func(x_and_y, f)\n",
            "x_and_y.plot(x, f(x), '.', color=cur_color[:])\n",
            "k_and_x.plot(k, x, '.', color=cur_color[:])\n",
            "plt.tight_layout()\n",
            "\n",
            "ani = FuncAnimation(fig, update, blit=False, repeat=False, interval=INTERVAL)\n",
            "plt.show()\n",
            "\n",
            "\n",
            "-------------------\n",
            "So I think there are a few concepts being mixed up in your question, I will do my best to address them one by one.\n",
            "The &quot;weight matrix&quot; you refer to (if it's the same one as Aneesh Joshi's post) is related to &quot;attention&quot;, a concept in Deep Learning that, in simple terms, changes the priority given to different parts of the input (quite literally like human attention) depending on how it is configured. This is not necessarily a the weight of a single neuron in a neural network, so I will come back to this once I have cleared up the other concepts.\n",
            "To understand Transformers and NLP models, we should go back to just basic neural networks, and to understand basic neural networks we should understand what a neuron is, how it works, and how it is trained.\n",
            "History break: basically for a long time (in particular popularity since the 1800s) a lot of modelling was done using linear models and linear regression (you see the effects of this in classical statistics/econometrics). However, as the name implies, this only captures linear relationships. There were two questions that led to resolving this:\n",
            "(1) how do we capture non-linear relationships?\n",
            "(2) how do we do so automatically in an algorithmic manner?\n",
            "For the first one, many approaches existed, such as Logistic regression. You might notice that in logistic regression, there is still a linear equation that is being estimated, just not as the final output. This disconnect between the input linear equation and output non-linear equation is now known as having an input Linear cell with an Activation function. In logistic regression, the Activation function is the logistic function, whereas in linear regression it is just the Identity function.\n",
            "In case it is still not clear, imagine this: Input -&gt; Linear Function -&gt; Activation Function -&gt; Output.\n",
            "Putting that entire sequence together gives you the Perceptron (first introduced in the 1940s). Methods of optimizing this were done via gradient descent and various algorithms. Gradient descent is probably the most important to keep in mind and helps us answer the second question.\n",
            "Essentially what you are trying to do in any form of automated model development, is you have some linear equation (i.e. $y = \\beta\\times w + \\beta_{0}$), which you feed an input, pass through an activation function (i.e. Identity, sigmoid, ReLu, tanh, etc), then get an output. And you want that output to match a value that you already know (in statistics imagine $y$ and $y_{hat}$. In order to do that (at least in the case of a continuous target class) you need to know how far off is your prediction from the true value. You do this by taking the difference. However the difference can be positive/negative, so usually we have some way of making this semi-definite positive by either squaring (Mean Squared Error) or taking the absolute value (Mean Absolute Error). This is known as our loss function. Essentially we would have developed a good estimator/predictor/model if our loss is 0 (which means that our prediction matches our target value).\n",
            "Linear regression solves this using Maximum Likelihood Estimation. Gradient descent effectively is an iterative algorithm that does the following. First we take the derivative of our loss function with respect to one of our model weights (i.e. w) and we apply something similar to the update that happens in the Newton-Raphson method, namely $w_{new} = w_{old} - step\\times\\frac{dL}{dw}$ where step is a step-size (either fixed or changing). A stopping condition is typically if your change from the new weight to the old weight is sufficiently small you stop updating as you have reached a minimum. In a convex scenario this would also be your global minimum, however classically you have no way of knowing this, which is why multiple algorithms use some approaches like random starts (multiple starting points randomly generated) to try and avoid getting stuck in local minima (aka a loss value that is low but not the lowest it could be).\n",
            "Ok so if you have read that take a quick stretch and process it since I am not entirely sure of the reader's background so it may have been a lot to process or fairly straightforward. So far we covered how to capture non-linear relationships and how to do it in an automated way. So does a single neuron train like that? Yes. Do a collection of neurons train like that? No.\n",
            "Recall that a neuron is just a linear function with an activation function on top that performs some form of gradient descent. However a neural network is a chain of these, sometimes interconnecting, etc. So then how do we get a derivative of the loss which is at the end of a network to the very beginning. Through a technique that took off in the mid-1980s the technique called backpropagation (as a rediscovery of techniques dating back to the 1960s). Essentially we take partial derivatives, and then through an application of the chain rule are easily able to propagate various portions of the loss backwards, updating our weights along the way. This is all done in a single pass of training, and thankfully, automatically. The classical approach is to feed in your entire dataset THEN take the loss gradient, known as Gradient Descent. Or you can feed only a single data point before updating, known as Stochastic Gradient Descent. There is, of course a middle ground, thanks to GPUs, taking a batch of points known as Batch learning.\n",
            "A question that might have come up is, is the derivative of our loss function just linear? No, because the activation can be non-linear, so essentially you are taking the derivatives of these non-linear functions, and that can be computationally expensive. The popular choice today is ReLU (Rectified Linear Unit) which, for some linear output $y$ is basically a max function saying $output = max(y,0)$, that's it. Things like weight initialization make more of an impact as performance across different non-linear activation functions are pretty comparable.\n",
            "Ok so we covered how a neuron works and how a neural network optimizes its weights in a single pass. A side note is you will often hear &quot;train/validation/test&quot; set, which are basically splits of your dataset. You split your dataset prior into a training subset, for, well, training, which is where you modify the weights through the process I described above for the entire training set or each data point (or batches of data points if you are using GPUs/batches in Deep Learning). Usually your data might need to be transformed depending on the functions you are using, and the mistake most practitioners make is pre-processing their data on the entire dataset, whereas the statistically correct way of doing so is on the training set only, and extrapolating to the validation/test set using that (since in the real world you may not have all the information). The validation set is there for you to test out different hyper-parameters (like step-size above, otherwise known as learning rate) or just compare different models. The test set is the final set that you use to truly see the quality of your model, after you have finished optimizing/training above.\n",
            "Now we can finally get to your question on attention. As I described a basic neural network, you may have noticed that it receives an input once, does a run through, and then optimizes. Well what if we want to get inputs that require some processing? Like images? Well this is where different architectures come up (and you can read this all over the web, I recommend D2L.ai or FastAI's free course), and for images the common one are Convolutional Neural Networks, which were useful in capturing reoccurring patterns and spatial locality.\n",
            "Sometimes we might want more than one input, aka a previous input influencing how we process this next input, this is where temporal based architectures come in like Recurrent Neural Networks, and this was what was initially used for languages, but chaining a bunch of neurons takes a while to process since we can't parallelize the operations (which is where the speed of neural network training comes from). Plus you would have to have some way of pre-processing your language input, such as converting everything to lowercase, removing non-useful words, tokenizing different parts of words, etc, depending on the task.\n",
            "There was quite a lot to deal with. Up until a few years ago when Attention based models came out called Transformers, and they have pretty much influenced all aspects of Deep Learning by allowing parallelization but also capturing interactions between inputs. A foundational approach is to pre-process inputs using an attention matrix (as you have mentioned). The attention matrix has a bit of complexity in terms of the math (Neuromatch's course covers this well), but to simplify it, it is effectively the shared context between two inputs (one denoted by the columns, the other the rows), like two sentences. The way this is trained (and it depends on the model) is generally by taking an input and converting it into a numerical representation (otherwise known as an embedding) and outer-multiplying these two vectors to produce a symmetric matrix, which obviously has the strongest parts on the main diagonal.\n",
            "The idea here is to then zero out the main diagonal, and then train the network to try and use the remaining cells to fill this main diagonal in (you can once again do this via a neural network and the process described above). Then you can apply this mechanism with two different inputs. In a translation task for example, you would have a sentence in language A and another in B, and their matrix would highlight the shared context, which may not be symmetric depending on the word order/structure, etc. Transformers are not like Recurrent Neural Networks in that they take an input at once and then give an output, but compromises exist and pre-training models is also a thing.\n",
            "There is a lot I haven't touched upon, but the resources I mentioned should be a useful starting point, and I wish you luck in your Data Science journey!\n",
            "\n",
            "-------------------\n",
            "It's probably possible.  I'll suggest a few plausibly-practical methods, starting from very crude (but probably not super-effective) to more complex (might be more effective but might be computationally expensive).  I expect that many of these approaches might be relatively slow to train the classifier but doable and worth a try.  But first, let me try to formalize the  problem more precisely.\n",
            "\n",
            "Theory\n",
            "\n",
            "In machine learning it's often helpful to formulate your problem by identifying a loss function.  Then you can formulate your problem as finding $f$ that minimizes the loss function.  In your case, I think a natural loss function would be that $L(f,g)$ counts the number of partitions $p$ such that conditions (2) or (3) are violated.  You could of course add a regularization term as well.\n",
            "\n",
            "Once you've defined a loss function, your problem becomes: given $g$, find $f \\in \\mathcal{H}$ that minimizes $L(f,g)$, where $\\mathcal{H}$ is the hypothesis space (the space of valid/legal models).\n",
            "\n",
            "Let's start with a theoretical thought experiment.  Suppose you didn't care about the computational time to train a classifier, and just want to find a valid classifier $f$.  Let $\\mathcal{H}$ be the hypothesis space, i.e., the set of allowable functions.  (For instance, for a linear SVM, $\\mathcal{H}$ would be the set of SVM weights -- i.e., the set of linear decision boundaries.)  If you didn't care about computational time, you could just enumerate all $f$ to find one that minimizes $L(f,g)$ -- so this proves that the problem is well-defined.\n",
            "\n",
            "More generally, suppose we have a space of classifiers $f$ that output probabilities instead of hard labels (think: logistic regression).  Then we can define a loss function based on the cross-entropy loss:\n",
            "\n",
            "$$L(f,g) = \\sum_{p : g(p)=0} \\max \\{-\\log(1-f(x)) : x \\in p\\} + \\sum_{p : g(p)=1} \\min \\{-\\log(f(x)) : x \\in p\\}$$\n",
            "\n",
            "You could add a regularization term here, too.\n",
            "\n",
            "Of course, other loss functions are possible as well; but the point is that once you have a loss function, then classification becomes well-defined as an optimization problem.\n",
            "\n",
            "Plausible method: one-class classifier\n",
            "\n",
            "One crude approach is to use a one-class classifier.  For each partition $p$ such that $g(p)=0$, you obtain many instances $x$ where you want to have $f(x)=0$.  So, you could add each one of those to a training set, and then train a one-class classifier on it.\n",
            "\n",
            "This is simple and easy to implement.  However, I suspect its accuracy might be low.\n",
            "\n",
            "Plausible method: active learning\n",
            "\n",
            "Suppose you have a way to obtain labelled instances if you need, through manual training.  In particular: let's assume that given a partition $p$, if you really needed to, you could construct hard labels for instances $x \\in p$, and specifically, if $g(p)=1$, you could find an instance $x \\in p$ that should receive the label $1$.  This manual-labelling process might be difficult and tedious and time-consuming/expensive, so not feasible to do on a large scale, but let's suppose you could do it on a small scale if needed.\n",
            "\n",
            "Then you could use active learning:\n",
            "\n",
            "\n",
            "Let $f$ be an arbitrary classifier (maybe trained using a one-class classifier).  Let $T$ be an initial training set, initially empty.\n",
            "Repeat until convergence:\n",
            "\n",
            "a. Pick a partition $p$ such that $f(x)$ is wrong for some $x \\in p$.\n",
            "\n",
            "b. If $g(p)=0$, add find $x \\in p$ such that $f(x)=1$, and add $(x,0)$ to $T$.\n",
            "\n",
            "c. If $g(p)=1$, use the manual-labelling process to find some $x \\in p$ that should be labelled $1$ and add $(x,1)$ to $T$.\n",
            "\n",
            "d. Train a new boolean classifier $f$ using standard supervised learning methods on the training set $T$.\n",
            "\n",
            "\n",
            "There are various optimizations you could use to break ties in step 2c (in hopes of reducing the amount of manual labelling); e.g., if you're using the cross-entropy loss defined above, you could choose the $p$ that contributes the most to the total loss for $f$.  Or, you could choose the $p$ that is most \"different\" from all previously considered $p$ with $g(p)=1$.  Or, you might be able to adapt other active learning methods.\n",
            "\n",
            "This might work.  However, it does require the ability to do some manual labelling of individual instances, which is more than you promised us in the original problem statement.\n",
            "\n",
            "Plausible approach: direct optimization\n",
            "\n",
            "Another possible approach would be to choose a differentiable classifier, and the optimize the (cross-entropy) loss $L(f,g)$ defined above directly using gradient descent.  For instance, you could choose $f$ to be a logistic regression or neural network classifier, as these are differentiable.  Now, you can compute the gradient of $L(f_\\theta,g)$ with respect to the parameters $\\theta$ of the model (e.g., for logistic regression, $\\theta$ are the weights) and then apply gradient descent to find the model parameters $\\theta$ that minimize $L(f_\\theta,g)$.\n",
            "\n",
            "This may get a little messy, as loss function $L$ contains $\\min$ and $\\max$ functions, which have points where they are non-differentiable.  If you don't do anything special, this can cause oscillation around the non-differentiable boundary.  One possible trick is to replace each $\\min$ by a softened version of the min, and replace each $\\max$ by a softened version of the max, so that the loss function becomes differentiable.\n",
            "\n",
            "Alternatively, you could adjust ordinary gradient descent to behave well around them.  One standard technique is to reduce the step size when you get near the boundary to avoid stepping across the boundary, or to use projected gradient descent.  Thus, if we have a term\n",
            "\n",
            "$$\\ell(\\theta) = \\min(\\ell_1(\\theta),\\ell_2(\\theta))$$\n",
            "\n",
            "and we compute the gradient $\\nabla \\ell(\\theta)$, the gradient will be based on whichever of $\\ell_1(\\theta),\\ell_2(\\theta)$ is smaller for this particular value of $\\theta$.  Suppose the gradient is $g = \\nabla \\ell(\\theta)$.  Ordinary gradient descent would take a step in the direction $g$, say adjusting $\\theta$ to $\\theta - \\lambda \\cdot g$ for some small constant $\\lambda$.  You could additionally require that the step avoid crossing the boundary line $\\ell_1(\\theta)=\\ell_2(\\theta)$.  For instance, if $\\ell_1(\\theta)&lt;\\ell_2(\\theta)$ for this particular value of $\\theta$, we'll have $g = \\nabla \\ell(\\theta) = \\nabla \\ell_1(\\theta)$.  So, take a step $- \\lambda' \\cdot g$, but constrain $\\lambda'$ to be in the range $0 \\le \\lambda' \\le \\lambda$ and to satisfy $\\ell_1(\\theta - \\lambda' \\cdot g) \\le \\ell_2(\\theta - \\lambda' \\cdot g)$ -- i.e., don't step across the other side of the discontinuity.\n",
            "\n",
            "Note that using ordinary gradient descent will probably be very slow, as each evaluation of the loss function requires evaluating $f$ on every point in $X$: $O(|X|)$ time.  You'll probably do better by using stochastic gradient descent: in each iteration, pick a single partition $p$, compute the gradient of the term in $L(f,g)$ for $p$, and move in the direction of that gradient.  Or, pick a minibatch of 20 partitions $p$ and compute the gradient of the sum of those 20 terms.\n",
            "\n",
            "Plausible method: boosting\n",
            "\n",
            "Another reasonable method would be to use boosting with a very simple classifier, such as a decision stump.\n",
            "\n",
            "A decision stump is a decision tree with a single level: you pick one feature, compare it to some threshold, and make a binary decision based on whether the value of that feature is above or below the threshold.  Decision stumps are convenient because you can enumerate all candidate stumps -- all possible combinations of a feature and a threshold -- and evaluate the loss function for each.  (In practice, as an optimization, we don't enumerate all possible stumps.  Rather, for a particular feature, we randomly pick 100 candidate thresholds by sampling 100 times from the training set, and we compute the loss for each candidate threshold; we do this for each feature, and take the best overall combination.)\n",
            "\n",
            "You could use AdaBoost directly with decision stumps and minimize the loss function defined above, and you might get a reasonable classifier.\n",
            "\n",
            "For even better results, you could use gradient boosting.  This is a slight generalization where each leaf of each stump outputs a continuous value rather than a boolean; the classifier's output is obtained by summing the value from each stump, and then checking whether the sum is positive or negative.  The general methodology can be applied with any differentiable loss function, so you could apply that methodology to the cross-entropy loss function defined above.\n",
            "\n",
            "Plausible method: incremental optimization\n",
            "\n",
            "Here's another approach that can be applied to classifiers that aren't differentiable.\n",
            "In my algorithm, I will maintain a tentative training set $T \\subseteq X \\times \\{0,1\\}$ that assigns a tentative label to some of the elements of $X$.  The training set $T$ will evolve throughout the algorithm, but it will always remain consistent with $g$, defined as follows:\n",
            "\n",
            "We say that $T$ is consistent with $g$ if, (a) for every $p$ such that $g(p)=0$, we have $(x,0) \\in T$ for all $x \\in p$, and (b) for every $p$ such that $g(p)=1$, there exists at least one $x \\in p$ such that $(x,1) \\in T$.\n",
            "\n",
            "The training set $T$ specifies one possible labelling that would be consistent with $g$.  It's not necessarily the right one; it's just one possibility.\n",
            "\n",
            "At each step, the algorithm uses $T$ as labels to train a classifier $f$ using standard supervised learning methods and computes $f$'s loss on $T$, then adjusts $T$ to reduce the loss of $f$ on $T$.  Define $\\text{train}(T)$ to be the classifier $f$ obtained by training on the training set $T$, using some boolean classifier of your choice.  Here is the algorithm:\n",
            "\n",
            "\n",
            "Set $T$ to be any training set that is consistent with $g$.  Compute $f := \\text{train}(T)$ and $\\ell = L(f,g)$.\n",
            "Repeat until convergence:\n",
            "\n",
            "a. Enumerate all training sets $T'$ that differ from $T$ in only one element yet remains consistent with $g$, and for each train a classifier and compute the loss.  Pick the $T'$ that minimizes $L(\\text{train}(T'),g)$.\n",
            "\n",
            "b. Set $T := T'$.\n",
            "\n",
            "\n",
            "Step 2a can be done by enumerating all partitions $p$ such that $g(p)=1$. Suppose $T$ includes $(x,1)$ where $x \\in p$.  Then enumerate all $x' \\in p$ and consider $T' = T \\setminus \\{(x,1)\\} \\cup \\{(x',1)\\}$.  There are at most $|X|$ such possibilities, and you have to train the classifier on each, so each iteration of the loop might be pretty slow... but one could hope that it will eventually converge on a reasonable solution.\n",
            "\n",
            "Basically, this tried to maintain some tentative labels that are consistent with your $g$, and that also are consistent with the regularity conditions imposed by the classifier $f$ (e.g., linear separability, or whatever), incrementally adjusting it at each step of the iteration.\n",
            "\n",
            "For your particular parameter values ($|X|=10^5$, $|P|=10^3$), I suspect this will be too slow.  But you could try experimenting with the approach for scaled-down versions of your problem, with a smaller feature space ($|X|$ is smaller) and see if it seems to lead to useful results.\n",
            "\n",
            "If this algorithm gives useful results, there many heuristics and optimizations you could use to speed up convergence:\n",
            "\n",
            "\n",
            "You could probably make a smarter choice for the initial value of $T$ and speed up convergence, by applying a one-class classifier to the set of 0-labelled instances (since the instances of the form $(x,0) \\in T$ never change throughout the algorithm) and using that to choose tentative initial label.\n",
            "You could use something akin to stochastic gradient descent to speed convergence: in step 2a, pick a single $p$ (or a small minibatch of $p$'s) and  consider just training sets $T'$ that differ from $T$ only in $p$ (but not in other partitions.\n",
            "You could try using an online/incremental training method for training $f$.  Since $T'$ is so similar to $T$, rather than training a new classifier from scratch, you might be able to reuse much of the effort from training on $T$ to more quickly compute $\\text{train}(T')$ given $\\text{train}(T)$.\n",
            "Depending on the nature of the feature space and the partition function, you might be able to solve the following problem more efficiently than brute force: given a training set $T$ and a partition $p$ such that $g(p)=1$ and an $x \\in p$ such that $(x,1) \\in T$, find some $x' \\in p$ such that minimizes the loss when training on $T' = T \\setminus \\{(x,1)\\} \\cup \\{(x',1)\\}$.  For instance, with logistic regression, you might be able to train a classifier $f_0$ on $T \\setminus \\{(x,1)\\}$, then classify each $x' \\in p$ using $f_0$ and rank them by the probability score that logistic regression outputs.  This is a heuristic and not guaranteed to find the optimal $x'$, but it's also a lot more efficient than enumerating all candidates for $T'$ and training a classifier on each.\n",
            "\n",
            "\n",
            "A useful subroutine\n",
            "\n",
            "Many of the above schemes require solving the following subproblem:\n",
            "\n",
            "Given a partition $p$ and a classifier $f$, find $x \\in p$ such that $f(x)=1$, or report that none exists.\n",
            "\n",
            "If the classifier outputs confidence/probability scores, then the subproblem is:\n",
            "\n",
            "Given a partition $p$ and a classifier $f$, find $x \\in p$ that maximizes $f(x)$.\n",
            "\n",
            "The naive way to solve this subproblem is to enumerate all $x \\in p$.  Depending on the particular classifier you choose and the structure of the partitions, there may be more efficient algorithms.\n",
            "\n",
            "For instance, if we have a linear classifier, maximizing $f(x)$ becomes equivalent to maximizing $x \\cdot w$ for some known vector $w$.  If $p$ has a simple structure, there might be faster ways to solve this optimization problem.  For instance, if $p$ is a convex region defined by linear inequalities, you can maximize $x \\cdot w$ subject to $x \\in p$ using linear programming.\n",
            "\n",
            "And you can use a fast algorithm for this subproblem to speed up the training process, for many of the methods sketched above.  Whether that is possible will depend on the specific type of classifier you choose and the structure of the partitions, but if you're trying to do this in practice, I recommend you ask a separate question about how to solve this subproblem for the particular type of classifier and partition structure that arises in your application -- that might enable significant computational optimizations.\n",
            "\n",
            "-------------------\n",
            "A similar question was asked on CV: Comprehensive list of activation functions in neural networks with pros/cons.\n",
            "\n",
            "I copy below one of the answers:\n",
            "\n",
            "\n",
            "  One such a list, though not much exhaustive:\n",
            "  http://cs231n.github.io/neural-networks-1/\n",
            "  \n",
            "  Commonly used activation functions\n",
            "  \n",
            "  Every activation function (or non-linearity) takes a single number\n",
            "  and performs a certain fixed mathematical operation on it. There are\n",
            "  several activation functions you may encounter in practice:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "           Left: Sigmoid non-linearity\n",
            "  squashes real numbers to range between [0,1] Right: The tanh\n",
            "  non-linearity squashes real numbers to range between [-1,1].\n",
            "  \n",
            "  \n",
            "  Sigmoid. The sigmoid non-linearity has the mathematical form $\\sigma(x) = 1 / (1 + e^{-x})$ and is shown in the image above on\n",
            "  the left. As alluded to in the previous section, it takes a\n",
            "  real-valued number and \"squashes\" it into range between 0 and 1. In\n",
            "  particular, large negative numbers become 0 and large positive numbers\n",
            "  become 1. The sigmoid function has seen frequent use historically\n",
            "  since it has a nice interpretation as the firing rate of a neuron:\n",
            "  from not firing at all (0) to fully-saturated firing at an assumed\n",
            "  maximum frequency (1). In practice, the sigmoid non-linearity has\n",
            "  recently fallen out of favor and it is rarely ever used. It has two\n",
            "  major drawbacks: \n",
            "  \n",
            "  \n",
            "  Sigmoids saturate and kill gradients. A very undesirable property of the sigmoid neuron is that when the neuron's activation\n",
            "  saturates at either tail of 0 or 1, the gradient at these regions is\n",
            "  almost zero. Recall that during backpropagation, this (local) gradient\n",
            "  will be multiplied to the gradient of this gate's output for the whole\n",
            "  objective. Therefore, if the local gradient is very small, it will\n",
            "  effectively \"kill\" the gradient and almost no signal will flow through\n",
            "  the neuron to its weights and recursively to its data. Additionally,\n",
            "  one must pay extra caution when initializing the weights of sigmoid\n",
            "  neurons to prevent saturation. For example, if the initial weights are\n",
            "  too large then most neurons would become saturated and the network\n",
            "  will barely learn.\n",
            "  Sigmoid outputs are not zero-centered. This is undesirable since neurons in later layers of processing in a Neural Network (more on\n",
            "  this soon) would be receiving data that is not zero-centered. This has\n",
            "  implications on the dynamics during gradient descent, because if the\n",
            "  data coming into a neuron is always positive (e.g. $x &gt; 0$\n",
            "  elementwise in $f = w^Tx + b$)), then the gradient on the weights\n",
            "  $w$ will during backpropagation become either all be positive, or\n",
            "  all negative (depending on the gradient of the whole expression\n",
            "  $f$). This could introduce undesirable zig-zagging dynamics in the\n",
            "  gradient updates for the weights. However, notice that once these\n",
            "  gradients are added up across a batch of data the final update for the\n",
            "  weights can have variable signs, somewhat mitigating this issue.\n",
            "  Therefore, this is an inconvenience but it has less severe\n",
            "  consequences compared to the saturated activation problem above.\n",
            "  \n",
            "  \n",
            "  Tanh. The tanh non-linearity is shown on the image above on the right. It squashes a real-valued number to the range [-1, 1]. Like the\n",
            "  sigmoid neuron, its activations saturate, but unlike the sigmoid\n",
            "  neuron its output is zero-centered. Therefore, in practice the tanh\n",
            "  non-linearity is always preferred to the sigmoid nonlinearity. Also\n",
            "  note that the tanh neuron is simply a scaled sigmoid neuron, in\n",
            "  particular the following holds: $ \\tanh(x) = 2 \\sigma(2x) -1  $.\n",
            "  \n",
            "  \n",
            "  \n",
            "           Left: Rectified Linear\n",
            "  Unit (ReLU) activation function, which is zero when x &lt 0 and then\n",
            "  linear with slope 1 when x &gt 0. Right: A plot from Krizhevsky\n",
            "  et al. (pdf) paper indicating the 6x improvement in convergence\n",
            "  with the ReLU unit compared to the tanh unit. \n",
            "  \n",
            "  ReLU. The Rectified Linear Unit has become very popular in the last few years. It computes the function $f(x) = \\max(0, x)$. In\n",
            "  other words, the activation is simply thresholded at zero (see image\n",
            "  above on the left). There are several pros and cons to using the\n",
            "  ReLUs: \n",
            "  \n",
            "  \n",
            "  (+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et\n",
            "  al.) the\n",
            "  convergence of stochastic gradient descent compared to the\n",
            "  sigmoid/tanh functions. It is argued that this is due to its linear,\n",
            "  non-saturating form.\n",
            "  (+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply\n",
            "  thresholding a matrix of activations at zero.\n",
            "  (-) Unfortunately, ReLU units can be fragile during training and can \"die\". For example, a large gradient flowing through a ReLU neuron\n",
            "  could cause the weights to update in such a way that the neuron will\n",
            "  never activate on any datapoint again. If this happens, then the\n",
            "  gradient flowing through the unit will forever be zero from that point\n",
            "  on. That is, the ReLU units can irreversibly die during training since\n",
            "  they can get knocked off the data manifold. For example, you may find\n",
            "  that as much as 40% of your network can be \"dead\" (i.e. neurons that\n",
            "  never activate across the entire training dataset) if the learning\n",
            "  rate is set too high. With a proper setting of the learning rate this\n",
            "  is less frequently an issue.\n",
            "  \n",
            "  \n",
            "  Leaky ReLU. Leaky ReLUs are one attempt to fix the \"dying ReLU\" problem. Instead of the function being zero when x &lt; 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes $f(x) = \\mathbb{1}(x &lt; 0) (\\alpha x) + \\mathbb{1}(x&gt;=0) (x) $ where $\\alpha$ is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in PReLU neurons, introduced in Delving Deep into Rectifiers, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  Maxout. Other types of units have been proposed that do not have the functional form $f(w^Tx + b)$ where a non-linearity is applied\n",
            "  on the dot product between the weights and the data. One relatively\n",
            "  popular choice is the Maxout neuron (introduced recently by\n",
            "  Goodfellow et\n",
            "  al.) that\n",
            "  generalizes the ReLU and its leaky version. The Maxout neuron computes\n",
            "  the function $\\max(w_1^Tx+b_1, w_2^Tx + b_2)$. Notice that both\n",
            "  ReLU and Leaky ReLU are a special case of this form (for example, for\n",
            "  ReLU we have $w_1, b_1 = 0$). The Maxout neuron therefore enjoys\n",
            "  all the benefits of a ReLU unit (linear regime of operation, no\n",
            "  saturation) and does not have its drawbacks (dying ReLU). However,\n",
            "  unlike the ReLU neurons it doubles the number of parameters for every\n",
            "  single neuron, leading to a high total number of parameters.\n",
            "  \n",
            "  This concludes our discussion of the most common types of neurons and\n",
            "  their activation functions. As a last comment, it is very rare to mix\n",
            "  and match different types of neurons in the same network, even though\n",
            "  there is no fundamental problem with doing so.\n",
            "  \n",
            "  TLDR: \"What neuron type should I use?\" Use the ReLU non-linearity, be careful with your learning rates and possibly\n",
            "  monitor the fraction of \"dead\" units in a network. If this concerns\n",
            "  you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but\n",
            "  expect it to work worse than ReLU/Maxout.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  License:\n",
            "  The MIT License (MIT)\n",
            "  \n",
            "  Copyright (c) 2015 Andrej Karpathy\n",
            "  \n",
            "  Permission is hereby granted, free of charge, to any person obtaining\n",
            "  a copy of this software and associated documentation files (the\n",
            "  \"Software\"), to deal in the Software without restriction, including\n",
            "  without limitation the rights to use, copy, modify, merge, publish,\n",
            "  distribute, sublicense, and/or sell copies of the Software, and to\n",
            "  permit persons to whom the Software is furnished to do so, subject to\n",
            "  the following conditions:\n",
            "  \n",
            "  The above copyright notice and this permission notice shall be\n",
            "  included in all copies or substantial portions of the Software.\n",
            "  \n",
            "  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n",
            "  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n",
            "  MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n",
            "  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY\n",
            "  CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\n",
            "  TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\n",
            "  SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.*\n",
            "\n",
            "\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "v = vectorizer_search(entry,vectorizer=vectorizer)\n",
        "# print(v)\n",
        "for _, row in v.iterrows():\n",
        "    print(row['cleaned_body'])\n",
        "    print('-------------------')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3AcnQSRd7XJz"
      },
      "source": [
        "How you can improve this approach ? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi4Y1vsqB_Sw"
      },
      "source": [
        "Answer here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hWNZJR6qYwcT"
      },
      "source": [
        "## Semantic similarity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6hbMp61saeBm"
      },
      "source": [
        "There are NLP methods that go further than word-by-word study, by taking into account the context of the terms. There are several methods: Word2vec, Bert.\n",
        "\n",
        "From the Sentence Transformers documentation: https://www.sbert.net/docs/pretrained_models.html choose the pre-trained model that you think is the most appropriate. Justify your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "gOLIblKWCSQ1"
      },
      "outputs": [],
      "source": [
        "sentence_transformer_model = 'distilbert-base-uncased'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8O6QZfgPCKMk"
      },
      "source": [
        "Answer Here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6r2keNXSeQ1J"
      },
      "source": [
        " To use Sentence Transformers it is recommended to activate the GPU of google colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "02cbfb1363ca47c3a7cc2999def08213",
            "4bb730174d4c4bbba4541378c4930b9a",
            "fd00d13717a2408d80a03cec7763b295",
            "daf36293a2a14b7f813cb5b5d647e418",
            "e6f675dc25a24500828fb93d771094d7",
            "82f969816ad447da859a3e6783eb91ad",
            "930f1f1678a24ae99b1ddf2e7f20fbb7",
            "ca8f9ce32124466c89bccc4565400592",
            "d557ff364a614615b0e699f6aec4c566",
            "1395a88926674243bcdedfb7d7275cc0",
            "8eb72fa3f46c42139fbc64a1aa08de33",
            "c31e7058036f4a5ab22a274c8ce1a6d1",
            "03232d89579f48a1a5e136585c9fde00",
            "d25d0d63d7e5478bac5f9dac00c1461e",
            "dda5d5c2dbf449e7963326f640853177",
            "9e71b3c469f046038afc7b2f7e3382ba",
            "abd43201cd1c4f7db45ddd229e6b14d7",
            "12bc320bc3af4f06a936c9a6265d377f",
            "4b3b5be0dc5c445b9a3050a1265d2b8b",
            "bf39cbc5495e4078a9ccbaa89ad89334",
            "20544ec0905344e9b2ade3b158e555b4",
            "9858c722a79f447f8c36a6e123531dc0",
            "7bafbc4f65d34c47bc032036ecad79be",
            "9f9e6718289f4b009e5cb4f36ca7bafb",
            "116a657b635243f1ba36a7972c7e8c7e",
            "f229cd2f2ccb429d8eff17791ae35e17",
            "b11de02b306d44e5a260b2d877473f5f",
            "b326af1bb0914bcd952c9dff5abe5d42",
            "579078e1423c48a681adc82059d847f8",
            "bb3564416a644d59a32b8d736f7e04a9",
            "2914a856431c4bf283304e29e9b01ed5",
            "620f8475e644452b87015d8eec37ebb0",
            "5e03079e6aa040c996a4bd2068b6c649",
            "9d92c18a3e06442db95bdb369c052f05",
            "d96e543eb74f48c6a5a5196505b60628",
            "8bee8bb002874630b335d8e0a8b801fe",
            "df4d9fb802124a6f9b9614eb3673bcf7",
            "4d9101a4b6024a39a8507ff585af1259",
            "8780b3fc6e7243f1a63c7c94fbdd8053",
            "d85ece8d99bb49139a70e016a06f97f1",
            "20281614a52b4fa7a93cddd31092adc5",
            "6ace7e1eaf8b451499dc60cbcc78d22e",
            "1daef32e6b51422eb47f135fdc8150f7",
            "799707ac4d0540a7ac88c80d95cc50fa",
            "a91b586064e740feb8eeb27c02590ccf",
            "a04d6415638344878fe8246ea326ffb0",
            "a1ed9c74fac441c9ace0aea654b3ae17",
            "f7d276d2f26041509e568c41e0f30420",
            "756d548f4c64446eb41d891c99b71c75",
            "3ad60474bdfc4b8fac306b1266371e9a",
            "ab54cc66da5f492a9830ca25f5f5acc7",
            "021c85fabd9047a493eb838440a46430",
            "73706f650c7e4ecd923fd51f5e1a714e",
            "890caa151ead4c24a8ef52684b4d1936",
            "66212781471e4992aa8ad74d52cd6884",
            "ee4622ffc57b421d8ab54a678bf524c3",
            "34adba70a0ec49e2ab2c678bff13f7a3",
            "f9b5207a40ee43bfaac71a93ef3fbde8",
            "e2cb4079b5e5401799fd1a63b4ffa04a",
            "d7d995cd240f4986b80abfe88b3274d6",
            "aae788b1c20645138d1d2d40d238a8aa",
            "4aca7ebf6cca4e08ad5becd75fbb9998",
            "89fb3553f50b457eb0894ed332e23f92",
            "10c179a861ab4e97b3d12c5dae4e1cb4",
            "f76299c226814ecb889be4fff0a0a028",
            "df42d059791b419cab83b20391557492",
            "7aa7c814c69e4f30ab54c7fed798031f",
            "c4adea87965f4f5ebb14058c6a94923f",
            "61f154962b844c809845f92091a16bf6",
            "0e23fa3ddec14f5699ede7e0dd685918",
            "ed9ae88080e14fc7a91af2cf8fba6a99",
            "edc62f56743a4b559e319134e994cbe8",
            "91b1d43e7b0f41729b62518b771239d8",
            "91926dcd6b9547b39d159e1169034f1c",
            "75c8821cea98493ab7aebeb5623131d6",
            "4463494d599648538e82ec1f0e908373",
            "a718f09041474a56a5ee86153f4f3bf3",
            "2ab01b6399ef40fba0e62527ccfe2b0c",
            "6558d1e3fd4b4cf291d83f5c3361caee",
            "0d1614f5f28b4673a99fc636195b44a6",
            "0613301aee514059be98d14668f1c5f1",
            "ce1d4f6b299344f5abed0651673ddf4a",
            "ac706715d0bb43e1b7c4723c9e5e4430",
            "b8ae662ba8cb468e9d51583ebec724b5",
            "a3f3dad3d24249269a74ba5f926c5dad",
            "1d400d2d61e94178ba7a10ecf88e8122",
            "464ab9096f6d4a3cb22c798aecaf389a",
            "3334558c60c34242aa337416918e5a61",
            "000421c2dae04292b304b5821cb1da18",
            "5d6ca948e1fb45c7874cfecd67baf42b",
            "8cfa043308094af1964a815860d4e6a9",
            "4480b9f250af492ca3855daa3a71a754",
            "1c98baf7eac4438d8e9a156b09c61e3d",
            "98c895752ff94e1ab0b9a5a650a55bd8",
            "9032c75a1b354ce8bda731535f6153d8",
            "99800bb0b202471081130fdd1ff908f5",
            "d6a7aada7e484e65a0027ff39fb0d160",
            "db1811b4dd6f449b8c4262382714405b",
            "b87e07ba46dd42e9966209ea70d0a056",
            "43fea82a481b4ecbb1ff5f230e8cc892",
            "8dc77074483f4aef97e47e6a04825cc5",
            "903f969ae3284dba85cec6e0c02f63c2",
            "7618bac4a10d4e73a3e584ad3d4709e8",
            "911c9cceca994aafa1807946894e15a1",
            "e4309448f07c458988dfec0059dbb060",
            "f87cdb0434dd4015bd0247cd8f76c44b",
            "d84f51b720b94d68ba6e217de01862a2",
            "30b2d058d9cd4fbf9a21a5a59e49a5d0",
            "45aeb8a8844f4bc4969f6658ee46eedf",
            "06e9b832566245e4aaab7267a966b303",
            "9ae513e627704134ad081e1dea561753",
            "b6d07ce384e74789869a47985a43f59e",
            "891bd71b9c654ac185c2d2642f40cfce",
            "6772b814e22f48c0935c78c4e0473bce",
            "8f9dc7eb45004f10b897bc97c808d0ce",
            "724890f8bb7248bf8b657c5cf9474d8a",
            "fccb0f67b73f4545a898bef4595e8c28",
            "3d6eb5d04dbb4601ac4dfdeb020fed99",
            "94d7d2d5cd194dc78ff9e39d3d3805f6",
            "ceb2ab98fa9d46189cef411c7078ffcf",
            "a298370ee5ab4396966d1ad4ab4f3fb8",
            "1d277ba2fc9a4223a7b1f5b5ad9a810c",
            "aaf6f82410c3424286f243d7c23086a2",
            "e46fea149dad4c638cc5059498a5e72b",
            "e0ada647f63a46c5b233a56a7531b6d8",
            "1e32c7f8c2594c95bb71a750ee4e9d54",
            "ff3788b121ca499fb214d9c281215798",
            "cf9800f0b7af4cafa11862cfbae61bdf",
            "e5a4b46cf4324b4a9d6b7c0e3a43d5cc",
            "13e9c182313340bfa06aba7b26450dfe",
            "d2bcf14afe9e49d786143eb6f426d391",
            "e5b40b520c194eb3ac4e210054acfaac",
            "fafef24076c748fbb90b5493742fe60f",
            "531e17804a964eed8cbf0026be12a6a1",
            "e934ec0c85a941dba7bc9639e9954d63",
            "214b5c6307c24bc9b48f5af649381866",
            "0036236066614fa4a3e41edb26fd145e",
            "26b7262aa5b54d3db86a5e98f4cb42de",
            "8107de17103f43c3bdb363ae5e8c0345",
            "62bd6da8a62c40129d2df54709717982",
            "8d45f5c29b6a454085d8c0993ab4920d",
            "a553b7100609441c812b6da9f0dfaea6",
            "b3d38e70255846a7b9dc25e092a89322",
            "143eb44c074f4037a4eeb61be9d52517",
            "7c9ecabc3db3498ba32fe820ce34ca8a",
            "7cc1d25c4d1c4365a73eca3584959c67",
            "3ebc5279727b4f578dd55bd34e5b7f47",
            "9b69e77883e14a18935a48b1c1d62275",
            "2867eb3ec54949fca3b840e3f8c1b191",
            "446d5dfb2a3f43b196dad2104f859102",
            "678fa128e70745509675063c2bbe37a6",
            "47f6af648231461e8bea7026342f7269",
            "2cb298973e6244db94a4e66e1065eda5",
            "089846d096e0463fa15075a1a341d378"
          ]
        },
        "id": "dCTzpEkkeQAO",
        "outputId": "6c8ec330-3745-41fa-c3fe-bf327ac6f405"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No sentence-transformers model found with name /home/himmi/.cache/torch/sentence_transformers/distilbert-base-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at /home/himmi/.cache/torch/sentence_transformers/distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "MODEL_ST = SentenceTransformer(sentence_transformer_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph24xTeVgTpU"
      },
      "source": [
        "Use this algorithm to encode the data in the database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "v-TTENiM5vn2"
      },
      "outputs": [],
      "source": [
        "# keep only 100 posts for the sake of computation\n",
        "posts = posts.iloc[:100]\n",
        "embeddings = MODEL_ST.encode(posts.cleaned_body.values, normalize_embeddings=True) # ne pas faire tourner, trop lent"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TBJ0W5UygN9C"
      },
      "source": [
        "*If this process is slow, you can save this array in case you need to load it again*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Qkw285npGxmY"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open(os.path.join(DATA_PATH, 'embeddings.pkl', 'wb') as file:\n",
        "#     pickle.dump(embeddings, file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hy__GOwgbIG"
      },
      "source": [
        "Make a function that transforms the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "35As3cuYgRWq"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "def encode_query(query : str) ->  numpy.ndarray:\n",
        "    \n",
        "    encoded_query = MODEL_ST.encode(query, normalize_embeddings=True)\n",
        "    return encoded_query"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xFD6pJIpgEz8"
      },
      "source": [
        "Which distance is most relevant to measure the distance between the input and the data?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2XHL8gFRIUu8"
      },
      "source": [
        "Answer here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6LxvJiVKIv-V"
      },
      "source": [
        "Write a function that returns a matrix containing information about the similarity between the query and the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ZtDl3Cicf9wd"
      },
      "outputs": [],
      "source": [
        "def similarity(query, embeddings=embeddings):\n",
        "    \n",
        "    query_embedding = encode_query(query)\n",
        "    similarity_matrix = np.dot(query_embedding, embeddings.T)\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_oR0xnZCJKfc"
      },
      "outputs": [],
      "source": [
        "query = 'what is stochastic gradient descent ?'\n",
        "matrix_similarity = similarity(query, embeddings)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2dRGokvThOcs"
      },
      "source": [
        "How do you determine which documents in the data set most closely match the input?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jd1UD26VhN_9"
      },
      "outputs": [],
      "source": [
        "def ordre_en_fonction_similarit(matrix_similarity):\n",
        "    \n",
        "    ordre = np.argsort(matrix_similarity)[::-1]\n",
        "    return ordre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-CoY6VUj5EE",
        "outputId": "81ec6aa6-f24b-478f-c70a-6f85ae8ac78b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1 2 0]\n"
          ]
        }
      ],
      "source": [
        "print(ordre_en_fonction_similarit([0.6, 0.8, 0.7]))\n",
        "# assert ordre_en_fonction_similarit([0.6, 0.8, 0.7]) == [1, 2, 0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a3IBMm8HisWy"
      },
      "source": [
        "Put it all together in a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BOFygRpbifXw"
      },
      "outputs": [],
      "source": [
        "def closest_semantic_doc(query, embeddings=embeddings, top_n=10):\n",
        "\n",
        "    matrix_similarity = similarity(query, embeddings)\n",
        "    ordre = ordre_en_fonction_similarit(matrix_similarity)\n",
        "    closest_posts = posts.iloc[ordre[:top_n]]\n",
        "    return closest_posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What are the data conditions that we should watch out for, where p-values may not be the best way of deciding statistical significance?  Are there specific problem types that fall into this category?\n",
            "\n",
            "-------------------\n",
            "Logic often states that by overfitting a model, its capacity to generalize is limited, though this might only mean that overfitting stops a model from improving after a certain complexity. Does overfitting cause models to become worse regardless of the complexity of data, and if so, why is this the case?\n",
            "\n",
            "\n",
            "\n",
            "Related: Followup to the question above, \"When is a Model Underfitted?\"\n",
            "\n",
            "-------------------\n",
            "If small p-values are plentiful in big data, what is a comparable replacement for p-values in data with million of samples?\n",
            "\n",
            "-------------------\n",
            "Assume that we have a set of elements E and a similarity (not distance) function sim(ei, ej) between two elements ei,ej  E. \n",
            "\n",
            "How could we (efficiently) cluster the elements of E, using sim?\n",
            "\n",
            "k-means, for example, requires a given k, Canopy Clustering requires two threshold values. What if we don't want such predefined parameters?\n",
            "\n",
            "Note, that sim is not neccessarily a metric (i.e. the triangle inequality may, or may not hold). Moreover, it doesn't matter if the clusters are disjoint (partitions of E).\n",
            "\n",
            "-------------------\n",
            "The most basic relationship to describe is a linear relationship between variables, x and y, such that they can be said to be highly-correlated when every increase in x results in a proportional increase in y. They can also be said to be inversely proportional so that when x increases, y decreases. And finally, the two variables can be said to be independent in the event that there is no linear relationship between the two (they are uncorrelated, or have a Pearson correlation coefficient of 0. [LaTeX support would be highly desirable at this point.]\n",
            "\n",
            "Different correlation coefficients and their uses:\n",
            "\n",
            "Pearson correlation coefficient is useful.....\n",
            "[draft]\n",
            "\n",
            "-------------------\n",
            "In which situations would one system be preferred over the other? What are the relative advantages and disadvantages of relational databases versus non-relational databases?\n",
            "\n",
            "-------------------\n",
            "R has many libraries which are aimed at Data Analysis (e.g. JAGS, BUGS, ARULES etc..), and is mentioned in popular textbooks such as: J.Krusche, Doing Bayesian Data Analysis; B.Lantz, \"Machine Learning with R\".\n",
            "\n",
            "I've seen a guideline of 5TB for a dataset to be considered as Big Data.\n",
            "\n",
            "My question is: Is R suitable for the amount of Data typically seen in Big Data problems? \n",
            "Are there strategies to be employed when using R with this size of dataset?\n",
            "\n",
            "-------------------\n",
            "You shouldn't consider the p-value out of context.\n",
            "\n",
            "One rather basic point (as illustrated by xkcd) is that you need to consider how many tests you're actually doing.  Obviously, you shouldn't be shocked to see p &lt; 0.05 for one out of 20 tests, even if the null hypothesis is true every time.  \n",
            "\n",
            "A more subtle example of this occurs in high-energy physics, and is known as the look-elsewhere effect.  The larger the parameter space you search for a signal that might represent a new particle, the more likely you are to see an apparent signal that's really just due to random fluctuations. \n",
            "\n",
            "-------------------\n",
            "One thing you should be aware of is the sample size you are using. Very large samples, such as economists using census data, will lead to deflated p-values. This paper \"Too Big to Fail: Large Samples and the p-Value Problem\" covers some of the issues. \n",
            "\n",
            "-------------------\n",
            "An activity that seeks patterns in a continuous stream of data elements, usually involving summarizing the stream in some way.\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "results = closest_semantic_doc(query)\n",
        "# for _, row in results.iterrows():\n",
        "#     print(row['cleaned_body'])\n",
        "#     print('-------------------')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ouUjQrhgjEDs"
      },
      "source": [
        "What methods could be used to improve the recommendations of this algorithm?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_v7HI2pDJk-f"
      },
      "source": [
        "Answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vNP-BErhJm4N"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>PostTypeId</th>\n",
              "      <th>CreationDate</th>\n",
              "      <th>Score</th>\n",
              "      <th>ViewCount</th>\n",
              "      <th>Body</th>\n",
              "      <th>OwnerUserId</th>\n",
              "      <th>LastActivityDate</th>\n",
              "      <th>Title</th>\n",
              "      <th>Tags</th>\n",
              "      <th>...</th>\n",
              "      <th>ContentLicense</th>\n",
              "      <th>AcceptedAnswerId</th>\n",
              "      <th>LastEditorUserId</th>\n",
              "      <th>LastEditDate</th>\n",
              "      <th>ParentId</th>\n",
              "      <th>OwnerDisplayName</th>\n",
              "      <th>CommunityOwnedDate</th>\n",
              "      <th>LastEditorDisplayName</th>\n",
              "      <th>FavoriteCount</th>\n",
              "      <th>cleaned_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>71</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-14T22:12:37.203</td>\n",
              "      <td>14</td>\n",
              "      <td>759.0</td>\n",
              "      <td>&lt;p&gt;What are the data conditions that we should...</td>\n",
              "      <td>179.0</td>\n",
              "      <td>2014-05-15T08:25:47.933</td>\n",
              "      <td>When are p-values deceptive?</td>\n",
              "      <td>&lt;bigdata&gt;&lt;statistics&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What are the data conditions that we should wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>61</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-14T18:09:01.940</td>\n",
              "      <td>56</td>\n",
              "      <td>16477.0</td>\n",
              "      <td>&lt;p&gt;Logic often states that by overfitting a mo...</td>\n",
              "      <td>158.0</td>\n",
              "      <td>2017-09-17T02:27:31.110</td>\n",
              "      <td>Why Is Overfitting Bad in Machine Learning?</td>\n",
              "      <td>&lt;machine-learning&gt;&lt;predictive-modeling&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2017-04-13T12:50:41.230</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Logic often states that by overfitting a model...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>75</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-15T00:26:11.387</td>\n",
              "      <td>5</td>\n",
              "      <td>166.0</td>\n",
              "      <td>&lt;p&gt;If small p-values are plentiful in big data...</td>\n",
              "      <td>158.0</td>\n",
              "      <td>2019-05-07T04:16:29.673</td>\n",
              "      <td>Is there a replacement for small p-values in b...</td>\n",
              "      <td>&lt;statistics&gt;&lt;bigdata&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>1330.0</td>\n",
              "      <td>2019-05-07T04:16:29.673</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>If small p-values are plentiful in big data, w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>103</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-16T14:26:12.270</td>\n",
              "      <td>24</td>\n",
              "      <td>9227.0</td>\n",
              "      <td>&lt;p&gt;Assume that we have a set of elements &lt;em&gt;E...</td>\n",
              "      <td>113.0</td>\n",
              "      <td>2021-06-28T09:13:21.753</td>\n",
              "      <td>Clustering based on similarity scores</td>\n",
              "      <td>&lt;clustering&gt;&lt;algorithms&gt;&lt;similarity&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Assume that we have a set of elements E and a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>123</td>\n",
              "      <td>5</td>\n",
              "      <td>2014-05-17T21:10:41.990</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;The most basic relationship to describe is ...</td>\n",
              "      <td>53.0</td>\n",
              "      <td>2014-05-20T13:50:21.763</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>53.0</td>\n",
              "      <td>2014-05-20T13:50:21.763</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The most basic relationship to describe is a l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-14T01:41:23.110</td>\n",
              "      <td>2</td>\n",
              "      <td>656.0</td>\n",
              "      <td>&lt;p&gt;In which situations would one system be pre...</td>\n",
              "      <td>64.0</td>\n",
              "      <td>2014-05-14T01:41:23.110</td>\n",
              "      <td>What are the advantages and disadvantages of S...</td>\n",
              "      <td>&lt;databases&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>In which situations would one system be prefer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "      <td>2014-05-14T11:15:40.907</td>\n",
              "      <td>55</td>\n",
              "      <td>10152.0</td>\n",
              "      <td>&lt;p&gt;R has many libraries which are aimed at Dat...</td>\n",
              "      <td>136.0</td>\n",
              "      <td>2019-02-23T11:34:41.513</td>\n",
              "      <td>Is the R language suitable for Big Data</td>\n",
              "      <td>&lt;bigdata&gt;&lt;r&gt;</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>2014-05-14T13:06:28.407</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>R has many libraries which are aimed at Data A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>73</td>\n",
              "      <td>2</td>\n",
              "      <td>2014-05-14T22:43:23.587</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;You shouldn't consider the p-value out of c...</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2014-05-14T22:43:23.587</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>71.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>You shouldn't consider the p-value out of cont...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>74</td>\n",
              "      <td>2</td>\n",
              "      <td>2014-05-14T22:58:11.583</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;p&gt;One thing you should be aware of is the sam...</td>\n",
              "      <td>64.0</td>\n",
              "      <td>2014-05-14T22:58:11.583</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>71.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One thing you should be aware of is the sample...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>109</td>\n",
              "      <td>4</td>\n",
              "      <td>2014-05-16T20:24:38.980</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>An activity that seeks patterns in a continuou...</td>\n",
              "      <td>200.0</td>\n",
              "      <td>2014-05-20T13:52:00.620</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>...</td>\n",
              "      <td>CC BY-SA 3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>200.0</td>\n",
              "      <td>2014-05-20T13:52:00.620</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>An activity that seeks patterns in a continuou...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows  23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Id  PostTypeId             CreationDate  Score  ViewCount  \\\n",
              "53   71           1  2014-05-14T22:12:37.203     14      759.0   \n",
              "44   61           1  2014-05-14T18:09:01.940     56    16477.0   \n",
              "57   75           1  2014-05-15T00:26:11.387      5      166.0   \n",
              "81  103           1  2014-05-16T14:26:12.270     24     9227.0   \n",
              "98  123           5  2014-05-17T21:10:41.990      0        NaN   \n",
              "5    15           1  2014-05-14T01:41:23.110      2      656.0   \n",
              "27   41           1  2014-05-14T11:15:40.907     55    10152.0   \n",
              "55   73           2  2014-05-14T22:43:23.587      5        NaN   \n",
              "56   74           2  2014-05-14T22:58:11.583      2        NaN   \n",
              "87  109           4  2014-05-16T20:24:38.980      0        NaN   \n",
              "\n",
              "                                                 Body  OwnerUserId  \\\n",
              "53  <p>What are the data conditions that we should...        179.0   \n",
              "44  <p>Logic often states that by overfitting a mo...        158.0   \n",
              "57  <p>If small p-values are plentiful in big data...        158.0   \n",
              "81  <p>Assume that we have a set of elements <em>E...        113.0   \n",
              "98  <p>The most basic relationship to describe is ...         53.0   \n",
              "5   <p>In which situations would one system be pre...         64.0   \n",
              "27  <p>R has many libraries which are aimed at Dat...        136.0   \n",
              "55  <p>You shouldn't consider the p-value out of c...         14.0   \n",
              "56  <p>One thing you should be aware of is the sam...         64.0   \n",
              "87  An activity that seeks patterns in a continuou...        200.0   \n",
              "\n",
              "           LastActivityDate  \\\n",
              "53  2014-05-15T08:25:47.933   \n",
              "44  2017-09-17T02:27:31.110   \n",
              "57  2019-05-07T04:16:29.673   \n",
              "81  2021-06-28T09:13:21.753   \n",
              "98  2014-05-20T13:50:21.763   \n",
              "5   2014-05-14T01:41:23.110   \n",
              "27  2019-02-23T11:34:41.513   \n",
              "55  2014-05-14T22:43:23.587   \n",
              "56  2014-05-14T22:58:11.583   \n",
              "87  2014-05-20T13:52:00.620   \n",
              "\n",
              "                                                Title  \\\n",
              "53                       When are p-values deceptive?   \n",
              "44        Why Is Overfitting Bad in Machine Learning?   \n",
              "57  Is there a replacement for small p-values in b...   \n",
              "81              Clustering based on similarity scores   \n",
              "98                                               None   \n",
              "5   What are the advantages and disadvantages of S...   \n",
              "27            Is the R language suitable for Big Data   \n",
              "55                                               None   \n",
              "56                                               None   \n",
              "87                                               None   \n",
              "\n",
              "                                       Tags  ...  ContentLicense  \\\n",
              "53                    <bigdata><statistics>  ...    CC BY-SA 3.0   \n",
              "44  <machine-learning><predictive-modeling>  ...    CC BY-SA 3.0   \n",
              "57                    <statistics><bigdata>  ...    CC BY-SA 3.0   \n",
              "81     <clustering><algorithms><similarity>  ...    CC BY-SA 3.0   \n",
              "98                                     None  ...    CC BY-SA 3.0   \n",
              "5                               <databases>  ...    CC BY-SA 3.0   \n",
              "27                             <bigdata><r>  ...    CC BY-SA 3.0   \n",
              "55                                     None  ...    CC BY-SA 3.0   \n",
              "56                                     None  ...    CC BY-SA 3.0   \n",
              "87                                     None  ...    CC BY-SA 3.0   \n",
              "\n",
              "    AcceptedAnswerId LastEditorUserId             LastEditDate  ParentId  \\\n",
              "53              84.0              NaN                     None       NaN   \n",
              "44              62.0             -1.0  2017-04-13T12:50:41.230       NaN   \n",
              "57              78.0           1330.0  2019-05-07T04:16:29.673       NaN   \n",
              "81               NaN              NaN                     None       NaN   \n",
              "98               NaN             53.0  2014-05-20T13:50:21.763       NaN   \n",
              "5                NaN              NaN                     None       NaN   \n",
              "27              44.0            118.0  2014-05-14T13:06:28.407       NaN   \n",
              "55               NaN              NaN                     None      71.0   \n",
              "56               NaN              NaN                     None      71.0   \n",
              "87               NaN            200.0  2014-05-20T13:52:00.620       NaN   \n",
              "\n",
              "    OwnerDisplayName CommunityOwnedDate  LastEditorDisplayName FavoriteCount  \\\n",
              "53              None               None                   None           NaN   \n",
              "44              None               None                   None           NaN   \n",
              "57              None               None                   None           NaN   \n",
              "81              None               None                   None           NaN   \n",
              "98              None               None                   None           NaN   \n",
              "5               None               None                   None           NaN   \n",
              "27              None               None                   None           NaN   \n",
              "55              None               None                   None           NaN   \n",
              "56              None               None                   None           NaN   \n",
              "87              None               None                   None           NaN   \n",
              "\n",
              "                                         cleaned_body  \n",
              "53  What are the data conditions that we should wa...  \n",
              "44  Logic often states that by overfitting a model...  \n",
              "57  If small p-values are plentiful in big data, w...  \n",
              "81  Assume that we have a set of elements E and a ...  \n",
              "98  The most basic relationship to describe is a l...  \n",
              "5   In which situations would one system be prefer...  \n",
              "27  R has many libraries which are aimed at Data A...  \n",
              "55  You shouldn't consider the p-value out of cont...  \n",
              "56  One thing you should be aware of is the sample...  \n",
              "87  An activity that seeks patterns in a continuou...  \n",
              "\n",
              "[10 rows x 23 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fa4Ij2usk8Gb"
      },
      "source": [
        "## Text clustering (BONUS)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EfZcQyxLox8U"
      },
      "source": [
        "We can use topic modeling techniques to identify groups of texts among our document base and classify the input to restrict the application of the proximity calculations seen previously."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kt9_ujt6sVMg"
      },
      "source": [
        "#### LDA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eH2IG1iyTxiD"
      },
      "source": [
        "Latent Dirichlet Allocation is a topic modeling algorithm that allows soft clustering. Soft clustering means that the LDA does not allocate an input to a cluster, but gives a probabilistic score for each identified cluster. This decomposition allows to identify topics within the documents. \n",
        "\n",
        "In order to compute this algorithm, you need to vectorize your data (you can use the one you have already done previously or make another one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BfpEOZYkk7kH"
      },
      "outputs": [],
      "source": [
        "# Vectorize document using TF-IDF\n",
        "vectorizer_lda = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "\n",
        "# Fit and Transform the documents\n",
        "train_data = vectorizer_lda.fit_transform(posts.cleaned_body.values) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WSZ1kI3JWfTL"
      },
      "source": [
        "You can use Gensim or scikit-learn to compute LDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oDGFnU1CJ6_w"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LatentDirichletAllocation()"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Init the Model\n",
        "lda_model = LatentDirichletAllocation(n_components=10, max_iter=10)\n",
        "\n",
        "# Fit the Model\n",
        "lda_model.fit(train_data)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nG_grQ93tMIB"
      },
      "source": [
        "Assign a main topic to each document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZM3cSJrWuYT_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.01808268 0.8372429  0.01808522 0.01808199 0.01808166 0.01808664\n",
            "  0.018082   0.01808126 0.01808391 0.01809172]\n",
            " [0.01924021 0.01924048 0.01923927 0.01924172 0.01924096 0.82683405\n",
            "  0.01924181 0.01923931 0.01923989 0.01924231]\n",
            " [0.01471923 0.01472343 0.01471998 0.01471924 0.01471926 0.01472362\n",
            "  0.0147213  0.01471876 0.0147193  0.86751587]\n",
            " [0.82627387 0.01930321 0.0192983  0.01930017 0.01930394 0.01930413\n",
            "  0.01930715 0.01929825 0.01930288 0.0193081 ]\n",
            " [0.02097993 0.0209786  0.02097855 0.02097818 0.02097826 0.81118557\n",
            "  0.02098118 0.02097949 0.02097834 0.02098192]\n",
            " [0.02662726 0.02662849 0.02662705 0.02662705 0.02662706 0.02662763\n",
            "  0.02662705 0.02662721 0.02662723 0.76035397]\n",
            " [0.01915965 0.01917139 0.8275217  0.01916477 0.01915682 0.01916545\n",
            "  0.0191633  0.01915639 0.01916189 0.01917865]\n",
            " [0.02740677 0.02740769 0.02740714 0.02740677 0.02740718 0.02740677\n",
            "  0.02740684 0.02740677 0.75333704 0.02740705]\n",
            " [0.1        0.1        0.1        0.1        0.1        0.1\n",
            "  0.1        0.1        0.1        0.1       ]\n",
            " [0.01650396 0.01650391 0.01650466 0.14625338 0.01650218 0.01651188\n",
            "  0.01650384 0.01650215 0.016502   0.72171205]\n",
            " [0.0172853  0.84441343 0.01728429 0.01728318 0.01728282 0.01729318\n",
            "  0.01728387 0.01728282 0.0172838  0.0173073 ]\n",
            " [0.011427   0.01142002 0.01142364 0.01141907 0.01141643 0.22144291\n",
            "  0.01141879 0.01141734 0.01142083 0.68719397]\n",
            " [0.01941359 0.01941046 0.82525637 0.01941241 0.01941613 0.01943723\n",
            "  0.01941192 0.01940828 0.01940898 0.01942464]\n",
            " [0.02726899 0.02726234 0.02726139 0.02726055 0.02726057 0.02726794\n",
            "  0.02726796 0.0272604  0.75462154 0.02726832]\n",
            " [0.70485871 0.01396101 0.01396526 0.01395933 0.01396473 0.18342592\n",
            "  0.01396528 0.01396451 0.01396109 0.01397415]\n",
            " [0.01309735 0.01309487 0.01309224 0.01309567 0.01309083 0.42792933\n",
            "  0.46731172 0.01309263 0.01309204 0.01310332]\n",
            " [0.0136534  0.01365334 0.01365134 0.01365208 0.01365092 0.01365624\n",
            "  0.01365221 0.01365083 0.0136514  0.87712824]\n",
            " [0.01997326 0.01997428 0.82023137 0.01997453 0.01997428 0.01997385\n",
            "  0.01997332 0.0199747  0.01997402 0.0199764 ]\n",
            " [0.73190293 0.02978798 0.02978765 0.02978745 0.02978841 0.02978976\n",
            "  0.0297909  0.02978743 0.02978776 0.02978973]\n",
            " [0.01382091 0.01382349 0.01381445 0.01380698 0.01380846 0.01381343\n",
            "  0.47598466 0.41350363 0.01380693 0.01381705]\n",
            " [0.53352111 0.01346964 0.01347338 0.013469   0.01346959 0.01348298\n",
            "  0.01347329 0.01346933 0.01346821 0.35870347]\n",
            " [0.01528512 0.01528291 0.01528318 0.01528976 0.01528528 0.01528968\n",
            "  0.8624237  0.01528212 0.01528534 0.01529291]\n",
            " [0.01736625 0.01736607 0.01736609 0.01736633 0.01736573 0.84370033\n",
            "  0.01736591 0.01736577 0.01736557 0.01737195]\n",
            " [0.01297916 0.01297927 0.01297591 0.01298355 0.01297712 0.52402864\n",
            "  0.0129841  0.01297534 0.01297695 0.37213996]\n",
            " [0.01661939 0.01661994 0.01661953 0.0166187  0.01661816 0.01662733\n",
            "  0.01661977 0.01661829 0.01662098 0.8504179 ]\n",
            " [0.0219342  0.02193311 0.80258455 0.02193195 0.02193235 0.0219379\n",
            "  0.02193586 0.02193204 0.02193225 0.02194579]\n",
            " [0.01400762 0.01400875 0.01400697 0.01400802 0.01400722 0.87391928\n",
            "  0.01400749 0.01400658 0.01400942 0.01401865]\n",
            " [0.01915165 0.01915489 0.01915016 0.01914979 0.01914963 0.01915148\n",
            "  0.01915325 0.01915041 0.01914958 0.82763916]\n",
            " [0.01445578 0.01445539 0.0144867  0.01445542 0.01445777 0.86985621\n",
            "  0.01445771 0.01445468 0.01445625 0.0144641 ]\n",
            " [0.01487747 0.01485992 0.86626001 0.01485168 0.01484392 0.01485726\n",
            "  0.01485491 0.01484916 0.01485056 0.01489513]\n",
            " [0.01875489 0.01875312 0.01875422 0.01875132 0.01875187 0.83120822\n",
            "  0.01875619 0.01875132 0.01875267 0.01876619]\n",
            " [0.01187062 0.01186849 0.01187077 0.01186622 0.0118667  0.01187772\n",
            "  0.89316856 0.0118662  0.01186846 0.01187627]\n",
            " [0.73585575 0.02934914 0.02935068 0.02934882 0.02934962 0.02934901\n",
            "  0.02934886 0.02934955 0.02934948 0.02934908]\n",
            " [0.01569577 0.01569579 0.01573379 0.0156925  0.01569358 0.01569499\n",
            "  0.01569243 0.01569062 0.01569116 0.85871937]\n",
            " [0.01198561 0.01198225 0.01198916 0.01198119 0.89213636 0.01198654\n",
            "  0.01198517 0.01197989 0.01198397 0.01198986]\n",
            " [0.02413445 0.02413127 0.02413702 0.02413265 0.02413402 0.02413147\n",
            "  0.78280304 0.02413116 0.02413116 0.02413376]\n",
            " [0.84355494 0.01738137 0.0173832  0.01738762 0.017382   0.01738358\n",
            "  0.01738263 0.01738118 0.01738156 0.01738191]\n",
            " [0.01966784 0.01966621 0.01966587 0.82299526 0.01966612 0.01966919\n",
            "  0.01966833 0.01966605 0.01966625 0.01966889]\n",
            " [0.02164765 0.80517619 0.02165911 0.02163929 0.02163928 0.02164811\n",
            "  0.02165151 0.02163936 0.02164057 0.02165892]\n",
            " [0.02187329 0.02187755 0.0218728  0.0218751  0.02187262 0.02187592\n",
            "  0.80313059 0.02187255 0.02187397 0.0218756 ]\n",
            " [0.0106024  0.0106111  0.0106005  0.01060143 0.01060285 0.20553271\n",
            "  0.01060216 0.01059994 0.38227067 0.33797623]\n",
            " [0.02010522 0.81904451 0.02011105 0.02010309 0.02010387 0.02010618\n",
            "  0.0201063  0.0201035  0.02010382 0.02011246]\n",
            " [0.01384097 0.87541357 0.0138396  0.01384239 0.01384355 0.01384938\n",
            "  0.01384255 0.01383807 0.01383825 0.01385169]\n",
            " [0.02140634 0.02140729 0.02141801 0.02141155 0.02140657 0.02142149\n",
            "  0.80729835 0.02140615 0.0214062  0.02141805]\n",
            " [0.02084121 0.02083845 0.02083793 0.02083449 0.81243452 0.02084413\n",
            "  0.020845   0.02083897 0.02083506 0.02085024]\n",
            " [0.01546738 0.01546663 0.01546551 0.01546338 0.01547106 0.01547584\n",
            "  0.8607779  0.01546227 0.01546533 0.01548469]\n",
            " [0.0120655  0.01206607 0.01206517 0.01206423 0.01206693 0.01206965\n",
            "  0.01206629 0.01206413 0.01206578 0.89140626]\n",
            " [0.1        0.1        0.1        0.1        0.1        0.1\n",
            "  0.1        0.1        0.1        0.1       ]\n",
            " [0.01747373 0.01746967 0.01747045 0.01746646 0.01746652 0.01747437\n",
            "  0.84275227 0.01746956 0.01746699 0.01748997]\n",
            " [0.0167156  0.01671389 0.01676486 0.01670302 0.84954358 0.01671353\n",
            "  0.01671282 0.01670039 0.01670117 0.01673114]\n",
            " [0.02037023 0.0203641  0.02036408 0.02036437 0.02037549 0.02036442\n",
            "  0.02036861 0.02036445 0.02036399 0.81670026]\n",
            " [0.6768354  0.01537539 0.01538664 0.0153721  0.0153776  0.01537974\n",
            "  0.0153766  0.01537697 0.01537638 0.20014318]\n",
            " [0.83568952 0.01826008 0.01825287 0.0182541  0.01825366 0.01826644\n",
            "  0.01825467 0.01825281 0.01825328 0.01826257]\n",
            " [0.02465747 0.02465359 0.02465528 0.02465294 0.02465298 0.02465333\n",
            "  0.77810471 0.02465255 0.0246528  0.02466435]\n",
            " [0.01821814 0.01821702 0.01820698 0.83605172 0.01820299 0.01823317\n",
            "  0.01822704 0.01820084 0.01820389 0.01823822]\n",
            " [0.01690032 0.01690096 0.0169005  0.01690112 0.01689956 0.84789488\n",
            "  0.01690004 0.01689954 0.01689998 0.01690312]\n",
            " [0.02108534 0.0210809  0.02108121 0.02108084 0.02108176 0.02108216\n",
            "  0.02108197 0.02108047 0.02108065 0.81026471]\n",
            " [0.7442757  0.02841095 0.02841175 0.02841077 0.02841029 0.02841726\n",
            "  0.02841256 0.02841037 0.02841043 0.02842993]\n",
            " [0.0190119  0.01898693 0.01898982 0.01896908 0.01896916 0.01903435\n",
            "  0.01903448 0.82888539 0.01897279 0.01914611]\n",
            " [0.01144375 0.89715862 0.01142664 0.01141616 0.01142413 0.01142382\n",
            "  0.01142442 0.0114263  0.01142507 0.0114311 ]\n",
            " [0.01695504 0.01695308 0.01695617 0.84740083 0.01695269 0.0169555\n",
            "  0.01695181 0.01695868 0.01695126 0.01696495]\n",
            " [0.01566985 0.01566815 0.01566796 0.01566732 0.0156677  0.01567391\n",
            "  0.85898157 0.0156671  0.01566769 0.01566876]\n",
            " [0.01990688 0.01990542 0.01990517 0.0199057  0.01990467 0.82083994\n",
            "  0.01991458 0.01990455 0.01990477 0.01990832]\n",
            " [0.01910729 0.0191071  0.49311727 0.35403172 0.0191047  0.01910769\n",
            "  0.01910634 0.01910333 0.01910436 0.0191102 ]\n",
            " [0.02088904 0.02088726 0.81200575 0.02088788 0.02088777 0.02088752\n",
            "  0.02088865 0.02088707 0.02088743 0.02089164]\n",
            " [0.6096023  0.0139207  0.0139194  0.01391793 0.01391955 0.01392062\n",
            "  0.01391705 0.01391661 0.01391792 0.27904793]\n",
            " [0.01504036 0.8646367  0.01503818 0.01504117 0.01503698 0.0150442\n",
            "  0.01504159 0.01503562 0.01503655 0.01504864]\n",
            " [0.01385683 0.01384882 0.01384843 0.0138525  0.01384819 0.01385158\n",
            "  0.01384767 0.01384698 0.01384738 0.87535162]\n",
            " [0.02199611 0.02191266 0.02190613 0.02192288 0.02191919 0.68939648\n",
            "  0.13523585 0.02188544 0.02190918 0.02191607]\n",
            " [0.01340308 0.01340496 0.01339979 0.01339703 0.0133971  0.01340052\n",
            "  0.01340402 0.01339802 0.87938172 0.01341376]\n",
            " [0.01437339 0.01437369 0.01437916 0.01436978 0.87060087 0.01438111\n",
            "  0.01437975 0.01436961 0.01437289 0.01439975]\n",
            " [0.01751252 0.01752514 0.84229565 0.01751709 0.01751881 0.01754275\n",
            "  0.0175287  0.01751226 0.01751331 0.01753377]\n",
            " [0.008973   0.00896936 0.14404149 0.00897217 0.00897222 0.00899937\n",
            "  0.00896687 0.00896351 0.00896331 0.7841787 ]\n",
            " [0.01279572 0.01279737 0.0127919  0.01279091 0.01279075 0.88486537\n",
            "  0.01279026 0.01279033 0.01278927 0.01279812]\n",
            " [0.01647633 0.01648194 0.01647794 0.01648568 0.01647953 0.01650353\n",
            "  0.85165148 0.01647436 0.01647679 0.0164924 ]\n",
            " [0.02516009 0.02516781 0.02516054 0.02516003 0.02516008 0.77354811\n",
            "  0.02516088 0.02516003 0.02516003 0.0251624 ]\n",
            " [0.01969895 0.82268931 0.01969931 0.01969945 0.01969906 0.01970775\n",
            "  0.01970085 0.01969944 0.01969919 0.01970669]\n",
            " [0.01789498 0.8389818  0.01789035 0.01788883 0.01788821 0.01789755\n",
            "  0.01788689 0.01788751 0.0178913  0.01789257]\n",
            " [0.01422002 0.01423692 0.0142191  0.01422174 0.01422008 0.01424102\n",
            "  0.01422231 0.01422056 0.01422161 0.87197664]\n",
            " [0.01303694 0.01302461 0.01303084 0.01302482 0.01303156 0.01302859\n",
            "  0.01302945 0.01302014 0.88271231 0.01306074]\n",
            " [0.01907187 0.01907365 0.01907315 0.82833684 0.01907142 0.0190791\n",
            "  0.0190724  0.01907151 0.01907166 0.0190784 ]\n",
            " [0.01873916 0.0187218  0.0187245  0.01872424 0.01872105 0.01872159\n",
            "  0.0187238  0.01872068 0.83147708 0.0187261 ]\n",
            " [0.02271692 0.02272119 0.02271661 0.02271772 0.02271688 0.79554111\n",
            "  0.022718   0.02271667 0.02271728 0.02271762]\n",
            " [0.03353881 0.69814394 0.03353911 0.03353874 0.03353875 0.03354043\n",
            "  0.03353956 0.03353875 0.03353873 0.03354319]\n",
            " [0.85291044 0.01634112 0.01634271 0.01634037 0.0163409  0.01634722\n",
            "  0.01635056 0.01634064 0.0163403  0.01634575]\n",
            " [0.01597107 0.01596666 0.01596516 0.01596617 0.01596562 0.01596698\n",
            "  0.01596597 0.01596522 0.01596651 0.85630063]\n",
            " [0.1        0.1        0.1        0.1        0.1        0.1\n",
            "  0.1        0.1        0.1        0.1       ]\n",
            " [0.75683924 0.02701737 0.02701702 0.02701624 0.02701631 0.02702113\n",
            "  0.02701949 0.02701624 0.02701736 0.02701959]\n",
            " [0.01616573 0.01616866 0.01616914 0.01616732 0.01617353 0.01617548\n",
            "  0.01616652 0.01616543 0.85446689 0.01618129]\n",
            " [0.69665775 0.01256386 0.01255352 0.01255617 0.01255117 0.2028625\n",
            "  0.01256469 0.01255046 0.01255569 0.01258421]\n",
            " [0.02247618 0.02246553 0.02245999 0.02246116 0.02246013 0.7978219\n",
            "  0.02246029 0.02246472 0.02246111 0.022469  ]\n",
            " [0.02142925 0.02142128 0.02141861 0.02141915 0.80718804 0.02143147\n",
            "  0.02142472 0.02142117 0.02141949 0.02142682]\n",
            " [0.01567241 0.01565311 0.01563696 0.8591647  0.01563256 0.01564738\n",
            "  0.01565834 0.01563475 0.01563363 0.01566617]\n",
            " [0.01322729 0.88097297 0.01325454 0.01321712 0.01321769 0.01322595\n",
            "  0.01321958 0.01322461 0.01321713 0.01322311]\n",
            " [0.02126322 0.02126605 0.02126279 0.02126103 0.02126234 0.02126688\n",
            "  0.0212615  0.8086242  0.0212612  0.02127078]\n",
            " [0.02919356 0.02919408 0.0291933  0.02919338 0.02919363 0.73725679\n",
            "  0.02919344 0.02919328 0.02919367 0.02919487]\n",
            " [0.01350235 0.01349757 0.01349886 0.01351799 0.87846985 0.01350724\n",
            "  0.01349985 0.01349568 0.0135052  0.01350541]\n",
            " [0.02075148 0.02075272 0.02075369 0.02075128 0.02075104 0.02075285\n",
            "  0.02075197 0.02075145 0.02075564 0.81322787]\n",
            " [0.01963026 0.01962935 0.01962934 0.01962872 0.01963083 0.01963493\n",
            "  0.01962914 0.01962871 0.82331629 0.01964244]\n",
            " [0.02178591 0.02178972 0.02178557 0.02178576 0.02178585 0.02178606\n",
            "  0.02178682 0.02178536 0.02178617 0.80392278]]\n"
          ]
        }
      ],
      "source": [
        "topic_documents = lda_model.transform(train_data)\n",
        "\n",
        "print(topic_documents)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oO1ZaPy6s6Uf"
      },
      "source": [
        "Write a function that assigns a topic to the query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "n86QrUuus5Bh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n"
          ]
        }
      ],
      "source": [
        "def get_topic_query(query, vectorizer=vectorizer_lda, lda_model=lda_model):\n",
        "\n",
        "    query_vectorized = vectorizer.transform([query])\n",
        "    topic_query = lda_model.transform(query_vectorized)\n",
        "    return topic_query[0]\n",
        "query = 'what is stochastic gradient descent ?'\n",
        "topic_query = get_topic_query(query)\n",
        "print(topic_query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQfMqiMpvKk5"
      },
      "source": [
        "## Merge methods"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll1RCEV-vVbJ"
      },
      "source": [
        "Write an algorithm to merge the methods seen. Which methods to use? How can you check if they are relevant ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-vIibmanKPLC"
      },
      "source": [
        "Answer here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "v2GUl3uHSJej"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "X has 2515 features, but LatentDirichletAllocation is expecting 1000 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m     matching_posts \u001b[39m=\u001b[39m posts\u001b[39m.\u001b[39miloc[indices]\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m matching_posts\n\u001b[0;32m---> 25\u001b[0m results \u001b[39m=\u001b[39m nlp_search_algorithm(query)\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m _, row \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m     27\u001b[0m     \u001b[39mprint\u001b[39m(row[\u001b[39m'\u001b[39m\u001b[39mcleaned_body\u001b[39m\u001b[39m'\u001b[39m])\n",
            "Cell \u001b[0;32mIn[42], line 14\u001b[0m, in \u001b[0;36mnlp_search_algorithm\u001b[0;34m(query, topic_documents, vectors, vectorizer, vectorizer_lda, lda_model, embeddings, top_n)\u001b[0m\n\u001b[1;32m     12\u001b[0m topic_query \u001b[39m=\u001b[39m get_topic_query(query, vectorizer_lda, lda_model)\n\u001b[1;32m     13\u001b[0m \u001b[39m# get the topic of the documents\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m topic_documents \u001b[39m=\u001b[39m lda_model\u001b[39m.\u001b[39;49mtransform(vectors)\n\u001b[1;32m     15\u001b[0m \u001b[39m# compute the similarity between the query and the documents\u001b[39;00m\n\u001b[1;32m     16\u001b[0m similarity_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(topic_query, topic_documents\u001b[39m.\u001b[39mT)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    148\u001b[0m         )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:738\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39m\"\"\"Transform data X according to the fitted model.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \n\u001b[1;32m    724\u001b[0m \u001b[39m   .. versionchanged:: 0.18\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[39m    Document topic distribution for X.\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 738\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_non_neg_array(\n\u001b[1;32m    739\u001b[0m     X, reset_n_features\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, whom\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLatentDirichletAllocation.transform\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    740\u001b[0m )\n\u001b[1;32m    741\u001b[0m doc_topic_distr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unnormalized_transform(X)\n\u001b[1;32m    742\u001b[0m doc_topic_distr \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m doc_topic_distr\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[:, np\u001b[39m.\u001b[39mnewaxis]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/decomposition/_lda.py:561\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._check_non_neg_array\u001b[0;34m(self, X, reset_n_features, whom)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39m\"\"\"check X format\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \n\u001b[1;32m    552\u001b[0m \u001b[39mcheck X format and make sure no negative value in X.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m \n\u001b[1;32m    558\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    559\u001b[0m dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32] \u001b[39mif\u001b[39;00m reset_n_features \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponents_\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> 561\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    562\u001b[0m     X,\n\u001b[1;32m    563\u001b[0m     reset\u001b[39m=\u001b[39;49mreset_n_features,\n\u001b[1;32m    564\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    565\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    566\u001b[0m )\n\u001b[1;32m    567\u001b[0m check_non_negative(X, whom)\n\u001b[1;32m    569\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    571\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 370\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: X has 2515 features, but LatentDirichletAllocation is expecting 1000 features as input."
          ]
        }
      ],
      "source": [
        "def nlp_search_algorithm(query,\n",
        "                         topic_documents=topic_documents,\n",
        "                         vectors=vectors,\n",
        "                         vectorizer=vectorizer,\n",
        "                         vectorizer_lda=vectorizer_lda,\n",
        "                         lda_model=lda_model,\n",
        "                         embeddings=embeddings,\n",
        "                         top_n=10\n",
        "                         )->list:\n",
        "    \n",
        "    \n",
        "    return matching_posts\n",
        "\n",
        "results = nlp_search_algorithm(query)\n",
        "for _, row in results.iterrows():\n",
        "    print(row['cleaned_body'])\n",
        "    print('-------------------')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mWh3EuXiT_Qc"
      },
      "source": [
        "Once you have a list of possible results, you can it: (you can use one of the ranking algorithms you have previously done or make up a new one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FbEcpONT-1Z"
      },
      "outputs": [],
      "source": [
        "def rank(possible_results):\n",
        "    #to_do\n",
        "    return possible_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ3vhg1kpX6l"
      },
      "source": [
        "## Incorporation in the search engine"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4CDbNI-qpf3z"
      },
      "source": [
        "### Addition of metadata"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yZBXyHN3MKBx"
      },
      "source": [
        "You must now have a new set of metadata and data to add to your original index. You can load the index you had as a result of Day 2 and today's work to it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK9U-NB0MctD"
      },
      "outputs": [],
      "source": [
        "#load previous data \n",
        "\n",
        "#TODO "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV6jul7jNW7y"
      },
      "outputs": [],
      "source": [
        "# add the new data to the previous index"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HvDxs_EZKyeO"
      },
      "source": [
        "Hint : if you have changed the preprocessing function at the beginning of this notebook make sure to update the Clean Body attribute"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ8PuvLBUap8"
      },
      "source": [
        "### Adaptation to the index format\n",
        "\n",
        "Adapt your nlp search results to the index format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alY_kdTOUaFa"
      },
      "outputs": [],
      "source": [
        "def nlp_search_in_index(query,\n",
        "                        index,\n",
        "                        args):\n",
        "\n",
        "    return matching_posts\n",
        "  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vPC4lvgeUpAX"
      },
      "source": [
        "### Compare the new searching and ranking method to the previous ones\n",
        "\n",
        "Compare in terms of efficiency (precision, completeness, speed, memory usage)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N_yte-oapjFW"
      },
      "source": [
        "### merge all methods to make an efficient search algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGcqIGl8pXhX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU2e3tgiIHzr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
